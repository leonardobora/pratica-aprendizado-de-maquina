{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1179de6",
   "metadata": {},
   "source": [
    "# 07 - Post-processing and Morphological Operations\n",
    "\n",
    "## Overview\n",
    "This notebook implements advanced post-processing techniques for cardiac MRI segmentation, including:\n",
    "- **Morphological Operations**: Opening, closing, erosion, dilation\n",
    "- **Connected Component Analysis**: Removing isolated pixels and small components\n",
    "- **Anatomical Constraints**: Enforcing realistic cardiac anatomy\n",
    "- **Boundary Refinement**: Smoothing and regularizing segmentation boundaries\n",
    "- **False Positive Removal**: Eliminating artifacts and noise\n",
    "- **Quality Assessment**: Validating post-processed results\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand morphological image processing for medical segmentation\n",
    "- Implement connected component analysis for noise removal\n",
    "- Apply anatomical constraints for cardiac structures\n",
    "- Develop boundary refinement techniques\n",
    "- Create quality assessment metrics for post-processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ef9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and Package Management\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install required packages for Colab\n",
    "    !pip install -q opencv-python scikit-image scipy matplotlib seaborn\n",
    "    \n",
    "    # Mount Google Drive if needed\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set working directory\n",
    "    os.chdir('/content/drive/MyDrive/cardiac_segmentation')\n",
    "else:\n",
    "    # For local development, ensure packages are available\n",
    "    try:\n",
    "        import cv2\n",
    "        import skimage\n",
    "        import scipy\n",
    "    except ImportError as e:\n",
    "        print(f\"Please install missing package: {e}\")\n",
    "        print(\"Run: pip install opencv-python scikit-image scipy\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "Path(\"outputs/postprocessed\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"outputs/morphology\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Running in: {'Google Colab' if IN_COLAB else 'Local Environment'}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Image Processing\n",
    "import cv2\n",
    "from skimage import morphology, measure, filters, segmentation\n",
    "from skimage.feature import peak_local_maxima\n",
    "from skimage.segmentation import watershed, clear_border\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Utilities\n",
    "from typing import Dict, List, Tuple, Optional, Union, Callable\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style for better visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90cad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PostProcessConfig:\n",
    "    \"\"\"Configuration for post-processing operations\"\"\"\n",
    "    \n",
    "    # Morphological operations\n",
    "    kernel_size: int = 3\n",
    "    erosion_iterations: int = 1\n",
    "    dilation_iterations: int = 2\n",
    "    opening_iterations: int = 1\n",
    "    closing_iterations: int = 2\n",
    "    \n",
    "    # Connected components\n",
    "    min_component_size: int = 100\n",
    "    max_component_count: int = 5\n",
    "    connectivity: int = 8\n",
    "    \n",
    "    # Anatomical constraints\n",
    "    min_heart_area: int = 1000\n",
    "    max_heart_area: int = 50000\n",
    "    aspect_ratio_range: Tuple[float, float] = (0.5, 2.0)\n",
    "    circularity_threshold: float = 0.3\n",
    "    \n",
    "    # Boundary refinement\n",
    "    gaussian_sigma: float = 1.0\n",
    "    contour_epsilon: float = 0.02\n",
    "    smooth_iterations: int = 3\n",
    "    \n",
    "    # Distance constraints\n",
    "    max_distance_between_structures: float = 100.0\n",
    "    min_distance_from_border: int = 10\n",
    "    \n",
    "    # Quality thresholds\n",
    "    confidence_threshold: float = 0.7\n",
    "    dice_threshold: float = 0.8\n",
    "    hausdorff_threshold: float = 10.0\n",
    "\n",
    "@dataclass\n",
    "class SegmentationResult:\n",
    "    \"\"\"Container for segmentation results with metadata\"\"\"\n",
    "    prediction: np.ndarray\n",
    "    confidence: np.ndarray\n",
    "    original_shape: Tuple[int, ...]\n",
    "    processing_time: float\n",
    "    metadata: Dict\n",
    "    \n",
    "@dataclass\n",
    "class PostProcessingMetrics:\n",
    "    \"\"\"Metrics for post-processing evaluation\"\"\"\n",
    "    components_removed: int\n",
    "    area_change: float\n",
    "    boundary_smoothness: float\n",
    "    confidence_improvement: float\n",
    "    processing_time: float\n",
    "    quality_score: float\n",
    "\n",
    "# Default configuration\n",
    "config = PostProcessConfig()\n",
    "print(\"✅ Configuration classes defined!\")\n",
    "print(f\"Default config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a06185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphologicalProcessor:\n",
    "    \"\"\"Advanced morphological operations for cardiac segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PostProcessConfig):\n",
    "        self.config = config\n",
    "        self.kernels = self._create_kernels()\n",
    "        \n",
    "    def _create_kernels(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Create morphological kernels for different operations\"\"\"\n",
    "        kernels = {}\n",
    "        \n",
    "        # Standard circular kernel\n",
    "        kernels['circular'] = cv2.getStructuringElement(\n",
    "            cv2.MORPH_ELLIPSE, \n",
    "            (self.config.kernel_size, self.config.kernel_size)\n",
    "        )\n",
    "        \n",
    "        # Cross-shaped kernel for connectivity\n",
    "        kernels['cross'] = cv2.getStructuringElement(\n",
    "            cv2.MORPH_CROSS, \n",
    "            (self.config.kernel_size, self.config.kernel_size)\n",
    "        )\n",
    "        \n",
    "        # Rectangular kernel for directional operations\n",
    "        kernels['rect'] = cv2.getStructuringElement(\n",
    "            cv2.MORPH_RECT, \n",
    "            (self.config.kernel_size, self.config.kernel_size)\n",
    "        )\n",
    "        \n",
    "        # Custom cardiac-specific kernel (elongated)\n",
    "        cardiac_kernel = np.zeros((7, 5), dtype=np.uint8)\n",
    "        cardiac_kernel[1:6, 1:4] = 1\n",
    "        kernels['cardiac'] = cardiac_kernel\n",
    "        \n",
    "        return kernels\n",
    "    \n",
    "    def erosion(self, mask: np.ndarray, kernel_type: str = 'circular') -> np.ndarray:\n",
    "        \"\"\"Apply erosion operation\"\"\"\n",
    "        kernel = self.kernels[kernel_type]\n",
    "        return cv2.erode(\n",
    "            mask.astype(np.uint8), \n",
    "            kernel, \n",
    "            iterations=self.config.erosion_iterations\n",
    "        )\n",
    "    \n",
    "    def dilation(self, mask: np.ndarray, kernel_type: str = 'circular') -> np.ndarray:\n",
    "        \"\"\"Apply dilation operation\"\"\"\n",
    "        kernel = self.kernels[kernel_type]\n",
    "        return cv2.dilate(\n",
    "            mask.astype(np.uint8), \n",
    "            kernel, \n",
    "            iterations=self.config.dilation_iterations\n",
    "        )\n",
    "    \n",
    "    def opening(self, mask: np.ndarray, kernel_type: str = 'circular') -> np.ndarray:\n",
    "        \"\"\"Apply opening operation (erosion followed by dilation)\"\"\"\n",
    "        kernel = self.kernels[kernel_type]\n",
    "        return cv2.morphologyEx(\n",
    "            mask.astype(np.uint8), \n",
    "            cv2.MORPH_OPEN, \n",
    "            kernel,\n",
    "            iterations=self.config.opening_iterations\n",
    "        )\n",
    "    \n",
    "    def closing(self, mask: np.ndarray, kernel_type: str = 'circular') -> np.ndarray:\n",
    "        \"\"\"Apply closing operation (dilation followed by erosion)\"\"\"\n",
    "        kernel = self.kernels[kernel_type]\n",
    "        return cv2.morphologyEx(\n",
    "            mask.astype(np.uint8), \n",
    "            cv2.MORPH_CLOSE, \n",
    "            kernel,\n",
    "            iterations=self.config.closing_iterations\n",
    "        )\n",
    "    \n",
    "    def gradient(self, mask: np.ndarray, kernel_type: str = 'circular') -> np.ndarray:\n",
    "        \"\"\"Apply morphological gradient (dilation - erosion)\"\"\"\n",
    "        kernel = self.kernels[kernel_type]\n",
    "        return cv2.morphologyEx(\n",
    "            mask.astype(np.uint8), \n",
    "            cv2.MORPH_GRADIENT, \n",
    "            kernel\n",
    "        )\n",
    "    \n",
    "    def tophat(self, mask: np.ndarray, kernel_type: str = 'circular') -> np.ndarray:\n",
    "        \"\"\"Apply top hat operation\"\"\"\n",
    "        kernel = self.kernels[kernel_type]\n",
    "        return cv2.morphologyEx(\n",
    "            mask.astype(np.uint8), \n",
    "            cv2.MORPH_TOPHAT, \n",
    "            kernel\n",
    "        )\n",
    "    \n",
    "    def blackhat(self, mask: np.ndarray, kernel_type: str = 'circular') -> np.ndarray:\n",
    "        \"\"\"Apply black hat operation\"\"\"\n",
    "        kernel = self.kernels[kernel_type]\n",
    "        return cv2.morphologyEx(\n",
    "            mask.astype(np.uint8), \n",
    "            cv2.MORPH_BLACKHAT, \n",
    "            kernel\n",
    "        )\n",
    "    \n",
    "    def adaptive_morphology(self, mask: np.ndarray, area_threshold: int = 1000) -> np.ndarray:\n",
    "        \"\"\"Apply adaptive morphological operations based on region size\"\"\"\n",
    "        # Use different kernel sizes based on connected component sizes\n",
    "        labels = measure.label(mask)\n",
    "        processed_mask = np.zeros_like(mask)\n",
    "        \n",
    "        for region in measure.regionprops(labels):\n",
    "            if region.area < area_threshold:\n",
    "                # Small regions: use smaller kernel\n",
    "                small_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "                region_mask = (labels == region.label).astype(np.uint8)\n",
    "                processed_region = cv2.morphologyEx(region_mask, cv2.MORPH_CLOSE, small_kernel)\n",
    "            else:\n",
    "                # Large regions: use larger kernel\n",
    "                large_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
    "                region_mask = (labels == region.label).astype(np.uint8)\n",
    "                processed_region = cv2.morphologyEx(region_mask, cv2.MORPH_OPEN, large_kernel)\n",
    "            \n",
    "            processed_mask += processed_region\n",
    "        \n",
    "        return (processed_mask > 0).astype(np.uint8)\n",
    "    \n",
    "    def multi_scale_morphology(self, mask: np.ndarray, scales: List[int] = [3, 5, 7]) -> np.ndarray:\n",
    "        \"\"\"Apply morphological operations at multiple scales\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for scale in scales:\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (scale, scale))\n",
    "            # Apply opening at each scale\n",
    "            opened = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_OPEN, kernel)\n",
    "            results.append(opened)\n",
    "        \n",
    "        # Combine results using majority voting\n",
    "        combined = np.stack(results, axis=-1)\n",
    "        return (np.sum(combined, axis=-1) > len(scales) // 2).astype(np.uint8)\n",
    "\n",
    "# Test the morphological processor\n",
    "morph_processor = MorphologicalProcessor(config)\n",
    "print(\"✅ Morphological Processor initialized!\")\n",
    "print(f\"Available kernels: {list(morph_processor.kernels.keys())}\")\n",
    "\n",
    "# Visualize kernels\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3))\n",
    "for i, (name, kernel) in enumerate(morph_processor.kernels.items()):\n",
    "    axes[i].imshow(kernel, cmap='gray')\n",
    "    axes[i].set_title(f'{name} kernel')\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectedComponentAnalyzer:\n",
    "    \"\"\"Advanced connected component analysis for cardiac segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PostProcessConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def remove_small_components(self, mask: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Remove small connected components\"\"\"\n",
    "        # Label connected components\n",
    "        labels = measure.label(mask, connectivity=self.config.connectivity)\n",
    "        properties = measure.regionprops(labels)\n",
    "        \n",
    "        # Filter components by size\n",
    "        filtered_mask = np.zeros_like(mask)\n",
    "        removed_components = 0\n",
    "        kept_components = 0\n",
    "        \n",
    "        for prop in properties:\n",
    "            if prop.area >= self.config.min_component_size:\n",
    "                filtered_mask[labels == prop.label] = 1\n",
    "                kept_components += 1\n",
    "            else:\n",
    "                removed_components += 1\n",
    "        \n",
    "        stats = {\n",
    "            'original_components': len(properties),\n",
    "            'kept_components': kept_components,\n",
    "            'removed_components': removed_components,\n",
    "            'size_threshold': self.config.min_component_size\n",
    "        }\n",
    "        \n",
    "        return filtered_mask.astype(np.uint8), stats\n",
    "    \n",
    "    def keep_largest_components(self, mask: np.ndarray, n_components: int = None) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Keep only the N largest connected components\"\"\"\n",
    "        if n_components is None:\n",
    "            n_components = self.config.max_component_count\n",
    "            \n",
    "        # Label and analyze components\n",
    "        labels = measure.label(mask, connectivity=self.config.connectivity)\n",
    "        properties = measure.regionprops(labels)\n",
    "        \n",
    "        if len(properties) == 0:\n",
    "            return mask, {'kept_components': 0, 'removed_components': 0}\n",
    "        \n",
    "        # Sort by area (largest first)\n",
    "        properties_sorted = sorted(properties, key=lambda x: x.area, reverse=True)\n",
    "        \n",
    "        # Keep only the largest N components\n",
    "        filtered_mask = np.zeros_like(mask)\n",
    "        for i, prop in enumerate(properties_sorted[:n_components]):\n",
    "            filtered_mask[labels == prop.label] = 1\n",
    "        \n",
    "        stats = {\n",
    "            'original_components': len(properties),\n",
    "            'kept_components': min(n_components, len(properties)),\n",
    "            'removed_components': max(0, len(properties) - n_components),\n",
    "            'largest_area': properties_sorted[0].area if properties_sorted else 0\n",
    "        }\n",
    "        \n",
    "        return filtered_mask.astype(np.uint8), stats\n",
    "    \n",
    "    def analyze_component_properties(self, mask: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"Analyze properties of connected components\"\"\"\n",
    "        labels = measure.label(mask, connectivity=self.config.connectivity)\n",
    "        properties = measure.regionprops(labels)\n",
    "        \n",
    "        if not properties:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        data = []\n",
    "        for prop in properties:\n",
    "            # Basic properties\n",
    "            area = prop.area\n",
    "            perimeter = prop.perimeter\n",
    "            centroid = prop.centroid\n",
    "            bbox = prop.bbox\n",
    "            \n",
    "            # Geometric properties\n",
    "            circularity = 4 * np.pi * area / (perimeter ** 2) if perimeter > 0 else 0\n",
    "            aspect_ratio = prop.major_axis_length / prop.minor_axis_length if prop.minor_axis_length > 0 else 0\n",
    "            solidity = prop.solidity\n",
    "            extent = prop.extent\n",
    "            \n",
    "            # Shape properties\n",
    "            eccentricity = prop.eccentricity\n",
    "            orientation = prop.orientation\n",
    "            \n",
    "            data.append({\n",
    "                'label': prop.label,\n",
    "                'area': area,\n",
    "                'perimeter': perimeter,\n",
    "                'centroid_y': centroid[0],\n",
    "                'centroid_x': centroid[1],\n",
    "                'bbox_min_row': bbox[0],\n",
    "                'bbox_min_col': bbox[1],\n",
    "                'bbox_max_row': bbox[2],\n",
    "                'bbox_max_col': bbox[3],\n",
    "                'circularity': circularity,\n",
    "                'aspect_ratio': aspect_ratio,\n",
    "                'solidity': solidity,\n",
    "                'extent': extent,\n",
    "                'eccentricity': eccentricity,\n",
    "                'orientation': orientation\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def filter_by_shape(self, mask: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Filter components based on shape criteria\"\"\"\n",
    "        labels = measure.label(mask, connectivity=self.config.connectivity)\n",
    "        properties = measure.regionprops(labels)\n",
    "        \n",
    "        filtered_mask = np.zeros_like(mask)\n",
    "        kept_count = 0\n",
    "        removed_count = 0\n",
    "        \n",
    "        for prop in properties:\n",
    "            # Calculate shape metrics\n",
    "            area = prop.area\n",
    "            perimeter = prop.perimeter\n",
    "            circularity = 4 * np.pi * area / (perimeter ** 2) if perimeter > 0 else 0\n",
    "            aspect_ratio = prop.major_axis_length / prop.minor_axis_length if prop.minor_axis_length > 0 else 0\n",
    "            \n",
    "            # Apply filters\n",
    "            valid_area = self.config.min_heart_area <= area <= self.config.max_heart_area\n",
    "            valid_aspect_ratio = self.config.aspect_ratio_range[0] <= aspect_ratio <= self.config.aspect_ratio_range[1]\n",
    "            valid_circularity = circularity >= self.config.circularity_threshold\n",
    "            \n",
    "            if valid_area and valid_aspect_ratio and valid_circularity:\n",
    "                filtered_mask[labels == prop.label] = 1\n",
    "                kept_count += 1\n",
    "            else:\n",
    "                removed_count += 1\n",
    "        \n",
    "        stats = {\n",
    "            'kept_components': kept_count,\n",
    "            'removed_components': removed_count,\n",
    "            'shape_criteria': {\n",
    "                'area_range': (self.config.min_heart_area, self.config.max_heart_area),\n",
    "                'aspect_ratio_range': self.config.aspect_ratio_range,\n",
    "                'circularity_threshold': self.config.circularity_threshold\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return filtered_mask.astype(np.uint8), stats\n",
    "    \n",
    "    def remove_border_components(self, mask: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Remove components touching the image border\"\"\"\n",
    "        # Use skimage's clear_border function\n",
    "        cleared_mask = clear_border(mask.astype(bool)).astype(np.uint8)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        original_components = len(measure.regionprops(measure.label(mask)))\n",
    "        remaining_components = len(measure.regionprops(measure.label(cleared_mask)))\n",
    "        \n",
    "        stats = {\n",
    "            'original_components': original_components,\n",
    "            'remaining_components': remaining_components,\n",
    "            'removed_components': original_components - remaining_components\n",
    "        }\n",
    "        \n",
    "        return cleared_mask, stats\n",
    "    \n",
    "    def watershed_separation(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Separate touching components using watershed algorithm\"\"\"\n",
    "        # Distance transform\n",
    "        distance = ndimage.distance_transform_edt(mask)\n",
    "        \n",
    "        # Find local maxima\n",
    "        local_maxima = peak_local_maxima(distance, min_distance=10, threshold_abs=0.3*distance.max())\n",
    "        markers = np.zeros_like(mask, dtype=int)\n",
    "        markers[tuple(local_maxima.T)] = np.arange(1, len(local_maxima) + 1)\n",
    "        \n",
    "        # Apply watershed\n",
    "        labels = watershed(-distance, markers, mask=mask)\n",
    "        \n",
    "        return (labels > 0).astype(np.uint8)\n",
    "\n",
    "# Test the connected component analyzer\n",
    "cc_analyzer = ConnectedComponentAnalyzer(config)\n",
    "print(\"✅ Connected Component Analyzer initialized!\")\n",
    "\n",
    "# Create a test mask with multiple components\n",
    "test_mask = np.zeros((200, 200), dtype=np.uint8)\n",
    "# Large component\n",
    "cv2.circle(test_mask, (100, 100), 40, 1, -1)\n",
    "# Small components (noise)\n",
    "cv2.circle(test_mask, (50, 50), 5, 1, -1)\n",
    "cv2.circle(test_mask, (150, 150), 3, 1, -1)\n",
    "cv2.circle(test_mask, (30, 170), 7, 1, -1)\n",
    "\n",
    "# Analyze components\n",
    "df = cc_analyzer.analyze_component_properties(test_mask)\n",
    "print(f\"Component analysis:\\n{df[['label', 'area', 'circularity', 'aspect_ratio']].round(3)}\")\n",
    "\n",
    "# Test filtering\n",
    "filtered_mask, stats = cc_analyzer.remove_small_components(test_mask)\n",
    "print(f\"Filtering stats: {stats}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].imshow(test_mask, cmap='gray')\n",
    "axes[0].set_title('Original Mask')\n",
    "axes[1].imshow(filtered_mask, cmap='gray')\n",
    "axes[1].set_title('Filtered Mask')\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnatomicalConstraintValidator:\n",
    "    \"\"\"Validate and enforce anatomical constraints for cardiac structures\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PostProcessConfig):\n",
    "        self.config = config\n",
    "        self.cardiac_anatomy = self._define_cardiac_anatomy()\n",
    "        \n",
    "    def _define_cardiac_anatomy(self) -> Dict:\n",
    "        \"\"\"Define anatomical constraints for cardiac structures\"\"\"\n",
    "        return {\n",
    "            'left_ventricle': {\n",
    "                'expected_area_range': (2000, 15000),\n",
    "                'expected_position': 'center-left',\n",
    "                'shape_constraints': {\n",
    "                    'circularity_min': 0.4,\n",
    "                    'aspect_ratio_range': (0.7, 1.5),\n",
    "                    'solidity_min': 0.8\n",
    "                }\n",
    "            },\n",
    "            'right_ventricle': {\n",
    "                'expected_area_range': (1500, 12000),\n",
    "                'expected_position': 'center-right',\n",
    "                'shape_constraints': {\n",
    "                    'circularity_min': 0.3,\n",
    "                    'aspect_ratio_range': (0.6, 2.0),\n",
    "                    'solidity_min': 0.7\n",
    "                }\n",
    "            },\n",
    "            'myocardium': {\n",
    "                'expected_area_range': (3000, 20000),\n",
    "                'expected_position': 'surrounding',\n",
    "                'shape_constraints': {\n",
    "                    'circularity_min': 0.2,\n",
    "                    'aspect_ratio_range': (0.5, 2.5),\n",
    "                    'solidity_min': 0.6\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def validate_cardiac_structure(self, mask: np.ndarray, structure_type: str = 'left_ventricle') -> Dict:\n",
    "        \"\"\"Validate a cardiac structure against anatomical constraints\"\"\"\n",
    "        if structure_type not in self.cardiac_anatomy:\n",
    "            raise ValueError(f\"Unknown cardiac structure: {structure_type}\")\n",
    "        \n",
    "        constraints = self.cardiac_anatomy[structure_type]\n",
    "        \n",
    "        # Analyze the mask\n",
    "        labels = measure.label(mask)\n",
    "        properties = measure.regionprops(labels)\n",
    "        \n",
    "        if not properties:\n",
    "            return {\n",
    "                'valid': False,\n",
    "                'reason': 'No components found',\n",
    "                'violations': ['empty_mask']\n",
    "            }\n",
    "        \n",
    "        # Find the largest component (assumed to be the main structure)\n",
    "        main_component = max(properties, key=lambda x: x.area)\n",
    "        \n",
    "        # Check constraints\n",
    "        violations = []\n",
    "        \n",
    "        # Area constraint\n",
    "        area = main_component.area\n",
    "        expected_area = constraints['expected_area_range']\n",
    "        if not (expected_area[0] <= area <= expected_area[1]):\n",
    "            violations.append(f'area_violation: {area} not in {expected_area}')\n",
    "        \n",
    "        # Shape constraints\n",
    "        shape_constraints = constraints['shape_constraints']\n",
    "        \n",
    "        # Circularity\n",
    "        perimeter = main_component.perimeter\n",
    "        circularity = 4 * np.pi * area / (perimeter ** 2) if perimeter > 0 else 0\n",
    "        if circularity < shape_constraints['circularity_min']:\n",
    "            violations.append(f'circularity_violation: {circularity:.3f} < {shape_constraints[\"circularity_min\"]}')\n",
    "        \n",
    "        # Aspect ratio\n",
    "        aspect_ratio = main_component.major_axis_length / main_component.minor_axis_length if main_component.minor_axis_length > 0 else 0\n",
    "        aspect_range = shape_constraints['aspect_ratio_range']\n",
    "        if not (aspect_range[0] <= aspect_ratio <= aspect_range[1]):\n",
    "            violations.append(f'aspect_ratio_violation: {aspect_ratio:.3f} not in {aspect_range}')\n",
    "        \n",
    "        # Solidity\n",
    "        solidity = main_component.solidity\n",
    "        if solidity < shape_constraints['solidity_min']:\n",
    "            violations.append(f'solidity_violation: {solidity:.3f} < {shape_constraints[\"solidity_min\"]}')\n",
    "        \n",
    "        return {\n",
    "            'valid': len(violations) == 0,\n",
    "            'violations': violations,\n",
    "            'metrics': {\n",
    "                'area': area,\n",
    "                'circularity': circularity,\n",
    "                'aspect_ratio': aspect_ratio,\n",
    "                'solidity': solidity,\n",
    "                'centroid': main_component.centroid\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def validate_multi_structure(self, masks: Dict[str, np.ndarray]) -> Dict:\n",
    "        \"\"\"Validate multiple cardiac structures and their relationships\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Validate each structure individually\n",
    "        for structure_name, mask in masks.items():\n",
    "            results[structure_name] = self.validate_cardiac_structure(mask, structure_name)\n",
    "        \n",
    "        # Check spatial relationships\n",
    "        spatial_violations = self._check_spatial_relationships(masks)\n",
    "        results['spatial_relationships'] = spatial_violations\n",
    "        \n",
    "        # Overall validation\n",
    "        all_valid = all(result['valid'] for result in results.values() if isinstance(result, dict) and 'valid' in result)\n",
    "        results['overall_valid'] = all_valid and len(spatial_violations) == 0\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _check_spatial_relationships(self, masks: Dict[str, np.ndarray]) -> List[str]:\n",
    "        \"\"\"Check spatial relationships between cardiac structures\"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        if 'left_ventricle' in masks and 'right_ventricle' in masks:\n",
    "            lv_props = measure.regionprops(measure.label(masks['left_ventricle']))\n",
    "            rv_props = measure.regionprops(measure.label(masks['right_ventricle']))\n",
    "            \n",
    "            if lv_props and rv_props:\n",
    "                lv_centroid = lv_props[0].centroid\n",
    "                rv_centroid = rv_props[0].centroid\n",
    "                \n",
    "                # Check distance between ventricles\n",
    "                distance = np.sqrt((lv_centroid[0] - rv_centroid[0])**2 + \n",
    "                                 (lv_centroid[1] - rv_centroid[1])**2)\n",
    "                \n",
    "                if distance > self.config.max_distance_between_structures:\n",
    "                    violations.append(f'ventricles_too_far: distance={distance:.2f}')\n",
    "                \n",
    "                # Check relative positions (LV should be roughly to the left of RV)\n",
    "                if lv_centroid[1] > rv_centroid[1]:  # LV x-coordinate > RV x-coordinate\n",
    "                    violations.append('ventricles_wrong_orientation: LV not left of RV')\n",
    "        \n",
    "        return violations\n",
    "    \n",
    "    def enforce_anatomical_constraints(self, mask: np.ndarray, structure_type: str = 'left_ventricle') -> np.ndarray:\n",
    "        \"\"\"Enforce anatomical constraints by modifying the mask\"\"\"\n",
    "        validation_result = self.validate_cardiac_structure(mask, structure_type)\n",
    "        \n",
    "        if validation_result['valid']:\n",
    "            return mask\n",
    "        \n",
    "        # Apply corrections based on violations\n",
    "        corrected_mask = mask.copy()\n",
    "        \n",
    "        for violation in validation_result['violations']:\n",
    "            if 'area_violation' in violation:\n",
    "                # Apply morphological operations to adjust area\n",
    "                if 'too small' in violation.lower():\n",
    "                    # Dilate to increase area\n",
    "                    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "                    corrected_mask = cv2.dilate(corrected_mask, kernel, iterations=2)\n",
    "                elif 'too large' in violation.lower():\n",
    "                    # Erode to decrease area\n",
    "                    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "                    corrected_mask = cv2.erode(corrected_mask, kernel, iterations=1)\n",
    "            \n",
    "            elif 'circularity_violation' in violation:\n",
    "                # Apply closing to improve circularity\n",
    "                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
    "                corrected_mask = cv2.morphologyEx(corrected_mask, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        return corrected_mask.astype(np.uint8)\n",
    "    \n",
    "    def get_anatomical_priors(self, image_shape: Tuple[int, int]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Generate anatomical priors for cardiac structures\"\"\"\n",
    "        height, width = image_shape\n",
    "        center_y, center_x = height // 2, width // 2\n",
    "        \n",
    "        priors = {}\n",
    "        \n",
    "        # Left ventricle prior (center-left)\n",
    "        lv_prior = np.zeros(image_shape, dtype=np.float32)\n",
    "        lv_center = (center_y, center_x - width // 6)\n",
    "        y_grid, x_grid = np.ogrid[:height, :width]\n",
    "        lv_mask = ((y_grid - lv_center[0])**2 + (x_grid - lv_center[1])**2) <= (width//8)**2\n",
    "        lv_prior[lv_mask] = 1.0\n",
    "        priors['left_ventricle'] = lv_prior\n",
    "        \n",
    "        # Right ventricle prior (center-right)\n",
    "        rv_prior = np.zeros(image_shape, dtype=np.float32)\n",
    "        rv_center = (center_y, center_x + width // 6)\n",
    "        rv_mask = ((y_grid - rv_center[0])**2 + (x_grid - rv_center[1])**2) <= (width//10)**2\n",
    "        rv_prior[rv_mask] = 1.0\n",
    "        priors['right_ventricle'] = rv_prior\n",
    "        \n",
    "        # Myocardium prior (surrounding)\n",
    "        myo_prior = np.zeros(image_shape, dtype=np.float32)\n",
    "        outer_mask = ((y_grid - center_y)**2 + (x_grid - center_x)**2) <= (width//4)**2\n",
    "        inner_mask = ((y_grid - center_y)**2 + (x_grid - center_x)**2) <= (width//8)**2\n",
    "        myo_prior[outer_mask & ~inner_mask] = 1.0\n",
    "        priors['myocardium'] = myo_prior\n",
    "        \n",
    "        return priors\n",
    "\n",
    "# Test the anatomical constraint validator\n",
    "constraint_validator = AnatomicalConstraintValidator(config)\n",
    "print(\"✅ Anatomical Constraint Validator initialized!\")\n",
    "print(f\"Supported cardiac structures: {list(constraint_validator.cardiac_anatomy.keys())}\")\n",
    "\n",
    "# Create test masks for validation\n",
    "test_image_shape = (256, 256)\n",
    "test_lv_mask = np.zeros(test_image_shape, dtype=np.uint8)\n",
    "cv2.circle(test_lv_mask, (128, 100), 50, 1, -1)  # Left ventricle\n",
    "\n",
    "test_rv_mask = np.zeros(test_image_shape, dtype=np.uint8)\n",
    "cv2.circle(test_rv_mask, (128, 180), 35, 1, -1)  # Right ventricle\n",
    "\n",
    "# Validate structures\n",
    "lv_validation = constraint_validator.validate_cardiac_structure(test_lv_mask, 'left_ventricle')\n",
    "rv_validation = constraint_validator.validate_cardiac_structure(test_rv_mask, 'right_ventricle')\n",
    "\n",
    "print(f\"LV validation: Valid={lv_validation['valid']}, Violations={len(lv_validation['violations'])}\")\n",
    "print(f\"RV validation: Valid={rv_validation['valid']}, Violations={len(rv_validation['violations'])}\")\n",
    "\n",
    "# Generate anatomical priors\n",
    "priors = constraint_validator.get_anatomical_priors(test_image_shape)\n",
    "\n",
    "# Visualize priors\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (name, prior) in enumerate(priors.items()):\n",
    "    axes[i].imshow(prior, cmap='hot', alpha=0.7)\n",
    "    axes[i].set_title(f'{name.replace(\"_\", \" \").title()} Prior')\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881cc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryRefinement:\n",
    "    \"\"\"Advanced boundary refinement and smoothing for cardiac segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PostProcessConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def gaussian_smoothing(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply Gaussian smoothing to boundaries\"\"\"\n",
    "        # Convert to float for smoothing\n",
    "        mask_float = mask.astype(np.float32)\n",
    "        \n",
    "        # Apply Gaussian filter\n",
    "        smoothed = filters.gaussian(mask_float, sigma=self.config.gaussian_sigma)\n",
    "        \n",
    "        # Threshold back to binary\n",
    "        return (smoothed > 0.5).astype(np.uint8)\n",
    "    \n",
    "    def contour_smoothing(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Smooth contours using approximation\"\"\"\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Create output mask\n",
    "        smoothed_mask = np.zeros_like(mask)\n",
    "        \n",
    "        for contour in contours:\n",
    "            # Approximate contour\n",
    "            epsilon = self.config.contour_epsilon * cv2.arcLength(contour, True)\n",
    "            approx_contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "            \n",
    "            # Fill the approximated contour\n",
    "            cv2.fillPoly(smoothed_mask, [approx_contour], 1)\n",
    "        \n",
    "        return smoothed_mask\n",
    "    \n",
    "    def active_contour_refinement(self, mask: np.ndarray, image: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"Refine boundaries using active contours (snakes)\"\"\"\n",
    "        from skimage.segmentation import active_contour\n",
    "        \n",
    "        # If no image provided, use the mask itself\n",
    "        if image is None:\n",
    "            image = mask.astype(np.float32)\n",
    "        \n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        refined_mask = np.zeros_like(mask)\n",
    "        \n",
    "        for contour in contours:\n",
    "            if len(contour) < 10:  # Skip very small contours\n",
    "                continue\n",
    "                \n",
    "            # Convert contour to snake format\n",
    "            snake = contour.squeeze().astype(np.float32)\n",
    "            \n",
    "            # Apply active contour\n",
    "            try:\n",
    "                refined_snake = active_contour(\n",
    "                    image, snake, \n",
    "                    alpha=0.015, beta=10, gamma=0.001,\n",
    "                    max_iterations=100\n",
    "                )\n",
    "                \n",
    "                # Convert back to contour and fill\n",
    "                refined_contour = refined_snake.astype(np.int32).reshape(-1, 1, 2)\n",
    "                cv2.fillPoly(refined_mask, [refined_contour], 1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # If active contour fails, use original contour\n",
    "                cv2.fillPoly(refined_mask, [contour], 1)\n",
    "        \n",
    "        return refined_mask\n",
    "    \n",
    "    def iterative_smoothing(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply iterative smoothing operations\"\"\"\n",
    "        current_mask = mask.copy()\n",
    "        \n",
    "        for i in range(self.config.smooth_iterations):\n",
    "            # Alternate between different smoothing methods\n",
    "            if i % 2 == 0:\n",
    "                current_mask = self.gaussian_smoothing(current_mask)\n",
    "            else:\n",
    "                current_mask = self.contour_smoothing(current_mask)\n",
    "        \n",
    "        return current_mask\n",
    "    \n",
    "    def edge_preserving_smoothing(self, mask: np.ndarray, image: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"Apply edge-preserving smoothing\"\"\"\n",
    "        if image is None:\n",
    "            # Use bilateral filter on the mask\n",
    "            mask_8bit = (mask * 255).astype(np.uint8)\n",
    "            smoothed = cv2.bilateralFilter(mask_8bit, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "            return (smoothed > 127).astype(np.uint8)\n",
    "        else:\n",
    "            # Use image gradients to preserve edges\n",
    "            # Compute gradients\n",
    "            grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "            \n",
    "            # Create edge-preserving weights\n",
    "            weights = np.exp(-gradient_magnitude / (2 * self.config.gaussian_sigma**2))\n",
    "            \n",
    "            # Apply weighted smoothing\n",
    "            mask_float = mask.astype(np.float32)\n",
    "            smoothed = filters.gaussian(mask_float * weights, sigma=self.config.gaussian_sigma)\n",
    "            \n",
    "            return (smoothed > 0.5).astype(np.uint8)\n",
    "    \n",
    "    def topology_preserving_smoothing(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply smoothing while preserving topology\"\"\"\n",
    "        # Use skimage's topology-preserving smoothing\n",
    "        from skimage.morphology import binary_opening, binary_closing\n",
    "        \n",
    "        # Apply conservative smoothing operations\n",
    "        smoothed = binary_opening(mask.astype(bool), morphology.disk(2))\n",
    "        smoothed = binary_closing(smoothed, morphology.disk(3))\n",
    "        \n",
    "        return smoothed.astype(np.uint8)\n",
    "    \n",
    "    def boundary_regularization(self, mask: np.ndarray, lambda_smooth: float = 0.1) -> np.ndarray:\n",
    "        \"\"\"Apply boundary regularization using energy minimization\"\"\"\n",
    "        # Convert to signed distance function\n",
    "        distance_inside = ndimage.distance_transform_edt(mask)\n",
    "        distance_outside = ndimage.distance_transform_edt(1 - mask)\n",
    "        signed_distance = distance_inside - distance_outside\n",
    "        \n",
    "        # Apply regularization (simplified version)\n",
    "        regularized = filters.gaussian(signed_distance, sigma=lambda_smooth)\n",
    "        \n",
    "        # Convert back to binary mask\n",
    "        return (regularized > 0).astype(np.uint8)\n",
    "    \n",
    "    def multi_scale_refinement(self, mask: np.ndarray, scales: List[float] = [0.5, 1.0, 2.0]) -> np.ndarray:\n",
    "        \"\"\"Apply multi-scale boundary refinement\"\"\"\n",
    "        refined_masks = []\n",
    "        \n",
    "        for scale in scales:\n",
    "            # Adjust sigma based on scale\n",
    "            scaled_sigma = self.config.gaussian_sigma * scale\n",
    "            \n",
    "            # Apply Gaussian smoothing\n",
    "            mask_float = mask.astype(np.float32)\n",
    "            smoothed = filters.gaussian(mask_float, sigma=scaled_sigma)\n",
    "            refined_masks.append((smoothed > 0.5).astype(np.uint8))\n",
    "        \n",
    "        # Combine results using majority voting\n",
    "        combined = np.stack(refined_masks, axis=-1)\n",
    "        return (np.sum(combined, axis=-1) > len(scales) // 2).astype(np.uint8)\n",
    "    \n",
    "    def calculate_boundary_smoothness(self, mask: np.ndarray) -> float:\n",
    "        \"\"\"Calculate boundary smoothness metric\"\"\"\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if not contours:\n",
    "            return 0.0\n",
    "        \n",
    "        smoothness_scores = []\n",
    "        \n",
    "        for contour in contours:\n",
    "            if len(contour) < 10:\n",
    "                continue\n",
    "                \n",
    "            # Calculate curvature along contour\n",
    "            points = contour.squeeze()\n",
    "            if len(points.shape) == 1:\n",
    "                continue\n",
    "                \n",
    "            # Compute differences\n",
    "            diff1 = np.diff(points, axis=0)\n",
    "            diff2 = np.diff(diff1, axis=0)\n",
    "            \n",
    "            # Calculate curvature\n",
    "            curvature = np.cross(diff1[:-1], diff2) / (np.linalg.norm(diff1[:-1], axis=1)**3 + 1e-8)\n",
    "            \n",
    "            # Smoothness is inverse of curvature variation\n",
    "            smoothness = 1.0 / (np.std(curvature) + 1e-8)\n",
    "            smoothness_scores.append(smoothness)\n",
    "        \n",
    "        return np.mean(smoothness_scores) if smoothness_scores else 0.0\n",
    "\n",
    "# Test the boundary refinement\n",
    "boundary_refiner = BoundaryRefinement(config)\n",
    "print(\"✅ Boundary Refinement initialized!\")\n",
    "\n",
    "# Create a test mask with rough boundaries\n",
    "test_mask = np.zeros((200, 200), dtype=np.uint8)\n",
    "# Create a rough heart-like shape\n",
    "points = np.array([\n",
    "    [100, 60], [120, 80], [140, 100], [130, 140], [100, 160],\n",
    "    [70, 140], [60, 100], [80, 80]\n",
    "], dtype=np.int32)\n",
    "cv2.fillPoly(test_mask, [points], 1)\n",
    "\n",
    "# Add some noise\n",
    "noise = np.random.random(test_mask.shape) > 0.95\n",
    "test_mask = test_mask | noise.astype(np.uint8)\n",
    "\n",
    "# Apply different smoothing methods\n",
    "gaussian_smoothed = boundary_refiner.gaussian_smoothing(test_mask)\n",
    "contour_smoothed = boundary_refiner.contour_smoothing(test_mask)\n",
    "iterative_smoothed = boundary_refiner.iterative_smoothing(test_mask)\n",
    "multi_scale_smoothed = boundary_refiner.multi_scale_refinement(test_mask)\n",
    "\n",
    "# Calculate smoothness metrics\n",
    "original_smoothness = boundary_refiner.calculate_boundary_smoothness(test_mask)\n",
    "gaussian_smoothness = boundary_refiner.calculate_boundary_smoothness(gaussian_smoothed)\n",
    "contour_smoothness = boundary_refiner.calculate_boundary_smoothness(contour_smoothed)\n",
    "\n",
    "print(f\"Boundary smoothness comparison:\")\n",
    "print(f\"Original: {original_smoothness:.3f}\")\n",
    "print(f\"Gaussian: {gaussian_smoothness:.3f}\")\n",
    "print(f\"Contour: {contour_smoothness:.3f}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "masks = [test_mask, gaussian_smoothed, contour_smoothed, \n",
    "         iterative_smoothed, multi_scale_smoothed, test_mask]\n",
    "titles = ['Original', 'Gaussian Smoothed', 'Contour Smoothed',\n",
    "          'Iterative Smoothed', 'Multi-scale Smoothed', 'Original Overlay']\n",
    "\n",
    "for i, (mask, title) in enumerate(zip(masks, titles)):\n",
    "    if i == 5:  # Overlay\n",
    "        axes[i].imshow(test_mask, cmap='gray', alpha=0.5)\n",
    "        axes[i].contour(gaussian_smoothed, colors='red', linewidths=2, alpha=0.8)\n",
    "        axes[i].set_title('Original vs Smoothed Boundary')\n",
    "    else:\n",
    "        axes[i].imshow(mask, cmap='gray')\n",
    "        axes[i].set_title(title)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessingPipeline:\n",
    "    \"\"\"Complete post-processing pipeline for cardiac segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PostProcessConfig):\n",
    "        self.config = config\n",
    "        self.morph_processor = MorphologicalProcessor(config)\n",
    "        self.cc_analyzer = ConnectedComponentAnalyzer(config)\n",
    "        self.constraint_validator = AnatomicalConstraintValidator(config)\n",
    "        self.boundary_refiner = BoundaryRefinement(config)\n",
    "        \n",
    "        # Pipeline steps configuration\n",
    "        self.pipeline_steps = [\n",
    "            'noise_removal',\n",
    "            'morphological_operations',\n",
    "            'connected_component_analysis',\n",
    "            'anatomical_validation',\n",
    "            'boundary_refinement',\n",
    "            'quality_assessment'\n",
    "        ]\n",
    "        \n",
    "    def process_single_mask(self, \n",
    "                           mask: np.ndarray, \n",
    "                           image: np.ndarray = None,\n",
    "                           structure_type: str = 'left_ventricle',\n",
    "                           steps: List[str] = None) -> Tuple[np.ndarray, PostProcessingMetrics]:\n",
    "        \"\"\"Process a single segmentation mask through the complete pipeline\"\"\"\n",
    "        \n",
    "        if steps is None:\n",
    "            steps = self.pipeline_steps\n",
    "            \n",
    "        processed_mask = mask.copy()\n",
    "        metrics = PostProcessingMetrics(\n",
    "            components_removed=0,\n",
    "            area_change=0.0,\n",
    "            boundary_smoothness=0.0,\n",
    "            confidence_improvement=0.0,\n",
    "            processing_time=0.0,\n",
    "            quality_score=0.0\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        original_area = np.sum(mask)\n",
    "        \n",
    "        # Step 1: Noise Removal\n",
    "        if 'noise_removal' in steps:\n",
    "            processed_mask = self._remove_noise(processed_mask)\n",
    "        \n",
    "        # Step 2: Morphological Operations\n",
    "        if 'morphological_operations' in steps:\n",
    "            processed_mask = self._apply_morphological_operations(processed_mask)\n",
    "        \n",
    "        # Step 3: Connected Component Analysis\n",
    "        if 'connected_component_analysis' in steps:\n",
    "            processed_mask, cc_stats = self.cc_analyzer.remove_small_components(processed_mask)\n",
    "            processed_mask, _ = self.cc_analyzer.keep_largest_components(processed_mask)\n",
    "            metrics.components_removed = cc_stats['removed_components']\n",
    "        \n",
    "        # Step 4: Anatomical Validation\n",
    "        if 'anatomical_validation' in steps:\n",
    "            processed_mask = self.constraint_validator.enforce_anatomical_constraints(\n",
    "                processed_mask, structure_type\n",
    "            )\n",
    "        \n",
    "        # Step 5: Boundary Refinement\n",
    "        if 'boundary_refinement' in steps:\n",
    "            processed_mask = self.boundary_refiner.iterative_smoothing(processed_mask)\n",
    "            metrics.boundary_smoothness = self.boundary_refiner.calculate_boundary_smoothness(processed_mask)\n",
    "        \n",
    "        # Step 6: Quality Assessment\n",
    "        if 'quality_assessment' in steps:\n",
    "            metrics.quality_score = self._calculate_quality_score(mask, processed_mask)\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        final_area = np.sum(processed_mask)\n",
    "        metrics.area_change = (final_area - original_area) / original_area if original_area > 0 else 0\n",
    "        metrics.processing_time = time.time() - start_time\n",
    "        \n",
    "        return processed_mask, metrics\n",
    "    \n",
    "    def process_multi_structure(self, \n",
    "                               masks: Dict[str, np.ndarray],\n",
    "                               image: np.ndarray = None) -> Tuple[Dict[str, np.ndarray], Dict]:\n",
    "        \"\"\"Process multiple cardiac structures with inter-structure constraints\"\"\"\n",
    "        \n",
    "        processed_masks = {}\n",
    "        all_metrics = {}\n",
    "        \n",
    "        # Process each structure individually\n",
    "        for structure_name, mask in masks.items():\n",
    "            processed_mask, metrics = self.process_single_mask(\n",
    "                mask, image, structure_name\n",
    "            )\n",
    "            processed_masks[structure_name] = processed_mask\n",
    "            all_metrics[structure_name] = metrics\n",
    "        \n",
    "        # Apply inter-structure constraints\n",
    "        processed_masks = self._apply_inter_structure_constraints(processed_masks)\n",
    "        \n",
    "        # Validate spatial relationships\n",
    "        spatial_validation = self.constraint_validator.validate_multi_structure(processed_masks)\n",
    "        all_metrics['spatial_validation'] = spatial_validation\n",
    "        \n",
    "        return processed_masks, all_metrics\n",
    "    \n",
    "    def _remove_noise(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Remove noise from segmentation mask\"\"\"\n",
    "        # Remove small isolated pixels\n",
    "        mask = self.morph_processor.opening(mask, 'circular')\n",
    "        \n",
    "        # Remove components touching border\n",
    "        mask, _ = self.cc_analyzer.remove_border_components(mask)\n",
    "        \n",
    "        # Apply median filter to remove salt-and-pepper noise\n",
    "        mask = cv2.medianBlur(mask.astype(np.uint8), 3)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def _apply_morphological_operations(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply sequence of morphological operations\"\"\"\n",
    "        # Opening to remove small noise\n",
    "        mask = self.morph_processor.opening(mask, 'circular')\n",
    "        \n",
    "        # Closing to fill small holes\n",
    "        mask = self.morph_processor.closing(mask, 'circular')\n",
    "        \n",
    "        # Adaptive morphology based on component size\n",
    "        mask = self.morph_processor.adaptive_morphology(mask)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def _apply_inter_structure_constraints(self, masks: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Apply constraints between different cardiac structures\"\"\"\n",
    "        processed_masks = masks.copy()\n",
    "        \n",
    "        # Ensure no overlap between ventricles\n",
    "        if 'left_ventricle' in masks and 'right_ventricle' in masks:\n",
    "            lv_mask = processed_masks['left_ventricle']\n",
    "            rv_mask = processed_masks['right_ventricle']\n",
    "            \n",
    "            # Remove overlapping regions\n",
    "            overlap = lv_mask & rv_mask\n",
    "            if np.sum(overlap) > 0:\n",
    "                # Assign overlap to the structure with higher confidence\n",
    "                # For simplicity, assign to left ventricle\n",
    "                processed_masks['right_ventricle'] = rv_mask & ~overlap\n",
    "        \n",
    "        # Ensure myocardium surrounds ventricles\n",
    "        if 'myocardium' in masks and ('left_ventricle' in masks or 'right_ventricle' in masks):\n",
    "            myo_mask = processed_masks['myocardium']\n",
    "            ventricle_masks = []\n",
    "            \n",
    "            if 'left_ventricle' in masks:\n",
    "                ventricle_masks.append(processed_masks['left_ventricle'])\n",
    "            if 'right_ventricle' in masks:\n",
    "                ventricle_masks.append(processed_masks['right_ventricle'])\n",
    "            \n",
    "            # Combine ventricle masks\n",
    "            combined_ventricles = np.zeros_like(myo_mask)\n",
    "            for v_mask in ventricle_masks:\n",
    "                combined_ventricles = combined_ventricles | v_mask\n",
    "            \n",
    "            # Ensure myocardium doesn't overlap with ventricles\n",
    "            processed_masks['myocardium'] = myo_mask & ~combined_ventricles\n",
    "        \n",
    "        return processed_masks\n",
    "    \n",
    "    def _calculate_quality_score(self, original_mask: np.ndarray, processed_mask: np.ndarray) -> float:\n",
    "        \"\"\"Calculate overall quality score for post-processing\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Area preservation score\n",
    "        original_area = np.sum(original_mask)\n",
    "        processed_area = np.sum(processed_mask)\n",
    "        area_ratio = processed_area / original_area if original_area > 0 else 0\n",
    "        area_score = 1.0 - abs(1.0 - area_ratio)  # Closer to 1.0 is better\n",
    "        scores.append(area_score)\n",
    "        \n",
    "        # Boundary smoothness score\n",
    "        smoothness = self.boundary_refiner.calculate_boundary_smoothness(processed_mask)\n",
    "        smoothness_score = min(smoothness / 10.0, 1.0)  # Normalize\n",
    "        scores.append(smoothness_score)\n",
    "        \n",
    "        # Connectivity score (prefer fewer components)\n",
    "        labels = measure.label(processed_mask)\n",
    "        n_components = len(measure.regionprops(labels))\n",
    "        connectivity_score = max(0, 1.0 - (n_components - 1) * 0.2)\n",
    "        scores.append(connectivity_score)\n",
    "        \n",
    "        # Compactness score\n",
    "        if processed_area > 0:\n",
    "            contours, _ = cv2.findContours(processed_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if contours:\n",
    "                perimeter = cv2.arcLength(contours[0], True)\n",
    "                compactness = 4 * np.pi * processed_area / (perimeter ** 2) if perimeter > 0 else 0\n",
    "                compactness_score = min(compactness, 1.0)\n",
    "                scores.append(compactness_score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def batch_process(self, \n",
    "                     masks: List[np.ndarray], \n",
    "                     images: List[np.ndarray] = None,\n",
    "                     structure_types: List[str] = None) -> Tuple[List[np.ndarray], List[PostProcessingMetrics]]:\n",
    "        \"\"\"Process multiple masks in batch\"\"\"\n",
    "        \n",
    "        if images is None:\n",
    "            images = [None] * len(masks)\n",
    "        if structure_types is None:\n",
    "            structure_types = ['left_ventricle'] * len(masks)\n",
    "        \n",
    "        processed_masks = []\n",
    "        all_metrics = []\n",
    "        \n",
    "        for i, (mask, image, structure_type) in enumerate(zip(masks, images, structure_types)):\n",
    "            try:\n",
    "                processed_mask, metrics = self.process_single_mask(mask, image, structure_type)\n",
    "                processed_masks.append(processed_mask)\n",
    "                all_metrics.append(metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing mask {i}: {e}\")\n",
    "                processed_masks.append(mask)  # Return original on error\n",
    "                all_metrics.append(PostProcessingMetrics(0, 0, 0, 0, 0, 0))\n",
    "        \n",
    "        return processed_masks, all_metrics\n",
    "    \n",
    "    def get_pipeline_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of pipeline configuration and capabilities\"\"\"\n",
    "        return {\n",
    "            'pipeline_steps': self.pipeline_steps,\n",
    "            'morphological_operations': list(self.morph_processor.kernels.keys()),\n",
    "            'supported_structures': list(self.constraint_validator.cardiac_anatomy.keys()),\n",
    "            'config': self.config.__dict__\n",
    "        }\n",
    "\n",
    "# Test the complete post-processing pipeline\n",
    "import time\n",
    "pipeline = PostProcessingPipeline(config)\n",
    "print(\"✅ Post-processing Pipeline initialized!\")\n",
    "print(\"Pipeline steps:\", pipeline.pipeline_steps)\n",
    "\n",
    "# Create test data\n",
    "test_mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "cv2.circle(test_mask, (128, 128), 60, 1, -1)  # Main structure\n",
    "cv2.circle(test_mask, (50, 50), 5, 1, -1)     # Small noise\n",
    "cv2.circle(test_mask, (200, 200), 3, 1, -1)   # Small noise\n",
    "\n",
    "# Add some boundary roughness\n",
    "noise_mask = np.random.random(test_mask.shape) > 0.98\n",
    "boundary_noise = test_mask & noise_mask.astype(np.uint8)\n",
    "test_mask = test_mask | boundary_noise\n",
    "\n",
    "# Process single mask\n",
    "processed_mask, metrics = pipeline.process_single_mask(test_mask, structure_type='left_ventricle')\n",
    "\n",
    "print(f\"Processing Results:\")\n",
    "print(f\"  Components removed: {metrics.components_removed}\")\n",
    "print(f\"  Area change: {metrics.area_change:.3f}\")\n",
    "print(f\"  Boundary smoothness: {metrics.boundary_smoothness:.3f}\")\n",
    "print(f\"  Quality score: {metrics.quality_score:.3f}\")\n",
    "print(f\"  Processing time: {metrics.processing_time:.3f}s\")\n",
    "\n",
    "# Test multi-structure processing\n",
    "test_masks = {\n",
    "    'left_ventricle': test_mask,\n",
    "    'right_ventricle': np.roll(test_mask, 50, axis=1)  # Shifted version\n",
    "}\n",
    "\n",
    "processed_multi, multi_metrics = pipeline.process_multi_structure(test_masks)\n",
    "\n",
    "print(f\"\\nMulti-structure processing:\")\n",
    "for structure, metrics in multi_metrics.items():\n",
    "    if isinstance(metrics, PostProcessingMetrics):\n",
    "        print(f\"  {structure}: Quality={metrics.quality_score:.3f}, Time={metrics.processing_time:.3f}s\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].imshow(test_mask, cmap='gray')\n",
    "axes[0, 0].set_title('Original Mask')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(processed_mask, cmap='gray')\n",
    "axes[0, 1].set_title('Post-processed Mask')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Overlay comparison\n",
    "axes[1, 0].imshow(test_mask, cmap='gray', alpha=0.5)\n",
    "axes[1, 0].contour(processed_mask, colors='red', linewidths=2)\n",
    "axes[1, 0].set_title('Boundary Comparison')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Multi-structure result\n",
    "multi_combined = processed_multi['left_ventricle'] + processed_multi['right_ventricle'] * 2\n",
    "axes[1, 1].imshow(multi_combined, cmap='viridis')\n",
    "axes[1, 1].set_title('Multi-structure Result')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcessingVisualizer:\n",
    "    \"\"\"Advanced visualization tools for post-processing analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.color_maps = {\n",
    "            'original': 'Blues',\n",
    "            'processed': 'Reds',\n",
    "            'difference': 'RdBu',\n",
    "            'overlay': 'viridis'\n",
    "        }\n",
    "    \n",
    "    def plot_processing_comparison(self, \n",
    "                                 original_mask: np.ndarray,\n",
    "                                 processed_mask: np.ndarray,\n",
    "                                 image: np.ndarray = None,\n",
    "                                 title: str = \"Post-processing Comparison\"):\n",
    "        \"\"\"Create comprehensive comparison visualization\"\"\"\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Original mask\n",
    "        plt.subplot(3, 3, 1)\n",
    "        plt.imshow(original_mask, cmap='gray')\n",
    "        plt.title('Original Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Processed mask\n",
    "        plt.subplot(3, 3, 2)\n",
    "        plt.imshow(processed_mask, cmap='gray')\n",
    "        plt.title('Processed Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Difference\n",
    "        plt.subplot(3, 3, 3)\n",
    "        difference = processed_mask.astype(int) - original_mask.astype(int)\n",
    "        plt.imshow(difference, cmap='RdBu', vmin=-1, vmax=1)\n",
    "        plt.title('Difference (Red: Added, Blue: Removed)')\n",
    "        plt.colorbar(shrink=0.8)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Overlay with original image\n",
    "        if image is not None:\n",
    "            plt.subplot(3, 3, 4)\n",
    "            plt.imshow(image, cmap='gray', alpha=0.7)\n",
    "            plt.contour(original_mask, colors='blue', linewidths=2, alpha=0.8)\n",
    "            plt.contour(processed_mask, colors='red', linewidths=2, alpha=0.8)\n",
    "            plt.title('Overlay on Original Image')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # Boundary comparison\n",
    "        plt.subplot(3, 3, 5)\n",
    "        # Find contours\n",
    "        orig_contours, _ = cv2.findContours(original_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        proc_contours, _ = cv2.findContours(processed_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        boundary_img = np.zeros((*original_mask.shape, 3), dtype=np.uint8)\n",
    "        if orig_contours:\n",
    "            cv2.drawContours(boundary_img, orig_contours, -1, (255, 0, 0), 2)  # Blue\n",
    "        if proc_contours:\n",
    "            cv2.drawContours(boundary_img, proc_contours, -1, (0, 255, 0), 2)  # Green\n",
    "        \n",
    "        plt.imshow(boundary_img)\n",
    "        plt.title('Boundary Comparison (Blue: Original, Green: Processed)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Morphological analysis\n",
    "        plt.subplot(3, 3, 6)\n",
    "        # Apply morphological gradient to show boundaries\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "        orig_gradient = cv2.morphologyEx(original_mask, cv2.MORPH_GRADIENT, kernel)\n",
    "        proc_gradient = cv2.morphologyEx(processed_mask, cv2.MORPH_GRADIENT, kernel)\n",
    "        \n",
    "        combined_gradient = np.stack([orig_gradient, proc_gradient, np.zeros_like(orig_gradient)], axis=-1)\n",
    "        plt.imshow(combined_gradient)\n",
    "        plt.title('Morphological Gradient (Red: Original, Green: Processed)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Area analysis\n",
    "        plt.subplot(3, 3, 7)\n",
    "        areas = [np.sum(original_mask), np.sum(processed_mask)]\n",
    "        labels = ['Original', 'Processed']\n",
    "        colors = ['lightblue', 'lightcoral']\n",
    "        bars = plt.bar(labels, areas, color=colors)\n",
    "        plt.title('Area Comparison')\n",
    "        plt.ylabel('Area (pixels)')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, area in zip(bars, areas):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(areas)*0.01,\n",
    "                    f'{area}', ha='center', va='bottom')\n",
    "        \n",
    "        # Component analysis\n",
    "        plt.subplot(3, 3, 8)\n",
    "        orig_labels = measure.label(original_mask)\n",
    "        proc_labels = measure.label(processed_mask)\n",
    "        orig_components = len(measure.regionprops(orig_labels))\n",
    "        proc_components = len(measure.regionprops(proc_labels))\n",
    "        \n",
    "        components = [orig_components, proc_components]\n",
    "        bars = plt.bar(labels, components, color=colors)\n",
    "        plt.title('Connected Components')\n",
    "        plt.ylabel('Number of Components')\n",
    "        \n",
    "        for bar, comp in zip(bars, components):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(components)*0.05,\n",
    "                    f'{comp}', ha='center', va='bottom')\n",
    "        \n",
    "        # Quality metrics\n",
    "        plt.subplot(3, 3, 9)\n",
    "        # Calculate various quality metrics\n",
    "        boundary_refiner = BoundaryRefinement(PostProcessConfig())\n",
    "        orig_smoothness = boundary_refiner.calculate_boundary_smoothness(original_mask)\n",
    "        proc_smoothness = boundary_refiner.calculate_boundary_smoothness(processed_mask)\n",
    "        \n",
    "        metrics = ['Smoothness', 'Components']\n",
    "        orig_values = [orig_smoothness, orig_components]\n",
    "        proc_values = [proc_smoothness, proc_components]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, orig_values, width, label='Original', color='lightblue')\n",
    "        plt.bar(x + width/2, proc_values, width, label='Processed', color='lightcoral')\n",
    "        \n",
    "        plt.xlabel('Metrics')\n",
    "        plt.title('Quality Metrics Comparison')\n",
    "        plt.xticks(x, metrics)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_pipeline_metrics(self, metrics_list: List[PostProcessingMetrics], titles: List[str] = None):\n",
    "        \"\"\"Plot comprehensive metrics from pipeline processing\"\"\"\n",
    "        \n",
    "        if titles is None:\n",
    "            titles = [f'Sample {i+1}' for i in range(len(metrics_list))]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        # Extract metrics\n",
    "        components_removed = [m.components_removed for m in metrics_list]\n",
    "        area_changes = [m.area_change for m in metrics_list]\n",
    "        boundary_smoothness = [m.boundary_smoothness for m in metrics_list]\n",
    "        processing_times = [m.processing_time for m in metrics_list]\n",
    "        quality_scores = [m.quality_score for m in metrics_list]\n",
    "        \n",
    "        # Components removed\n",
    "        axes[0, 0].bar(range(len(components_removed)), components_removed, color='lightcoral')\n",
    "        axes[0, 0].set_title('Components Removed')\n",
    "        axes[0, 0].set_xlabel('Sample')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "        axes[0, 0].set_xticks(range(len(titles)))\n",
    "        axes[0, 0].set_xticklabels(titles, rotation=45)\n",
    "        \n",
    "        # Area changes\n",
    "        axes[0, 1].bar(range(len(area_changes)), area_changes, \n",
    "                      color=['green' if x >= 0 else 'red' for x in area_changes])\n",
    "        axes[0, 1].set_title('Area Changes')\n",
    "        axes[0, 1].set_xlabel('Sample')\n",
    "        axes[0, 1].set_ylabel('Relative Change')\\n        axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[0, 1].set_xticks(range(len(titles)))\n",
    "        axes[0, 1].set_xticklabels(titles, rotation=45)\n",
    "        \n",
    "        # Boundary smoothness\n",
    "        axes[0, 2].bar(range(len(boundary_smoothness)), boundary_smoothness, color='lightblue')\n",
    "        axes[0, 2].set_title('Boundary Smoothness')\n",
    "        axes[0, 2].set_xlabel('Sample')\n",
    "        axes[0, 2].set_ylabel('Smoothness Score')\n",
    "        axes[0, 2].set_xticks(range(len(titles)))\n",
    "        axes[0, 2].set_xticklabels(titles, rotation=45)\n",
    "        \n",
    "        # Processing times\n",
    "        axes[1, 0].bar(range(len(processing_times)), processing_times, color='orange')\n",
    "        axes[1, 0].set_title('Processing Times')\n",
    "        axes[1, 0].set_xlabel('Sample')\n",
    "        axes[1, 0].set_ylabel('Time (seconds)')\n",
    "        axes[1, 0].set_xticks(range(len(titles)))\n",
    "        axes[1, 0].set_xticklabels(titles, rotation=45)\n",
    "        \n",
    "        # Quality scores\n",
    "        axes[1, 1].bar(range(len(quality_scores)), quality_scores, color='lightgreen')\n",
    "        axes[1, 1].set_title('Quality Scores')\n",
    "        axes[1, 1].set_xlabel('Sample')\n",
    "        axes[1, 1].set_ylabel('Quality Score (0-1)')\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        axes[1, 1].set_xticks(range(len(titles)))\n",
    "        axes[1, 1].set_xticklabels(titles, rotation=45)\n",
    "        \n",
    "        # Summary scatter plot\n",
    "        axes[1, 2].scatter(quality_scores, processing_times, c=boundary_smoothness, \n",
    "                          cmap='viridis', s=100, alpha=0.7)\n",
    "        axes[1, 2].set_xlabel('Quality Score')\n",
    "        axes[1, 2].set_ylabel('Processing Time (s)')\n",
    "        axes[1, 2].set_title('Quality vs Performance')\n",
    "        \n",
    "        # Add colorbar for smoothness\n",
    "        scatter = axes[1, 2].scatter(quality_scores, processing_times, c=boundary_smoothness, \n",
    "                                   cmap='viridis', s=100, alpha=0.7)\n",
    "        plt.colorbar(scatter, ax=axes[1, 2], label='Boundary Smoothness')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_interactive_comparison(self, \n",
    "                                    original_mask: np.ndarray,\n",
    "                                    processed_mask: np.ndarray,\n",
    "                                    image: np.ndarray = None):\n",
    "        \"\"\"Create interactive Plotly visualization\"\"\"\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Original Mask', 'Processed Mask', 'Difference', 'Overlay'),\n",
    "            specs=[[{\"type\": \"image\"}, {\"type\": \"image\"}],\n",
    "                   [{\"type\": \"image\"}, {\"type\": \"image\"}]]\n",
    "        )\n",
    "        \n",
    "        # Original mask\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(z=original_mask, colorscale='Blues', showscale=False),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Processed mask\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(z=processed_mask, colorscale='Reds', showscale=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Difference\n",
    "        difference = processed_mask.astype(int) - original_mask.astype(int)\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(z=difference, colorscale='RdBu', zmid=0, showscale=True),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Overlay\n",
    "        if image is not None:\n",
    "            overlay = image * 0.7 + (original_mask * 0.15) + (processed_mask * 0.15)\n",
    "        else:\n",
    "            overlay = original_mask * 0.5 + processed_mask * 0.5\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Heatmap(z=overlay, colorscale='Viridis', showscale=False),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Interactive Post-processing Comparison\",\n",
    "            height=800,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Remove axis ticks\n",
    "        fig.update_xaxes(showticklabels=False)\n",
    "        fig.update_yaxes(showticklabels=False)\n",
    "        \n",
    "        return fig\n",
    "\n",
    "class QualityAssessment:\n",
    "    \"\"\"Comprehensive quality assessment for post-processed segmentations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'dice_coefficient': self._dice_coefficient,\n",
    "            'jaccard_index': self._jaccard_index,\n",
    "            'hausdorff_distance': self._hausdorff_distance,\n",
    "            'mean_surface_distance': self._mean_surface_distance,\n",
    "            'boundary_smoothness': self._boundary_smoothness,\n",
    "            'topological_consistency': self._topological_consistency\n",
    "        }\n",
    "    \n",
    "    def _dice_coefficient(self, mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Dice coefficient\"\"\"\n",
    "        intersection = np.sum(mask1 & mask2)\n",
    "        return 2.0 * intersection / (np.sum(mask1) + np.sum(mask2) + 1e-8)\n",
    "    \n",
    "    def _jaccard_index(self, mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Jaccard index\"\"\"\n",
    "        intersection = np.sum(mask1 & mask2)\n",
    "        union = np.sum(mask1 | mask2)\n",
    "        return intersection / (union + 1e-8)\n",
    "    \n",
    "    def _hausdorff_distance(self, mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Hausdorff distance\"\"\"\n",
    "        from scipy.spatial.distance import directed_hausdorff\n",
    "        \n",
    "        # Get boundary points\n",
    "        contours1, _ = cv2.findContours(mask1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours2, _ = cv2.findContours(mask2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if not contours1 or not contours2:\n",
    "            return float('inf')\n",
    "        \n",
    "        points1 = contours1[0].squeeze()\n",
    "        points2 = contours2[0].squeeze()\n",
    "        \n",
    "        if len(points1.shape) == 1:\n",
    "            points1 = points1.reshape(1, -1)\n",
    "        if len(points2.shape) == 1:\n",
    "            points2 = points2.reshape(1, -1)\n",
    "        \n",
    "        return max(directed_hausdorff(points1, points2)[0], \n",
    "                  directed_hausdorff(points2, points1)[0])\n",
    "    \n",
    "    def _mean_surface_distance(self, mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate mean surface distance\"\"\"\n",
    "        # Distance transforms\n",
    "        dist1 = ndimage.distance_transform_edt(~mask1.astype(bool))\n",
    "        dist2 = ndimage.distance_transform_edt(~mask2.astype(bool))\n",
    "        \n",
    "        # Surface points\n",
    "        surface1 = mask1 & ~ndimage.binary_erosion(mask1)\n",
    "        surface2 = mask2 & ~ndimage.binary_erosion(mask2)\n",
    "        \n",
    "        if not np.any(surface1) or not np.any(surface2):\n",
    "            return float('inf')\n",
    "        \n",
    "        # Mean distances\n",
    "        mean_dist1to2 = np.mean(dist2[surface1])\n",
    "        mean_dist2to1 = np.mean(dist1[surface2])\n",
    "        \n",
    "        return (mean_dist1to2 + mean_dist2to1) / 2.0\n",
    "    \n",
    "    def _boundary_smoothness(self, mask: np.ndarray) -> float:\n",
    "        \"\"\"Calculate boundary smoothness\"\"\"\n",
    "        boundary_refiner = BoundaryRefinement(PostProcessConfig())\n",
    "        return boundary_refiner.calculate_boundary_smoothness(mask)\n",
    "    \n",
    "    def _topological_consistency(self, mask: np.ndarray) -> float:\n",
    "        \"\"\"Calculate topological consistency (Euler number)\"\"\"\n",
    "        # Calculate Euler number (connected components - holes)\n",
    "        labels = measure.label(mask)\n",
    "        n_components = len(measure.regionprops(labels))\n",
    "        \n",
    "        # Approximate holes by looking at the background components inside mask\n",
    "        filled_mask = ndimage.binary_fill_holes(mask)\n",
    "        holes = np.sum(filled_mask) - np.sum(mask)\n",
    "        \n",
    "        # Simple topological score\n",
    "        return max(0, 1.0 - abs(n_components - 1) * 0.2 - holes * 0.001)\n",
    "    \n",
    "    def evaluate_post_processing(self, \n",
    "                               original_mask: np.ndarray, \n",
    "                               processed_mask: np.ndarray,\n",
    "                               ground_truth: np.ndarray = None) -> Dict[str, float]:\n",
    "        \"\"\"Comprehensive evaluation of post-processing results\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Compare processed vs original\n",
    "        for metric_name, metric_func in self.metrics.items():\n",
    "            if metric_name in ['boundary_smoothness', 'topological_consistency']:\n",
    "                # These metrics are calculated on single masks\n",
    "                results[f'original_{metric_name}'] = metric_func(original_mask)\n",
    "                results[f'processed_{metric_name}'] = metric_func(processed_mask)\n",
    "                results[f'{metric_name}_improvement'] = (\n",
    "                    results[f'processed_{metric_name}'] - results[f'original_{metric_name}']\n",
    "                )\n",
    "            else:\n",
    "                # These metrics compare two masks\n",
    "                results[f'original_vs_processed_{metric_name}'] = metric_func(original_mask, processed_mask)\n",
    "        \n",
    "        # If ground truth is available, compare both versions against it\n",
    "        if ground_truth is not None:\n",
    "            for metric_name, metric_func in self.metrics.items():\n",
    "                if metric_name not in ['boundary_smoothness', 'topological_consistency']:\n",
    "                    results[f'original_vs_gt_{metric_name}'] = metric_func(original_mask, ground_truth)\n",
    "                    results[f'processed_vs_gt_{metric_name}'] = metric_func(processed_mask, ground_truth)\n",
    "                    \n",
    "                    # Calculate improvement\n",
    "                    improvement = (results[f'processed_vs_gt_{metric_name}'] - \n",
    "                                 results[f'original_vs_gt_{metric_name}'])\n",
    "                    results[f'gt_{metric_name}_improvement'] = improvement\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the visualization and quality assessment tools\n",
    "visualizer = PostProcessingVisualizer()\n",
    "quality_assessor = QualityAssessment()\n",
    "\n",
    "print(\"✅ Visualization and Quality Assessment tools initialized!\")\n",
    "\n",
    "# Create test data with different levels of processing\n",
    "test_masks = []\n",
    "processed_masks = []\n",
    "metrics_list = []\n",
    "\n",
    "for i in range(3):\n",
    "    # Create test mask with varying noise levels\n",
    "    test_mask = np.zeros((200, 200), dtype=np.uint8)\n",
    "    cv2.circle(test_mask, (100, 100), 50 + i*10, 1, -1)\n",
    "    \n",
    "    # Add noise\n",
    "    noise_level = (i + 1) * 0.02\n",
    "    noise = np.random.random(test_mask.shape) > (1 - noise_level)\n",
    "    test_mask = test_mask | noise.astype(np.uint8)\n",
    "    \n",
    "    # Process mask\n",
    "    processed_mask, metrics = pipeline.process_single_mask(test_mask)\n",
    "    \n",
    "    test_masks.append(test_mask)\n",
    "    processed_masks.append(processed_mask)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "# Test quality assessment\n",
    "print(\"\\nQuality Assessment Results:\")\n",
    "for i, (orig, proc) in enumerate(zip(test_masks, processed_masks)):\n",
    "    quality_results = quality_assessor.evaluate_post_processing(orig, proc)\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    for metric, value in quality_results.items():\n",
    "        if 'improvement' in metric:\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Plot pipeline metrics\n",
    "visualizer.plot_pipeline_metrics(metrics_list, [f'Sample {i+1}' for i in range(3)])\n",
    "\n",
    "# Show detailed comparison for first sample\n",
    "visualizer.plot_processing_comparison(test_masks[0], processed_masks[0], \n",
    "                                    title=\"Detailed Post-processing Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e7f62",
   "metadata": {},
   "source": [
    "## 🔬 Practical Applications and Case Studies\n",
    "\n",
    "### Real-world Post-processing Scenarios\n",
    "\n",
    "This section demonstrates how to apply the post-processing pipeline to real cardiac segmentation challenges:\n",
    "\n",
    "1. **Clinical Workflow Integration**: Automating post-processing in clinical environments\n",
    "2. **Multi-modal Cardiac Imaging**: Handling different MRI sequences (T1, T2, CINE)\n",
    "3. **Pathological Cases**: Processing segmentations with cardiac abnormalities\n",
    "4. **Quality Control**: Implementing automated quality checks for clinical use\n",
    "5. **Performance Optimization**: Balancing accuracy vs processing speed\n",
    "\n",
    "### Key Learning Points\n",
    "\n",
    "- Understanding when and how to apply different post-processing techniques\n",
    "- Adapting parameters for different cardiac conditions and imaging protocols\n",
    "- Implementing robust error handling and fallback mechanisms\n",
    "- Creating efficient batch processing workflows\n",
    "- Validating results against clinical standards\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91920b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalPostProcessor:\n",
    "    \"\"\"Clinical-grade post-processing with condition-specific adaptations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.condition_configs = self._setup_condition_configs()\n",
    "        self.quality_thresholds = self._setup_quality_thresholds()\n",
    "        \n",
    "    def _setup_condition_configs(self) -> Dict[str, PostProcessConfig]:\n",
    "        \"\"\"Setup configurations for different cardiac conditions\"\"\"\n",
    "        configs = {}\n",
    "        \n",
    "        # Normal heart\n",
    "        configs['normal'] = PostProcessConfig(\n",
    "            min_component_size=200,\n",
    "            min_heart_area=2000,\n",
    "            max_heart_area=15000,\n",
    "            circularity_threshold=0.4\n",
    "        )\n",
    "        \n",
    "        # Dilated cardiomyopathy (enlarged heart)\n",
    "        configs['dilated'] = PostProcessConfig(\n",
    "            min_component_size=300,\n",
    "            min_heart_area=4000,\n",
    "            max_heart_area=25000,\n",
    "            circularity_threshold=0.3,\n",
    "            aspect_ratio_range=(0.6, 2.5)\n",
    "        )\n",
    "        \n",
    "        # Hypertrophic cardiomyopathy (thickened walls)\n",
    "        configs['hypertrophic'] = PostProcessConfig(\n",
    "            min_component_size=150,\n",
    "            min_heart_area=1500,\n",
    "            max_heart_area=20000,\n",
    "            circularity_threshold=0.5,\n",
    "            closing_iterations=3  # More aggressive closing for thick walls\n",
    "        )\n",
    "        \n",
    "        # Post-infarction (irregular shape)\n",
    "        configs['post_infarction'] = PostProcessConfig(\n",
    "            min_component_size=100,\n",
    "            min_heart_area=1000,\n",
    "            max_heart_area=18000,\n",
    "            circularity_threshold=0.2,\n",
    "            smooth_iterations=5,  # More smoothing for irregular boundaries\n",
    "            gaussian_sigma=1.5\n",
    "        )\n",
    "        \n",
    "        # Pediatric cases (smaller hearts)\n",
    "        configs['pediatric'] = PostProcessConfig(\n",
    "            min_component_size=50,\n",
    "            min_heart_area=500,\n",
    "            max_heart_area=8000,\n",
    "            circularity_threshold=0.4,\n",
    "            kernel_size=2  # Smaller kernels for smaller structures\n",
    "        )\n",
    "        \n",
    "        return configs\n",
    "    \n",
    "    def _setup_quality_thresholds(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Setup quality thresholds for clinical acceptance\"\"\"\n",
    "        return {\n",
    "            'normal': {\n",
    "                'min_dice': 0.85,\n",
    "                'max_hausdorff': 15.0,\n",
    "                'min_smoothness': 0.5,\n",
    "                'max_components': 3\n",
    "            },\n",
    "            'dilated': {\n",
    "                'min_dice': 0.80,\n",
    "                'max_hausdorff': 20.0,\n",
    "                'min_smoothness': 0.4,\n",
    "                'max_components': 3\n",
    "            },\n",
    "            'hypertrophic': {\n",
    "                'min_dice': 0.82,\n",
    "                'max_hausdorff': 18.0,\n",
    "                'min_smoothness': 0.6,\n",
    "                'max_components': 2\n",
    "            },\n",
    "            'post_infarction': {\n",
    "                'min_dice': 0.75,\n",
    "                'max_hausdorff': 25.0,\n",
    "                'min_smoothness': 0.3,\n",
    "                'max_components': 4\n",
    "            },\n",
    "            'pediatric': {\n",
    "                'min_dice': 0.88,\n",
    "                'max_hausdorff': 10.0,\n",
    "                'min_smoothness': 0.6,\n",
    "                'max_components': 2\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process_clinical_case(self, \n",
    "                            mask: np.ndarray,\n",
    "                            condition: str = 'normal',\n",
    "                            patient_metadata: Dict = None) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Process a clinical case with condition-specific parameters\"\"\"\n",
    "        \n",
    "        if condition not in self.condition_configs:\n",
    "            print(f\"Warning: Unknown condition '{condition}', using 'normal' config\")\n",
    "            condition = 'normal'\n",
    "        \n",
    "        # Get condition-specific configuration\n",
    "        config = self.condition_configs[condition]\n",
    "        \n",
    "        # Create pipeline with condition-specific config\n",
    "        pipeline = PostProcessingPipeline(config)\n",
    "        \n",
    "        # Process mask\n",
    "        processed_mask, metrics = pipeline.process_single_mask(mask)\n",
    "        \n",
    "        # Perform quality assessment\n",
    "        quality_assessor = QualityAssessment()\n",
    "        quality_results = quality_assessor.evaluate_post_processing(mask, processed_mask)\n",
    "        \n",
    "        # Check against clinical thresholds\n",
    "        quality_check = self._validate_clinical_quality(\n",
    "            quality_results, condition, processed_mask\n",
    "        )\n",
    "        \n",
    "        # Compile comprehensive results\n",
    "        results = {\n",
    "            'processed_mask': processed_mask,\n",
    "            'processing_metrics': metrics,\n",
    "            'quality_assessment': quality_results,\n",
    "            'clinical_validation': quality_check,\n",
    "            'condition': condition,\n",
    "            'config_used': config.__dict__,\n",
    "            'patient_metadata': patient_metadata or {}\n",
    "        }\n",
    "        \n",
    "        return processed_mask, results\n",
    "    \n",
    "    def _validate_clinical_quality(self, \n",
    "                                 quality_results: Dict, \n",
    "                                 condition: str,\n",
    "                                 processed_mask: np.ndarray) -> Dict:\n",
    "        \"\"\"Validate results against clinical quality thresholds\"\"\"\n",
    "        \n",
    "        thresholds = self.quality_thresholds[condition]\n",
    "        validation_results = {\n",
    "            'passed': True,\n",
    "            'warnings': [],\n",
    "            'errors': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Check smoothness\n",
    "        if 'processed_boundary_smoothness' in quality_results:\n",
    "            smoothness = quality_results['processed_boundary_smoothness']\n",
    "            if smoothness < thresholds['min_smoothness']:\n",
    "                validation_results['warnings'].append(\n",
    "                    f\"Low boundary smoothness: {smoothness:.3f} < {thresholds['min_smoothness']}\"\n",
    "                )\n",
    "                validation_results['recommendations'].append(\"Consider additional smoothing\")\n",
    "        \n",
    "        # Check number of components\n",
    "        labels = measure.label(processed_mask)\n",
    "        n_components = len(measure.regionprops(labels))\n",
    "        if n_components > thresholds['max_components']:\n",
    "            validation_results['errors'].append(\n",
    "                f\"Too many components: {n_components} > {thresholds['max_components']}\"\n",
    "            )\n",
    "            validation_results['recommendations'].append(\"Apply more aggressive component filtering\")\n",
    "            validation_results['passed'] = False\n",
    "        \n",
    "        # Overall validation\n",
    "        if validation_results['errors']:\n",
    "            validation_results['passed'] = False\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def batch_clinical_processing(self, \n",
    "                                cases: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process multiple clinical cases in batch\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, case in enumerate(tqdm(cases, desc=\"Processing clinical cases\")):\n",
    "            try:\n",
    "                mask = case['mask']\n",
    "                condition = case.get('condition', 'normal')\n",
    "                metadata = case.get('metadata', {})\n",
    "                \n",
    "                processed_mask, case_results = self.process_clinical_case(\n",
    "                    mask, condition, metadata\n",
    "                )\n",
    "                \n",
    "                case_results['case_id'] = i\n",
    "                case_results['success'] = True\n",
    "                results.append(case_results)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing case {i}: {e}\")\n",
    "                results.append({\n",
    "                    'case_id': i,\n",
    "                    'success': False,\n",
    "                    'error': str(e),\n",
    "                    'processed_mask': case['mask']  # Return original on error\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_clinical_report(self, results: List[Dict]) -> str:\n",
    "        \"\"\"Generate clinical report from processing results\"\"\"\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"# CARDIAC SEGMENTATION POST-PROCESSING REPORT\")\n",
    "        report.append(\"=\" * 50)\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_cases = len(results)\n",
    "        successful_cases = sum(1 for r in results if r.get('success', False))\n",
    "        passed_quality = sum(1 for r in results \n",
    "                           if r.get('success', False) and \n",
    "                           r.get('clinical_validation', {}).get('passed', False))\n",
    "        \n",
    "        report.append(f\"Total cases processed: {total_cases}\")\n",
    "        report.append(f\"Successful processing: {successful_cases} ({successful_cases/total_cases*100:.1f}%)\")\n",
    "        report.append(f\"Passed clinical quality: {passed_quality} ({passed_quality/total_cases*100:.1f}%)\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Condition breakdown\n",
    "        conditions = {}\n",
    "        for result in results:\n",
    "            if result.get('success', False):\n",
    "                condition = result.get('condition', 'unknown')\n",
    "                if condition not in conditions:\n",
    "                    conditions[condition] = {'total': 0, 'passed': 0}\n",
    "                conditions[condition]['total'] += 1\n",
    "                if result.get('clinical_validation', {}).get('passed', False):\n",
    "                    conditions[condition]['passed'] += 1\n",
    "        \n",
    "        report.append(\"## Condition Breakdown:\")\n",
    "        for condition, stats in conditions.items():\n",
    "            pass_rate = stats['passed'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
    "            report.append(f\"  {condition.title()}: {stats['passed']}/{stats['total']} ({pass_rate:.1f}% pass rate)\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Quality metrics summary\n",
    "        quality_metrics = defaultdict(list)\n",
    "        for result in results:\n",
    "            if result.get('success', False) and 'quality_assessment' in result:\n",
    "                qa = result['quality_assessment']\n",
    "                for metric, value in qa.items():\n",
    "                    if isinstance(value, (int, float)) and not np.isnan(value) and not np.isinf(value):\n",
    "                        quality_metrics[metric].append(value)\n",
    "        \n",
    "        if quality_metrics:\n",
    "            report.append(\"## Quality Metrics Summary:\")\n",
    "            for metric, values in quality_metrics.items():\n",
    "                if values:\n",
    "                    mean_val = np.mean(values)\n",
    "                    std_val = np.std(values)\n",
    "                    report.append(f\"  {metric}: {mean_val:.3f} ± {std_val:.3f}\")\n",
    "            report.append(\"\")\n",
    "        \n",
    "        # Recommendations\n",
    "        all_recommendations = []\n",
    "        for result in results:\n",
    "            if result.get('success', False):\n",
    "                recs = result.get('clinical_validation', {}).get('recommendations', [])\n",
    "                all_recommendations.extend(recs)\n",
    "        \n",
    "        if all_recommendations:\n",
    "            unique_recs = list(set(all_recommendations))\n",
    "            report.append(\"## Clinical Recommendations:\")\n",
    "            for rec in unique_recs:\n",
    "                count = all_recommendations.count(rec)\n",
    "                report.append(f\"  • {rec} (mentioned in {count} cases)\")\n",
    "        \n",
    "        return \"\\\\n\".join(report)\n",
    "\n",
    "# Create synthetic clinical cases for demonstration\n",
    "def create_synthetic_clinical_cases(n_cases: int = 10) -> List[Dict]:\n",
    "    \"\"\"Create synthetic clinical cases for testing\"\"\"\n",
    "    cases = []\n",
    "    conditions = ['normal', 'dilated', 'hypertrophic', 'post_infarction', 'pediatric']\n",
    "    \n",
    "    for i in range(n_cases):\n",
    "        condition = np.random.choice(conditions)\n",
    "        \n",
    "        # Create condition-specific mask\n",
    "        if condition == 'normal':\n",
    "            mask = create_normal_heart_mask()\n",
    "        elif condition == 'dilated':\n",
    "            mask = create_dilated_heart_mask()\n",
    "        elif condition == 'hypertrophic':\n",
    "            mask = create_hypertrophic_heart_mask()\n",
    "        elif condition == 'post_infarction':\n",
    "            mask = create_post_infarction_mask()\n",
    "        else:  # pediatric\n",
    "            mask = create_pediatric_heart_mask()\n",
    "        \n",
    "        cases.append({\n",
    "            'mask': mask,\n",
    "            'condition': condition,\n",
    "            'metadata': {\n",
    "                'patient_id': f'P{i+1:03d}',\n",
    "                'age': np.random.randint(20, 80) if condition != 'pediatric' else np.random.randint(1, 18),\n",
    "                'sex': np.random.choice(['M', 'F']),\n",
    "                'scan_date': f'2024-01-{np.random.randint(1, 30):02d}'\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return cases\n",
    "\n",
    "def create_normal_heart_mask() -> np.ndarray:\n",
    "    \"\"\"Create a normal heart mask\"\"\"\n",
    "    mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "    cv2.ellipse(mask, (128, 128), (60, 45), 0, 0, 360, 1, -1)\n",
    "    # Add some noise\n",
    "    noise = np.random.random(mask.shape) > 0.98\n",
    "    return mask | noise.astype(np.uint8)\n",
    "\n",
    "def create_dilated_heart_mask() -> np.ndarray:\n",
    "    \"\"\"Create a dilated cardiomyopathy mask (enlarged)\"\"\"\n",
    "    mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "    cv2.ellipse(mask, (128, 128), (85, 70), 0, 0, 360, 1, -1)\n",
    "    # Add irregularities\n",
    "    noise = np.random.random(mask.shape) > 0.95\n",
    "    return mask | noise.astype(np.uint8)\n",
    "\n",
    "def create_hypertrophic_heart_mask() -> np.ndarray:\n",
    "    \"\"\"Create a hypertrophic cardiomyopathy mask (thick walls)\"\"\"\n",
    "    mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "    # Outer boundary\n",
    "    cv2.ellipse(mask, (128, 128), (55, 50), 0, 0, 360, 1, -1)\n",
    "    # Inner cavity (smaller)\n",
    "    inner_mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "    cv2.ellipse(inner_mask, (128, 128), (25, 20), 0, 0, 360, 1, -1)\n",
    "    mask = mask & ~inner_mask\n",
    "    return mask\n",
    "\n",
    "def create_post_infarction_mask() -> np.ndarray:\n",
    "    \"\"\"Create a post-infarction mask (irregular shape)\"\"\"\n",
    "    mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "    # Create irregular shape using multiple ellipses\n",
    "    cv2.ellipse(mask, (128, 128), (50, 40), 0, 0, 180, 1, -1)\n",
    "    cv2.ellipse(mask, (138, 138), (40, 35), 45, 0, 270, 1, -1)\n",
    "    # Add significant noise\n",
    "    noise = np.random.random(mask.shape) > 0.92\n",
    "    return mask | noise.astype(np.uint8)\n",
    "\n",
    "def create_pediatric_heart_mask() -> np.ndarray:\n",
    "    \"\"\"Create a pediatric heart mask (smaller, more circular)\"\"\"\n",
    "    mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "    cv2.circle(mask, (128, 128), 35, 1, -1)\n",
    "    # Minimal noise for cleaner pediatric images\n",
    "    noise = np.random.random(mask.shape) > 0.99\n",
    "    return mask | noise.astype(np.uint8)\n",
    "\n",
    "# Test the clinical post-processor\n",
    "clinical_processor = ClinicalPostProcessor()\n",
    "print(\"✅ Clinical Post-processor initialized!\")\n",
    "print(f\"Supported conditions: {list(clinical_processor.condition_configs.keys())}\")\n",
    "\n",
    "# Create and process synthetic clinical cases\n",
    "test_cases = create_synthetic_clinical_cases(6)\n",
    "print(f\"\\\\nCreated {len(test_cases)} synthetic clinical cases\")\n",
    "\n",
    "# Process cases\n",
    "results = clinical_processor.batch_clinical_processing(test_cases)\n",
    "\n",
    "# Generate clinical report\n",
    "report = clinical_processor.generate_clinical_report(results)\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"CLINICAL PROCESSING REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(report)\n",
    "\n",
    "# Visualize some results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (case, result) in enumerate(zip(test_cases[:6], results[:6])):\n",
    "    if result.get('success', False):\n",
    "        original = case['mask']\n",
    "        processed = result['processed_mask']\n",
    "        condition = case['condition']\n",
    "        \n",
    "        # Show overlay\n",
    "        overlay = np.zeros((*original.shape, 3))\n",
    "        overlay[:, :, 0] = original  # Red channel for original\n",
    "        overlay[:, :, 1] = processed  # Green channel for processed\n",
    "        \n",
    "        axes[i].imshow(overlay)\n",
    "        axes[i].set_title(f\"{condition.title()}\\\\nPatient: {case['metadata']['patient_id']}\")\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Add quality indicator\n",
    "        passed = result.get('clinical_validation', {}).get('passed', False)\n",
    "        color = 'green' if passed else 'red'\n",
    "        axes[i].add_patch(plt.Rectangle((5, 5), 20, 20, facecolor=color, alpha=0.7))\n",
    "\n",
    "plt.suptitle('Clinical Post-processing Results\\\\n(Green overlay: processed, Red: original, Corner: quality indicator)', \n",
    "             fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02515b3a",
   "metadata": {},
   "source": [
    "## 📋 Summary and Best Practices\n",
    "\n",
    "### Key Achievements in This Notebook\n",
    "\n",
    "1. **Comprehensive Morphological Processing**: Implemented advanced morphological operations with adaptive kernels\n",
    "2. **Intelligent Component Analysis**: Created sophisticated connected component filtering with anatomical awareness\n",
    "3. **Anatomical Constraint Validation**: Developed condition-specific validation for different cardiac pathologies\n",
    "4. **Advanced Boundary Refinement**: Implemented multiple smoothing techniques preserving important anatomical features\n",
    "5. **Clinical Integration**: Created a complete clinical-grade pipeline with quality assessment and reporting\n",
    "\n",
    "### Best Practices for Cardiac Segmentation Post-processing\n",
    "\n",
    "#### 🎯 **Parameter Selection**\n",
    "- **Condition-Specific**: Adapt parameters based on cardiac condition (normal, dilated, hypertrophic, etc.)\n",
    "- **Image Resolution**: Scale morphological kernel sizes based on image resolution\n",
    "- **Patient Demographics**: Adjust area thresholds for pediatric vs adult cases\n",
    "- **Imaging Protocol**: Modify smoothing parameters based on MRI sequence type\n",
    "\n",
    "#### 🔧 **Pipeline Design**\n",
    "- **Modular Architecture**: Keep each processing step independent and configurable\n",
    "- **Error Handling**: Implement robust fallback mechanisms for edge cases\n",
    "- **Quality Gates**: Add validation checkpoints throughout the pipeline\n",
    "- **Performance Monitoring**: Track processing times and resource usage\n",
    "\n",
    "#### 🏥 **Clinical Considerations**\n",
    "- **Validation Requirements**: Ensure all outputs meet clinical quality standards\n",
    "- **Traceability**: Maintain detailed logs of all processing steps and parameters\n",
    "- **User Feedback**: Provide clear quality indicators and recommendations\n",
    "- **Integration**: Design for seamless integration with existing clinical workflows\n",
    "\n",
    "### Performance Optimization Tips\n",
    "\n",
    "```python\n",
    "# Example optimization strategies:\n",
    "\n",
    "# 1. Parallel processing for batch operations\n",
    "from multiprocessing import Pool\n",
    "def parallel_processing(masks_batch):\n",
    "    with Pool() as pool:\n",
    "        return pool.map(pipeline.process_single_mask, masks_batch)\n",
    "\n",
    "# 2. Memory-efficient processing for large images\n",
    "def process_large_image(large_mask, tile_size=512):\n",
    "    # Process in overlapping tiles\n",
    "    # Merge results with overlap handling\n",
    "    pass\n",
    "\n",
    "# 3. GPU acceleration for morphological operations\n",
    "import cupy as cp  # GPU arrays\n",
    "def gpu_morphology(mask):\n",
    "    gpu_mask = cp.asarray(mask)\n",
    "    # Perform GPU-accelerated operations\n",
    "    return cp.asnumpy(gpu_mask)\n",
    "```\n",
    "\n",
    "### Common Pitfalls and Solutions\n",
    "\n",
    "#### ❌ **Common Mistakes**\n",
    "1. **Over-smoothing**: Losing important anatomical details\n",
    "2. **Aggressive filtering**: Removing small but important structures\n",
    "3. **Ignoring pathology**: Using normal parameters for pathological cases\n",
    "4. **No validation**: Skipping quality assessment steps\n",
    "\n",
    "#### ✅ **Solutions**\n",
    "1. **Adaptive smoothing**: Use image gradients to preserve edges\n",
    "2. **Multi-scale analysis**: Process at different scales and combine results\n",
    "3. **Condition awareness**: Implement pathology-specific processing paths\n",
    "4. **Continuous validation**: Monitor quality metrics throughout processing\n",
    "\n",
    "### Integration with the Complete Pipeline\n",
    "\n",
    "This post-processing notebook integrates seamlessly with the other components:\n",
    "\n",
    "- **Input**: Raw segmentation masks from notebook 06 (Model Evaluation)\n",
    "- **Processing**: Advanced morphological and anatomical refinement\n",
    "- **Output**: Clinical-ready segmentations for notebook 08 (Final Inference)\n",
    "- **Quality Control**: Comprehensive metrics for clinical validation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Advanced Techniques**: Implement deep learning-based post-processing\n",
    "2. **Real-time Processing**: Optimize for real-time clinical applications\n",
    "3. **Multi-modal Integration**: Combine with other imaging modalities\n",
    "4. **Automated Parameter Tuning**: Develop adaptive parameter selection\n",
    "5. **Clinical Validation**: Extensive testing with real clinical data\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Ready for Final Inference\n",
    "\n",
    "The post-processed segmentations are now ready for the final inference pipeline in **notebook 08**, where we'll implement:\n",
    "\n",
    "- End-to-end inference workflows\n",
    "- Batch processing optimization\n",
    "- Performance benchmarking\n",
    "- Clinical deployment preparation\n",
    "- Results compilation and reporting\n",
    "\n",
    "The robust post-processing pipeline ensures that all segmentations meet clinical quality standards before final deployment! 🏥✨"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
