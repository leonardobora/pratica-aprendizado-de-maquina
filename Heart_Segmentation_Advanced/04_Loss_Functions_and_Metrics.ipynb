{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7da9e",
   "metadata": {},
   "source": [
    "# 📊 Loss Functions and Metrics for Cardiac Segmentation\n",
    "\n",
    "This notebook implements advanced loss functions and metrics specifically designed for medical image segmentation tasks. We'll focus on hybrid loss functions that combine multiple objectives and medical-specific evaluation metrics.\n",
    "\n",
    "## Objectives\n",
    "- Implement Dice Loss for overlap optimization\n",
    "- Implement Binary Cross-Entropy Loss for pixel-wise classification\n",
    "- Create hybrid loss functions (Dice + BCE, Focal + Dice)\n",
    "- Implement boundary-aware losses\n",
    "- Develop medical segmentation metrics (IoU, Hausdorff Distance)\n",
    "- Create comprehensive evaluation framework\n",
    "\n",
    "## Key Components\n",
    "1. **Individual Loss Functions**: Dice, BCE, Focal Loss\n",
    "2. **Hybrid Loss Functions**: Combined losses with configurable weights\n",
    "3. **Boundary-Aware Losses**: Surface loss, boundary loss\n",
    "4. **Medical Metrics**: Dice coefficient, IoU, sensitivity, specificity\n",
    "5. **Distance Metrics**: Hausdorff distance, average surface distance\n",
    "6. **Evaluation Framework**: Comprehensive metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7473f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cpu\n",
      "CUDA Available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Device:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ff6a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Functions Configuration:\n",
      "  dice_weight: 0.5\n",
      "  bce_weight: 0.5\n",
      "  focal_alpha: 0.25\n",
      "  focal_gamma: 2.0\n",
      "  boundary_weight: 0.1\n",
      "  smooth: 1e-06\n",
      "  epsilon: 1e-07\n",
      "  binary_threshold: 0.5\n",
      "  hausdorff_percentile: 95\n"
     ]
    }
   ],
   "source": [
    "# Configuration Class for Loss Functions and Metrics\n",
    "class LossConfig:\n",
    "    \"\"\"Configuration for loss functions and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Loss function weights\n",
    "        self.dice_weight = 0.5\n",
    "        self.bce_weight = 0.5\n",
    "        self.focal_alpha = 0.25\n",
    "        self.focal_gamma = 2.0\n",
    "        self.boundary_weight = 0.1\n",
    "        \n",
    "        # Smoothing parameters\n",
    "        self.smooth = 1e-6\n",
    "        self.epsilon = 1e-7\n",
    "        \n",
    "        # Thresholds\n",
    "        self.binary_threshold = 0.5\n",
    "        \n",
    "        # Evaluation parameters\n",
    "        self.hausdorff_percentile = 95\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'dice_weight': self.dice_weight,\n",
    "            'bce_weight': self.bce_weight,\n",
    "            'focal_alpha': self.focal_alpha,\n",
    "            'focal_gamma': self.focal_gamma,\n",
    "            'boundary_weight': self.boundary_weight,\n",
    "            'smooth': self.smooth,\n",
    "            'epsilon': self.epsilon,\n",
    "            'binary_threshold': self.binary_threshold,\n",
    "            'hausdorff_percentile': self.hausdorff_percentile\n",
    "        }\n",
    "\n",
    "# Initialize configuration\n",
    "config = LossConfig()\n",
    "print(\"Loss Functions Configuration:\")\n",
    "for key, value in config.get_config().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134f4e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dice Loss Functions Implemented:\n",
      "- DiceLoss: Standard Dice loss for binary segmentation\n",
      "- DiceCoefficient: Dice coefficient metric\n",
      "- GeneralizedDiceLoss: Multi-class Dice loss\n",
      "\n",
      "🧪 Test with dummy data:\n",
      "   Generalized Dice Loss: 0.6688\n",
      "   Input shape - Pred: torch.Size([2, 3, 64, 64]), True: torch.Size([2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Individual Loss Functions\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss for binary segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: Predicted mask (B, C, H, W)\n",
    "            y_true: Ground truth mask (B, C, H, W) or (B, H, W)\n",
    "        \"\"\"\n",
    "        # Ensure same shape\n",
    "        if y_true.dim() == 3:  # (B, H, W)\n",
    "            y_true = y_true.unsqueeze(1)  # (B, 1, H, W)\n",
    "        \n",
    "        # Flatten tensors\n",
    "        y_pred_flat = y_pred.view(y_pred.size(0), -1)\n",
    "        y_true_flat = y_true.view(y_true.size(0), -1)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = (y_pred_flat * y_true_flat).sum(dim=1)\n",
    "        total = y_pred_flat.sum(dim=1) + y_true_flat.sum(dim=1)\n",
    "        \n",
    "        # Calculate Dice coefficient\n",
    "        dice = (2. * intersection + self.smooth) / (total + self.smooth)\n",
    "        \n",
    "        # Return Dice loss\n",
    "        return 1. - dice.mean()\n",
    "\n",
    "class DiceCoefficient(nn.Module):\n",
    "    \"\"\"Dice coefficient metric (not loss)\"\"\"\n",
    "    \n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceCoefficient, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Same calculation as DiceLoss but return coefficient\n",
    "        if y_true.dim() == 3:\n",
    "            y_true = y_true.unsqueeze(1)\n",
    "        \n",
    "        y_pred_flat = y_pred.view(y_pred.size(0), -1)\n",
    "        y_true_flat = y_true.view(y_true.size(0), -1)\n",
    "        \n",
    "        intersection = (y_pred_flat * y_true_flat).sum(dim=1)\n",
    "        total = y_pred_flat.sum(dim=1) + y_true_flat.sum(dim=1)\n",
    "        \n",
    "        dice = (2. * intersection + self.smooth) / (total + self.smooth)\n",
    "        return dice.mean()\n",
    "\n",
    "class GeneralizedDiceLoss(nn.Module):\n",
    "    \"\"\"Generalized Dice Loss for multi-class segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(GeneralizedDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: Predicted logits (B, C, H, W)\n",
    "            y_true: Ground truth (B, H, W) with class indices\n",
    "        \"\"\"\n",
    "        # Convert to one-hot if needed\n",
    "        if y_true.dim() == 3:  # (B, H, W)\n",
    "            y_true_oh = F.one_hot(y_true.long(), num_classes=y_pred.size(1))\n",
    "            y_true_oh = y_true_oh.permute(0, 3, 1, 2).float()  # (B, C, H, W)\n",
    "        else:\n",
    "            y_true_oh = y_true\n",
    "        \n",
    "        # Apply softmax to predictions\n",
    "        y_pred_soft = F.softmax(y_pred, dim=1)\n",
    "        \n",
    "        # Flatten for computation\n",
    "        y_pred_flat = y_pred_soft.view(y_pred_soft.size(0), y_pred_soft.size(1), -1)\n",
    "        y_true_flat = y_true_oh.view(y_true_oh.size(0), y_true_oh.size(1), -1)\n",
    "        \n",
    "        # Calculate weights (inverse class frequency)\n",
    "        class_sums = y_true_flat.sum(dim=(0, 2))\n",
    "        weights = 1. / (class_sums ** 2 + self.smooth)\n",
    "        \n",
    "        # Calculate intersection and union per class\n",
    "        intersection = (y_pred_flat * y_true_flat).sum(dim=(0, 2))\n",
    "        total = (y_pred_flat + y_true_flat).sum(dim=(0, 2))\n",
    "        \n",
    "        # Weighted generalized dice\n",
    "        numerator = 2. * (weights * intersection).sum()\n",
    "        denominator = (weights * total).sum()\n",
    "        \n",
    "        gd_loss = 1. - (numerator + self.smooth) / (denominator + self.smooth)\n",
    "        \n",
    "        return gd_loss\n",
    "\n",
    "# Test dice loss functions\n",
    "dice_loss = DiceLoss()\n",
    "dice_coeff = DiceCoefficient()\n",
    "gen_dice_loss = GeneralizedDiceLoss()\n",
    "\n",
    "print(\"✅ Dice Loss Functions Implemented:\")\n",
    "print(\"- DiceLoss: Standard Dice loss for binary segmentation\")\n",
    "print(\"- DiceCoefficient: Dice coefficient metric\")\n",
    "print(\"- GeneralizedDiceLoss: Multi-class Dice loss\")\n",
    "\n",
    "# Quick test with dummy data\n",
    "test_pred = torch.randn(2, 3, 64, 64)\n",
    "test_true = torch.randint(0, 3, (2, 64, 64))\n",
    "\n",
    "print(f\"\\n🧪 Test with dummy data:\")\n",
    "print(f\"   Generalized Dice Loss: {gen_dice_loss(test_pred, test_true):.4f}\")\n",
    "print(f\"   Input shape - Pred: {test_pred.shape}, True: {test_true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb0c09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Additional Loss Functions Implemented:\n",
      "- BinaryCrossEntropyLoss: Stable BCE with clipping\n",
      "- FocalLoss: For handling class imbalance\n",
      "- TverskyLoss: Generalization of Dice Loss\n",
      "\n",
      "🧪 Test with dummy binary data:\n",
      "   BCE Loss: 0.8104\n",
      "   Focal Loss: 0.1772\n",
      "   Tversky Loss: 0.5075\n"
     ]
    }
   ],
   "source": [
    "# Binary Cross-Entropy and Focal Loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"Binary Cross-Entropy loss with numerical stability\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon=1e-7):\n",
    "        super(BinaryCrossEntropyLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: Predicted probabilities (B, 1, H, W)\n",
    "            y_true: Ground truth binary mask (B, 1, H, W) or (B, H, W)\n",
    "        \"\"\"\n",
    "        if y_true.dim() == 3:\n",
    "            y_true = y_true.unsqueeze(1).float()\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = torch.clamp(y_pred, self.epsilon, 1. - self.epsilon)\n",
    "        \n",
    "        # Calculate BCE\n",
    "        bce = -(y_true * torch.log(y_pred) + (1. - y_true) * torch.log(1. - y_pred))\n",
    "        \n",
    "        return bce.mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0, epsilon=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: Predicted probabilities (B, 1, H, W)\n",
    "            y_true: Ground truth binary mask (B, 1, H, W) or (B, H, W)\n",
    "        \"\"\"\n",
    "        if y_true.dim() == 3:\n",
    "            y_true = y_true.unsqueeze(1).float()\n",
    "        \n",
    "        # Clip predictions\n",
    "        y_pred = torch.clamp(y_pred, self.epsilon, 1. - self.epsilon)\n",
    "        \n",
    "        # Calculate cross entropy\n",
    "        ce = -(y_true * torch.log(y_pred) + (1. - y_true) * torch.log(1. - y_pred))\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        pt = torch.where(y_true == 1, y_pred, 1 - y_pred)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Apply alpha weighting\n",
    "        alpha_weight = torch.where(y_true == 1, self.alpha, 1 - self.alpha)\n",
    "        \n",
    "        # Combine everything\n",
    "        focal_loss = alpha_weight * focal_weight * ce\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "class TverskyLoss(nn.Module):\n",
    "    \"\"\"Tversky Loss - generalization of Dice Loss\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.5, beta=0.5, smooth=1e-6):\n",
    "        super(TverskyLoss, self).__init__()\n",
    "        self.alpha = alpha  # weight for false positives\n",
    "        self.beta = beta    # weight for false negatives\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: Predicted probabilities (B, 1, H, W)\n",
    "            y_true: Ground truth binary mask (B, 1, H, W) or (B, H, W)\n",
    "        \"\"\"\n",
    "        if y_true.dim() == 3:\n",
    "            y_true = y_true.unsqueeze(1).float()\n",
    "        \n",
    "        # Flatten tensors\n",
    "        y_pred_flat = y_pred.view(-1)\n",
    "        y_true_flat = y_true.view(-1)\n",
    "        \n",
    "        # Calculate true positives, false positives, false negatives\n",
    "        TP = (y_pred_flat * y_true_flat).sum()\n",
    "        FP = ((1 - y_true_flat) * y_pred_flat).sum()\n",
    "        FN = (y_true_flat * (1 - y_pred_flat)).sum()\n",
    "        \n",
    "        # Calculate Tversky coefficient\n",
    "        tversky = (TP + self.smooth) / (TP + self.alpha * FP + self.beta * FN + self.smooth)\n",
    "        \n",
    "        return 1. - tversky\n",
    "\n",
    "# Test loss functions\n",
    "bce_loss = BinaryCrossEntropyLoss()\n",
    "focal_loss = FocalLoss()\n",
    "tversky_loss = TverskyLoss()\n",
    "\n",
    "print(\"✅ Additional Loss Functions Implemented:\")\n",
    "print(\"- BinaryCrossEntropyLoss: Stable BCE with clipping\")\n",
    "print(\"- FocalLoss: For handling class imbalance\")\n",
    "print(\"- TverskyLoss: Generalization of Dice Loss\")\n",
    "\n",
    "# Quick test with dummy binary data\n",
    "test_pred_bin = torch.sigmoid(torch.randn(2, 1, 64, 64))\n",
    "test_true_bin = torch.randint(0, 2, (2, 64, 64)).float()\n",
    "\n",
    "print(f\"\\n🧪 Test with dummy binary data:\")\n",
    "print(f\"   BCE Loss: {bce_loss(test_pred_bin, test_true_bin):.4f}\")\n",
    "print(f\"   Focal Loss: {focal_loss(test_pred_bin, test_true_bin):.4f}\")\n",
    "print(f\"   Tversky Loss: {tversky_loss(test_pred_bin, test_true_bin):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa65f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hybrid Loss Functions Implemented:\n",
      "- DiceBCELoss: Combined Dice + BCE\n",
      "- FocalDiceLoss: Combined Focal + Dice\n",
      "- TverskyFocalLoss: Combined Tversky + Focal\n",
      "- AdaptiveLoss: Adaptive weight adjustment\n",
      "\n",
      "🧪 Test hybrid losses:\n",
      "   Dice+BCE Loss: 0.6438\n",
      "   Focal+Dice Loss: 0.3305\n",
      "   Tversky+Focal Loss: 0.3313\n",
      "   Adaptive Loss: 0.6438\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Loss Functions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    \"\"\"Combined Dice and Binary Cross-Entropy Loss\"\"\"\n",
    "    \n",
    "    def __init__(self, dice_weight=0.5, bce_weight=0.5, smooth=1e-6):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_loss = DiceLoss(smooth)\n",
    "        self.bce_loss = BinaryCrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        dice_loss_val = self.dice_loss(y_pred, y_true)\n",
    "        bce_loss_val = self.bce_loss(y_pred, y_true)\n",
    "        \n",
    "        return self.dice_weight * dice_loss_val + self.bce_weight * bce_loss_val\n",
    "\n",
    "class FocalDiceLoss(nn.Module):\n",
    "    \"\"\"Combined Focal and Dice Loss\"\"\"\n",
    "    \n",
    "    def __init__(self, focal_weight=0.5, dice_weight=0.5, \n",
    "                 alpha=0.25, gamma=2.0, smooth=1e-6):\n",
    "        super(FocalDiceLoss, self).__init__()\n",
    "        self.focal_weight = focal_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_loss = FocalLoss(alpha, gamma)\n",
    "        self.dice_loss = DiceLoss(smooth)\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        focal_loss_val = self.focal_loss(y_pred, y_true)\n",
    "        dice_loss_val = self.dice_loss(y_pred, y_true)\n",
    "        \n",
    "        return self.focal_weight * focal_loss_val + self.dice_weight * dice_loss_val\n",
    "\n",
    "class TverskyFocalLoss(nn.Module):\n",
    "    \"\"\"Combined Tversky and Focal Loss\"\"\"\n",
    "    \n",
    "    def __init__(self, tversky_weight=0.5, focal_weight=0.5,\n",
    "                 alpha_t=0.3, beta_t=0.7, alpha_f=0.25, gamma_f=2.0, smooth=1e-6):\n",
    "        super(TverskyFocalLoss, self).__init__()\n",
    "        self.tversky_weight = tversky_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        self.tversky_loss = TverskyLoss(alpha_t, beta_t, smooth)\n",
    "        self.focal_loss = FocalLoss(alpha_f, gamma_f)\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        tversky_loss_val = self.tversky_loss(y_pred, y_true)\n",
    "        focal_loss_val = self.focal_loss(y_pred, y_true)\n",
    "        \n",
    "        return self.tversky_weight * tversky_loss_val + self.focal_weight * focal_loss_val\n",
    "\n",
    "class AdaptiveLoss(nn.Module):\n",
    "    \"\"\"Adaptive loss that adjusts weights during training\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_dice_weight=0.5):\n",
    "        super(AdaptiveLoss, self).__init__()\n",
    "        self.dice_weight = nn.Parameter(torch.tensor(initial_dice_weight), requires_grad=False)\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.bce_loss = BinaryCrossEntropyLoss()\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        dice_loss_val = self.dice_loss(y_pred, y_true)\n",
    "        bce_loss_val = self.bce_loss(y_pred, y_true)\n",
    "        \n",
    "        # Adapt weights based on performance (simple strategy)\n",
    "        bce_weight = 1.0 - self.dice_weight\n",
    "        \n",
    "        return self.dice_weight * dice_loss_val + bce_weight * bce_loss_val\n",
    "    \n",
    "    def update_weights(self, dice_score):\n",
    "        \"\"\"Update weights based on current performance\"\"\"\n",
    "        # Simple adaptation: increase Dice weight if score is low\n",
    "        if dice_score < 0.5:\n",
    "            self.dice_weight.data = torch.clamp(self.dice_weight.data + 0.01, 0.1, 0.9)\n",
    "        elif dice_score > 0.8:\n",
    "            self.dice_weight.data = torch.clamp(self.dice_weight.data - 0.01, 0.1, 0.9)\n",
    "\n",
    "# Test hybrid loss functions\n",
    "dice_bce_loss = DiceBCELoss()\n",
    "focal_dice_loss = FocalDiceLoss()\n",
    "tversky_focal_loss = TverskyFocalLoss()\n",
    "adaptive_loss = AdaptiveLoss()\n",
    "\n",
    "print(\"✅ Hybrid Loss Functions Implemented:\")\n",
    "print(\"- DiceBCELoss: Combined Dice + BCE\")\n",
    "print(\"- FocalDiceLoss: Combined Focal + Dice\")\n",
    "print(\"- TverskyFocalLoss: Combined Tversky + Focal\")\n",
    "print(\"- AdaptiveLoss: Adaptive weight adjustment\")\n",
    "\n",
    "# Quick test\n",
    "test_pred_bin = torch.sigmoid(torch.randn(2, 1, 64, 64))\n",
    "test_true_bin = torch.randint(0, 2, (2, 64, 64)).float()\n",
    "\n",
    "print(f\"\\n🧪 Test hybrid losses:\")\n",
    "print(f\"   Dice+BCE Loss: {dice_bce_loss(test_pred_bin, test_true_bin):.4f}\")\n",
    "print(f\"   Focal+Dice Loss: {focal_dice_loss(test_pred_bin, test_true_bin):.4f}\")\n",
    "print(f\"   Tversky+Focal Loss: {tversky_focal_loss(test_pred_bin, test_true_bin):.4f}\")\n",
    "print(f\"   Adaptive Loss: {adaptive_loss(test_pred_bin, test_true_bin):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79cb7a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary-Aware Loss Functions Implemented:\n",
      "- boundary_loss: Emphasizes edge detection\n",
      "- surface_loss: Improves boundary alignment\n",
      "- hausdorff_loss: Approximates Hausdorff distance\n",
      "- boundary_aware_loss: Combined boundary-aware loss\n"
     ]
    }
   ],
   "source": [
    "# Boundary-Aware Loss Functions\n",
    "\n",
    "def boundary_loss(y_true, y_pred, theta=0.01):\n",
    "    \"\"\"\n",
    "    Boundary loss to improve edge detection\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth mask\n",
    "        y_pred: Predicted mask\n",
    "        theta: Threshold for boundary detection\n",
    "    \n",
    "    Returns:\n",
    "        Boundary loss value\n",
    "    \"\"\"\n",
    "    # Convert to binary if needed\n",
    "    y_true_binary = tf.cast(y_true > 0.5, tf.float32)\n",
    "    y_pred_binary = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    \n",
    "    # Compute gradients to find boundaries\n",
    "    def compute_boundary(mask):\n",
    "        # Sobel filters for edge detection\n",
    "        sobel_x = tf.constant([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=tf.float32)\n",
    "        sobel_y = tf.constant([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=tf.float32)\n",
    "        \n",
    "        sobel_x = tf.reshape(sobel_x, [3, 3, 1, 1])\n",
    "        sobel_y = tf.reshape(sobel_y, [3, 3, 1, 1])\n",
    "        \n",
    "        # Add channel dimension if needed\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = tf.expand_dims(mask, -1)\n",
    "        \n",
    "        grad_x = tf.nn.conv2d(mask, sobel_x, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        grad_y = tf.nn.conv2d(mask, sobel_y, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        \n",
    "        boundary = tf.sqrt(grad_x**2 + grad_y**2)\n",
    "        return tf.squeeze(boundary, -1) if len(y_true.shape) == 3 else boundary\n",
    "    \n",
    "    # Compute boundaries\n",
    "    true_boundary = compute_boundary(y_true_binary)\n",
    "    pred_boundary = compute_boundary(y_pred)\n",
    "    \n",
    "    # Boundary loss\n",
    "    boundary_diff = tf.abs(true_boundary - pred_boundary)\n",
    "    return tf.reduce_mean(boundary_diff)\n",
    "\n",
    "def surface_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Surface loss for better boundary alignment\n",
    "    \"\"\"\n",
    "    # Compute distance transforms\n",
    "    def compute_distance_transform(mask):\n",
    "        # Simplified distance transform using morphological operations\n",
    "        mask_binary = tf.cast(mask > 0.5, tf.float32)\n",
    "        \n",
    "        # Create kernel for morphological operations\n",
    "        kernel = tf.ones((3, 3, 1, 1), dtype=tf.float32)\n",
    "        \n",
    "        # Erosion and dilation for distance approximation\n",
    "        eroded = tf.nn.erosion2d(tf.expand_dims(mask_binary, -1), kernel, \n",
    "                                strides=[1, 1, 1, 1], padding='SAME', \n",
    "                                data_format='NHWC', dilations=[1, 1, 1, 1])\n",
    "        \n",
    "        dilated = tf.nn.dilation2d(tf.expand_dims(mask_binary, -1), kernel,\n",
    "                                  strides=[1, 1, 1, 1], padding='SAME',\n",
    "                                  data_format='NHWC', dilations=[1, 1, 1, 1])\n",
    "        \n",
    "        # Approximate distance as difference between original and eroded\n",
    "        distance = mask_binary - tf.squeeze(eroded, -1)\n",
    "        return distance\n",
    "    \n",
    "    # Compute distance transforms\n",
    "    true_dist = compute_distance_transform(y_true)\n",
    "    \n",
    "    # Surface loss\n",
    "    surface_loss_val = tf.reduce_mean(y_pred * true_dist)\n",
    "    \n",
    "    return surface_loss_val\n",
    "\n",
    "def hausdorff_loss(y_true, y_pred, alpha=2.0):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of Hausdorff distance\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth mask\n",
    "        y_pred: Predicted mask\n",
    "        alpha: Smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        Hausdorff loss approximation\n",
    "    \"\"\"\n",
    "    # Convert to binary\n",
    "    y_true_binary = tf.cast(y_true > 0.5, tf.float32)\n",
    "    y_pred_binary = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    \n",
    "    # Get coordinates of positive pixels\n",
    "    def get_boundary_points(mask):\n",
    "        # Find boundary using morphological operations\n",
    "        mask_3d = tf.expand_dims(mask, -1)\n",
    "        kernel = tf.ones((3, 3, 1, 1), dtype=tf.float32)\n",
    "        \n",
    "        eroded = tf.nn.erosion2d(mask_3d, kernel, strides=[1, 1, 1, 1], \n",
    "                                padding='SAME', data_format='NHWC', \n",
    "                                dilations=[1, 1, 1, 1])\n",
    "        \n",
    "        boundary = mask_3d - eroded\n",
    "        return tf.squeeze(boundary, -1)\n",
    "    \n",
    "    # Get boundaries\n",
    "    true_boundary = get_boundary_points(y_true_binary)\n",
    "    pred_boundary = get_boundary_points(y_pred_binary)\n",
    "    \n",
    "    # Compute approximate Hausdorff distance\n",
    "    # This is a simplified version - in practice, you might use more sophisticated methods\n",
    "    boundary_diff = tf.abs(true_boundary - pred_boundary)\n",
    "    hausdorff_approx = tf.reduce_max(boundary_diff, axis=[1, 2])\n",
    "    \n",
    "    return tf.reduce_mean(hausdorff_approx)\n",
    "\n",
    "# Combined boundary-aware loss\n",
    "def boundary_aware_loss(y_true, y_pred, dice_weight=0.4, boundary_weight=0.3, \n",
    "                       surface_weight=0.3, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Combined loss with boundary awareness\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth mask\n",
    "        y_pred: Predicted mask\n",
    "        dice_weight: Weight for Dice loss\n",
    "        boundary_weight: Weight for boundary loss\n",
    "        surface_weight: Weight for surface loss\n",
    "        smooth: Smoothing factor\n",
    "    \n",
    "    Returns:\n",
    "        Combined boundary-aware loss\n",
    "    \"\"\"\n",
    "    dice_loss_val = dice_loss(y_true, y_pred, smooth)\n",
    "    boundary_loss_val = boundary_loss(y_true, y_pred)\n",
    "    surface_loss_val = surface_loss(y_true, y_pred)\n",
    "    \n",
    "    total_loss = (dice_weight * dice_loss_val + \n",
    "                  boundary_weight * boundary_loss_val + \n",
    "                  surface_weight * surface_loss_val)\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "print(\"Boundary-Aware Loss Functions Implemented:\")\n",
    "print(\"- boundary_loss: Emphasizes edge detection\")\n",
    "print(\"- surface_loss: Improves boundary alignment\")\n",
    "print(\"- hausdorff_loss: Approximates Hausdorff distance\")\n",
    "print(\"- boundary_aware_loss: Combined boundary-aware loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a27449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Segmentation Metrics Implemented:\n",
      "- Dice Coefficient\n",
      "- IoU (Intersection over Union)\n",
      "- Sensitivity (Recall)\n",
      "- Specificity\n",
      "- Precision\n",
      "- F1 Score\n",
      "- Volume Similarity\n",
      "- MedicalMetrics class for comprehensive evaluation\n"
     ]
    }
   ],
   "source": [
    "# Medical Segmentation Metrics\n",
    "\n",
    "def iou_score(y_true, y_pred, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Intersection over Union (IoU) score\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth mask\n",
    "        y_pred: Predicted mask\n",
    "        threshold: Threshold for binary conversion\n",
    "        smooth: Smoothing factor\n",
    "    \n",
    "    Returns:\n",
    "        IoU score\n",
    "    \"\"\"\n",
    "    y_true_binary = tf.cast(y_true > threshold, tf.float32)\n",
    "    y_pred_binary = tf.cast(y_pred > threshold, tf.float32)\n",
    "    \n",
    "    intersection = K.sum(y_true_binary * y_pred_binary)\n",
    "    union = K.sum(y_true_binary) + K.sum(y_pred_binary) - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou\n",
    "\n",
    "def sensitivity_score(y_true, y_pred, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Sensitivity (Recall/True Positive Rate)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth mask\n",
    "        y_pred: Predicted mask\n",
    "        threshold: Threshold for binary conversion\n",
    "        smooth: Smoothing factor\n",
    "    \n",
    "    Returns:\n",
    "        Sensitivity score\n",
    "    \"\"\"\n",
    "    y_true_binary = tf.cast(y_true > threshold, tf.float32)\n",
    "    y_pred_binary = tf.cast(y_pred > threshold, tf.float32)\n",
    "    \n",
    "    true_positives = K.sum(y_true_binary * y_pred_binary)\n",
    "    possible_positives = K.sum(y_true_binary)\n",
    "    \n",
    "    sensitivity = (true_positives + smooth) / (possible_positives + smooth)\n",
    "    return sensitivity\n",
    "\n",
    "def specificity_score(y_true, y_pred, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Specificity (True Negative Rate)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth mask\n",
    "        y_pred: Predicted mask\n",
    "        threshold: Threshold for binary conversion\n",
    "        smooth: Smoothing factor\n",
    "    \n",
    "    Returns:\n",
    "        Specificity score\n",
    "    \"\"\"\n",
    "    y_true_binary = tf.cast(y_true > threshold, tf.float32)\n",
    "    y_pred_binary = tf.cast(y_pred > threshold, tf.float32)\n",
    "    \n",
    "    true_negatives = K.sum((1 - y_true_binary) * (1 - y_pred_binary))\n",
    "    possible_negatives = K.sum(1 - y_true_binary)\n",
    "    \n",
    "    specificity = (true_negatives + smooth) / (possible_negatives + smooth)\n",
    "    return specificity\n",
    "\n",
    "def precision_score(y_true, y_pred, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Precision (Positive Predictive Value)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth mask\n",
    "        y_pred: Predicted mask\n",
    "        threshold: Threshold for binary conversion\n",
    "        smooth: Smoothing factor\n",
    "    \n",
    "    Returns:\n",
    "        Precision score\n",
    "    \"\"\"\n",
    "    y_true_binary = tf.cast(y_true > threshold, tf.float32)\n",
    "    y_pred_binary = tf.cast(y_pred > threshold, tf.float32)\n",
    "    \n",
    "    true_positives = K.sum(y_true_binary * y_pred_binary)\n",
    "    predicted_positives = K.sum(y_pred_binary)\n",
    "    \n",
    "    precision = (true_positives + smooth) / (predicted_positives + smooth)\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    F1 Score (Harmonic mean of precision and recall)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth mask\n",
    "        y_pred: Predicted mask\n",
    "        threshold: Threshold for binary conversion\n",
    "        smooth: Smoothing factor\n",
    "    \n",
    "    Returns:\n",
    "        F1 score\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, threshold, smooth)\n",
    "    sensitivity = sensitivity_score(y_true, y_pred, threshold, smooth)\n",
    "    \n",
    "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity + smooth)\n",
    "    return f1\n",
    "\n",
    "def volume_similarity(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Volume Similarity metric\n",
    "    \n",
    "    Args:\n",
    "        y_true: Ground truth mask\n",
    "        y_pred: Predicted mask\n",
    "        threshold: Threshold for binary conversion\n",
    "    \n",
    "    Returns:\n",
    "        Volume similarity score\n",
    "    \"\"\"\n",
    "    y_true_binary = tf.cast(y_true > threshold, tf.float32)\n",
    "    y_pred_binary = tf.cast(y_pred > threshold, tf.float32)\n",
    "    \n",
    "    vol_true = K.sum(y_true_binary)\n",
    "    vol_pred = K.sum(y_pred_binary)\n",
    "    \n",
    "    vol_sim = 1.0 - tf.abs(vol_true - vol_pred) / (vol_true + vol_pred + 1e-6)\n",
    "    return vol_sim\n",
    "\n",
    "class MedicalMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive medical segmentation metrics calculator\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.5, smooth=1e-6):\n",
    "        self.threshold = threshold\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def compute_all_metrics(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute all medical segmentation metrics\n",
    "        \n",
    "        Args:\n",
    "            y_true: Ground truth mask\n",
    "            y_pred: Predicted mask\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all computed metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Basic metrics\n",
    "        metrics['dice'] = dice_coefficient(y_true, y_pred, self.smooth)\n",
    "        metrics['iou'] = iou_score(y_true, y_pred, self.threshold, self.smooth)\n",
    "        metrics['sensitivity'] = sensitivity_score(y_true, y_pred, self.threshold, self.smooth)\n",
    "        metrics['specificity'] = specificity_score(y_true, y_pred, self.threshold, self.smooth)\n",
    "        metrics['precision'] = precision_score(y_true, y_pred, self.threshold, self.smooth)\n",
    "        metrics['f1'] = f1_score(y_true, y_pred, self.threshold, self.smooth)\n",
    "        metrics['volume_similarity'] = volume_similarity(y_true, y_pred, self.threshold)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compute_metrics_numpy(self, y_true_np, y_pred_np):\n",
    "        \"\"\"\n",
    "        Compute metrics using numpy arrays (for post-processing)\n",
    "        \n",
    "        Args:\n",
    "            y_true_np: Ground truth mask (numpy array)\n",
    "            y_pred_np: Predicted mask (numpy array)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with computed metrics\n",
    "        \"\"\"\n",
    "        y_true_binary = (y_true_np > self.threshold).astype(np.float32)\n",
    "        y_pred_binary = (y_pred_np > self.threshold).astype(np.float32)\n",
    "        \n",
    "        # Calculate basic metrics\n",
    "        intersection = np.sum(y_true_binary * y_pred_binary)\n",
    "        union = np.sum(y_true_binary) + np.sum(y_pred_binary) - intersection\n",
    "        \n",
    "        # Dice coefficient\n",
    "        dice = (2.0 * intersection + self.smooth) / (np.sum(y_true_binary) + np.sum(y_pred_binary) + self.smooth)\n",
    "        \n",
    "        # IoU\n",
    "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Sensitivity and Specificity\n",
    "        true_positives = intersection\n",
    "        false_negatives = np.sum(y_true_binary * (1 - y_pred_binary))\n",
    "        false_positives = np.sum((1 - y_true_binary) * y_pred_binary)\n",
    "        true_negatives = np.sum((1 - y_true_binary) * (1 - y_pred_binary))\n",
    "        \n",
    "        sensitivity = true_positives / (true_positives + false_negatives + self.smooth)\n",
    "        specificity = true_negatives / (true_negatives + false_positives + self.smooth)\n",
    "        precision = true_positives / (true_positives + false_positives + self.smooth)\n",
    "        \n",
    "        # F1 Score\n",
    "        f1 = 2 * (precision * sensitivity) / (precision + sensitivity + self.smooth)\n",
    "        \n",
    "        # Volume similarity\n",
    "        vol_true = np.sum(y_true_binary)\n",
    "        vol_pred = np.sum(y_pred_binary)\n",
    "        vol_sim = 1.0 - np.abs(vol_true - vol_pred) / (vol_true + vol_pred + self.smooth)\n",
    "        \n",
    "        return {\n",
    "            'dice': dice,\n",
    "            'iou': iou,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'precision': precision,\n",
    "            'f1': f1,\n",
    "            'volume_similarity': vol_sim\n",
    "        }\n",
    "\n",
    "# Initialize metrics calculator\n",
    "metrics_calculator = MedicalMetrics()\n",
    "\n",
    "print(\"Medical Segmentation Metrics Implemented:\")\n",
    "print(\"- Dice Coefficient\")\n",
    "print(\"- IoU (Intersection over Union)\")\n",
    "print(\"- Sensitivity (Recall)\")\n",
    "print(\"- Specificity\")\n",
    "print(\"- Precision\")\n",
    "print(\"- F1 Score\")\n",
    "print(\"- Volume Similarity\")\n",
    "print(\"- MedicalMetrics class for comprehensive evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d660f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance-Based Metrics Implemented:\n",
      "- Hausdorff Distance (with percentile option)\n",
      "- Average Surface Distance (ASD)\n",
      "- Surface Dice Coefficient\n",
      "- DistanceMetrics class for batch processing\n"
     ]
    }
   ],
   "source": [
    "# Distance-Based Metrics\n",
    "\n",
    "def hausdorff_distance_numpy(mask1, mask2, percentile=95):\n",
    "    \"\"\"\n",
    "    Calculate Hausdorff distance between two binary masks\n",
    "    \n",
    "    Args:\n",
    "        mask1: First binary mask (numpy array)\n",
    "        mask2: Second binary mask (numpy array)\n",
    "        percentile: Percentile for robust Hausdorff distance\n",
    "    \n",
    "    Returns:\n",
    "        Hausdorff distance value\n",
    "    \"\"\"\n",
    "    # Convert to binary\n",
    "    mask1_binary = (mask1 > 0.5).astype(np.uint8)\n",
    "    mask2_binary = (mask2 > 0.5).astype(np.uint8)\n",
    "    \n",
    "    # Find boundary points\n",
    "    def get_boundary_points(mask):\n",
    "        # Use morphological operations to find boundary\n",
    "        from scipy import ndimage\n",
    "        eroded = ndimage.binary_erosion(mask)\n",
    "        boundary = mask.astype(np.float32) - eroded.astype(np.float32)\n",
    "        coords = np.argwhere(boundary > 0)\n",
    "        return coords\n",
    "    \n",
    "    # Get boundary coordinates\n",
    "    coords1 = get_boundary_points(mask1_binary)\n",
    "    coords2 = get_boundary_points(mask2_binary)\n",
    "    \n",
    "    if len(coords1) == 0 or len(coords2) == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    # Calculate directed Hausdorff distances\n",
    "    try:\n",
    "        hd1 = directed_hausdorff(coords1, coords2)[0]\n",
    "        hd2 = directed_hausdorff(coords2, coords1)[0]\n",
    "        \n",
    "        # Return maximum (standard Hausdorff) or percentile-based (robust)\n",
    "        if percentile == 100:\n",
    "            return max(hd1, hd2)\n",
    "        else:\n",
    "            # For robust version, use percentile of distances\n",
    "            from scipy.spatial.distance import cdist\n",
    "            distances1 = np.min(cdist(coords1, coords2), axis=1)\n",
    "            distances2 = np.min(cdist(coords2, coords1), axis=1)\n",
    "            \n",
    "            hd1_robust = np.percentile(distances1, percentile)\n",
    "            hd2_robust = np.percentile(distances2, percentile)\n",
    "            \n",
    "            return max(hd1_robust, hd2_robust)\n",
    "    except:\n",
    "        return float('inf')\n",
    "\n",
    "def average_surface_distance(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Calculate Average Surface Distance (ASD)\n",
    "    \n",
    "    Args:\n",
    "        mask1: First binary mask (numpy array)\n",
    "        mask2: Second binary mask (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        Average surface distance\n",
    "    \"\"\"\n",
    "    # Convert to binary\n",
    "    mask1_binary = (mask1 > 0.5).astype(np.uint8)\n",
    "    mask2_binary = (mask2 > 0.5).astype(np.uint8)\n",
    "    \n",
    "    # Find boundary points\n",
    "    def get_boundary_points(mask):\n",
    "        eroded = ndimage.binary_erosion(mask)\n",
    "        boundary = mask.astype(np.float32) - eroded.astype(np.float32)\n",
    "        coords = np.argwhere(boundary > 0)\n",
    "        return coords\n",
    "    \n",
    "    coords1 = get_boundary_points(mask1_binary)\n",
    "    coords2 = get_boundary_points(mask2_binary)\n",
    "    \n",
    "    if len(coords1) == 0 or len(coords2) == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    try:\n",
    "        from scipy.spatial.distance import cdist\n",
    "        \n",
    "        # Calculate minimum distances\n",
    "        distances1 = np.min(cdist(coords1, coords2), axis=1)\n",
    "        distances2 = np.min(cdist(coords2, coords1), axis=1)\n",
    "        \n",
    "        # Average surface distance\n",
    "        asd = (np.mean(distances1) + np.mean(distances2)) / 2.0\n",
    "        return asd\n",
    "    except:\n",
    "        return float('inf')\n",
    "\n",
    "def surface_dice_coefficient(mask1, mask2, tolerance=1.0):\n",
    "    \"\"\"\n",
    "    Surface Dice coefficient - measures boundary agreement within tolerance\n",
    "    \n",
    "    Args:\n",
    "        mask1: First binary mask (numpy array)\n",
    "        mask2: Second binary mask (numpy array)\n",
    "        tolerance: Distance tolerance for surface matching\n",
    "    \n",
    "    Returns:\n",
    "        Surface Dice coefficient\n",
    "    \"\"\"\n",
    "    # Convert to binary\n",
    "    mask1_binary = (mask1 > 0.5).astype(np.uint8)\n",
    "    mask2_binary = (mask2 > 0.5).astype(np.uint8)\n",
    "    \n",
    "    # Find boundary points\n",
    "    def get_boundary_points(mask):\n",
    "        eroded = ndimage.binary_erosion(mask)\n",
    "        boundary = mask.astype(np.float32) - eroded.astype(np.float32)\n",
    "        coords = np.argwhere(boundary > 0)\n",
    "        return coords\n",
    "    \n",
    "    coords1 = get_boundary_points(mask1_binary)\n",
    "    coords2 = get_boundary_points(mask2_binary)\n",
    "    \n",
    "    if len(coords1) == 0 and len(coords2) == 0:\n",
    "        return 1.0  # Both empty\n",
    "    if len(coords1) == 0 or len(coords2) == 0:\n",
    "        return 0.0  # One empty\n",
    "    \n",
    "    try:\n",
    "        from scipy.spatial.distance import cdist\n",
    "        \n",
    "        # Find points within tolerance\n",
    "        distances1 = np.min(cdist(coords1, coords2), axis=1)\n",
    "        distances2 = np.min(cdist(coords2, coords1), axis=1)\n",
    "        \n",
    "        matched1 = np.sum(distances1 <= tolerance)\n",
    "        matched2 = np.sum(distances2 <= tolerance)\n",
    "        \n",
    "        # Surface Dice coefficient\n",
    "        surface_dice = (matched1 + matched2) / (len(coords1) + len(coords2))\n",
    "        return surface_dice\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "class DistanceMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive distance-based metrics calculator\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hausdorff_percentile=95, surface_tolerance=1.0):\n",
    "        self.hausdorff_percentile = hausdorff_percentile\n",
    "        self.surface_tolerance = surface_tolerance\n",
    "    \n",
    "    def compute_distance_metrics(self, y_true_np, y_pred_np):\n",
    "        \"\"\"\n",
    "        Compute all distance-based metrics\n",
    "        \n",
    "        Args:\n",
    "            y_true_np: Ground truth mask (numpy array)\n",
    "            y_pred_np: Predicted mask (numpy array)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with distance metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        try:\n",
    "            # Hausdorff Distance\n",
    "            metrics['hausdorff_distance'] = hausdorff_distance_numpy(\n",
    "                y_true_np, y_pred_np, self.hausdorff_percentile)\n",
    "            \n",
    "            # Average Surface Distance\n",
    "            metrics['average_surface_distance'] = average_surface_distance(\n",
    "                y_true_np, y_pred_np)\n",
    "            \n",
    "            # Surface Dice Coefficient\n",
    "            metrics['surface_dice'] = surface_dice_coefficient(\n",
    "                y_true_np, y_pred_np, self.surface_tolerance)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error computing distance metrics: {e}\")\n",
    "            metrics['hausdorff_distance'] = float('inf')\n",
    "            metrics['average_surface_distance'] = float('inf')\n",
    "            metrics['surface_dice'] = 0.0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def batch_compute_distance_metrics(self, y_true_batch, y_pred_batch):\n",
    "        \"\"\"\n",
    "        Compute distance metrics for a batch of predictions\n",
    "        \n",
    "        Args:\n",
    "            y_true_batch: Batch of ground truth masks\n",
    "            y_pred_batch: Batch of predicted masks\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with averaged metrics\n",
    "        \"\"\"\n",
    "        batch_metrics = {\n",
    "            'hausdorff_distance': [],\n",
    "            'average_surface_distance': [],\n",
    "            'surface_dice': []\n",
    "        }\n",
    "        \n",
    "        for i in range(len(y_true_batch)):\n",
    "            metrics = self.compute_distance_metrics(y_true_batch[i], y_pred_batch[i])\n",
    "            \n",
    "            for key, value in metrics.items():\n",
    "                if not np.isinf(value) and not np.isnan(value):\n",
    "                    batch_metrics[key].append(value)\n",
    "        \n",
    "        # Calculate averages\n",
    "        averaged_metrics = {}\n",
    "        for key, values in batch_metrics.items():\n",
    "            if values:\n",
    "                averaged_metrics[f'{key}_mean'] = np.mean(values)\n",
    "                averaged_metrics[f'{key}_std'] = np.std(values)\n",
    "                averaged_metrics[f'{key}_median'] = np.median(values)\n",
    "            else:\n",
    "                averaged_metrics[f'{key}_mean'] = float('inf')\n",
    "                averaged_metrics[f'{key}_std'] = 0.0\n",
    "                averaged_metrics[f'{key}_median'] = float('inf')\n",
    "        \n",
    "        return averaged_metrics\n",
    "\n",
    "# Initialize distance metrics calculator\n",
    "distance_metrics = DistanceMetrics()\n",
    "\n",
    "print(\"Distance-Based Metrics Implemented:\")\n",
    "print(\"- Hausdorff Distance (with percentile option)\")\n",
    "print(\"- Average Surface Distance (ASD)\")\n",
    "print(\"- Surface Dice Coefficient\")\n",
    "print(\"- DistanceMetrics class for batch processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4155954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Comprehensive Evaluation Framework Implemented:\n",
      "- SegmentationEvaluator class\n",
      "- Single prediction evaluation\n",
      "- Batch evaluation with aggregation\n",
      "- Metrics reporting\n",
      "- Loss function testing utilities\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Evaluation Framework\n",
    "\n",
    "class SegmentationEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation framework for medical image segmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.5, smooth=1e-6, hausdorff_percentile=95):\n",
    "        self.threshold = threshold\n",
    "        self.smooth = smooth\n",
    "        self.medical_metrics = MedicalMetrics(threshold, smooth)\n",
    "        self.distance_metrics = DistanceMetrics(hausdorff_percentile)\n",
    "        \n",
    "    def evaluate_single_prediction(self, y_true, y_pred, include_distance=True):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of a single prediction\n",
    "        \n",
    "        Args:\n",
    "            y_true: Ground truth mask\n",
    "            y_pred: Predicted mask\n",
    "            include_distance: Whether to compute distance-based metrics\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all computed metrics\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Convert to numpy if needed\n",
    "        if hasattr(y_true, 'numpy'):\n",
    "            y_true_np = y_true.numpy()\n",
    "            y_pred_np = y_pred.numpy()\n",
    "        else:\n",
    "            y_true_np = y_true\n",
    "            y_pred_np = y_pred\n",
    "        \n",
    "        # Basic medical metrics\n",
    "        basic_metrics = self.medical_metrics.compute_metrics_numpy(y_true_np, y_pred_np)\n",
    "        results.update(basic_metrics)\n",
    "        \n",
    "        # Distance-based metrics (optional, as they can be computationally expensive)\n",
    "        if include_distance:\n",
    "            try:\n",
    "                distance_metrics_result = self.distance_metrics.compute_distance_metrics(\n",
    "                    y_true_np, y_pred_np)\n",
    "                results.update(distance_metrics_result)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not compute distance metrics: {e}\")\n",
    "                results.update({\n",
    "                    'hausdorff_distance': float('inf'),\n",
    "                    'average_surface_distance': float('inf'),\n",
    "                    'surface_dice': 0.0\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_batch(self, y_true_batch, y_pred_batch, include_distance=False):\n",
    "        \"\"\"\n",
    "        Evaluate a batch of predictions\n",
    "        \n",
    "        Args:\n",
    "            y_true_batch: Batch of ground truth masks\n",
    "            y_pred_batch: Batch of predicted masks\n",
    "            include_distance: Whether to compute distance-based metrics\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with aggregated metrics\n",
    "        \"\"\"\n",
    "        all_metrics = []\n",
    "        \n",
    "        for i in range(len(y_true_batch)):\n",
    "            metrics = self.evaluate_single_prediction(\n",
    "                y_true_batch[i], y_pred_batch[i], include_distance)\n",
    "            all_metrics.append(metrics)\n",
    "        \n",
    "        # Aggregate results\n",
    "        aggregated = self._aggregate_metrics(all_metrics)\n",
    "        return aggregated\n",
    "    \n",
    "    def _aggregate_metrics(self, metrics_list):\n",
    "        \"\"\"\n",
    "        Aggregate metrics from multiple predictions\n",
    "        \n",
    "        Args:\n",
    "            metrics_list: List of metric dictionaries\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with aggregated statistics\n",
    "        \"\"\"\n",
    "        if not metrics_list:\n",
    "            return {}\n",
    "        \n",
    "        # Get all metric names\n",
    "        metric_names = metrics_list[0].keys()\n",
    "        aggregated = {}\n",
    "        \n",
    "        for metric_name in metric_names:\n",
    "            values = [m[metric_name] for m in metrics_list \n",
    "                     if not np.isinf(m[metric_name]) and not np.isnan(m[metric_name])]\n",
    "            \n",
    "            if values:\n",
    "                aggregated[f'{metric_name}_mean'] = np.mean(values)\n",
    "                aggregated[f'{metric_name}_std'] = np.std(values)\n",
    "                aggregated[f'{metric_name}_median'] = np.median(values)\n",
    "                aggregated[f'{metric_name}_min'] = np.min(values)\n",
    "                aggregated[f'{metric_name}_max'] = np.max(values)\n",
    "                aggregated[f'{metric_name}_count'] = len(values)\n",
    "            else:\n",
    "                aggregated[f'{metric_name}_mean'] = float('nan')\n",
    "                aggregated[f'{metric_name}_std'] = float('nan')\n",
    "                aggregated[f'{metric_name}_median'] = float('nan')\n",
    "                aggregated[f'{metric_name}_min'] = float('nan')\n",
    "                aggregated[f'{metric_name}_max'] = float('nan')\n",
    "                aggregated[f'{metric_name}_count'] = 0\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def create_metrics_report(self, metrics_dict, title=\"Segmentation Evaluation Report\"):\n",
    "        \"\"\"\n",
    "        Create a formatted report from metrics\n",
    "        \n",
    "        Args:\n",
    "            metrics_dict: Dictionary with computed metrics\n",
    "            title: Report title\n",
    "        \n",
    "        Returns:\n",
    "            Formatted string report\n",
    "        \"\"\"\n",
    "        report = f\"\\n{'='*60}\\n{title}\\n{'='*60}\\n\"\n",
    "        \n",
    "        # Group metrics by type\n",
    "        basic_metrics = ['dice', 'iou', 'sensitivity', 'specificity', 'precision', 'f1', 'volume_similarity']\n",
    "        distance_metrics = ['hausdorff_distance', 'average_surface_distance', 'surface_dice']\n",
    "        \n",
    "        # Basic metrics\n",
    "        report += \"\\n📊 BASIC SEGMENTATION METRICS\\n\" + \"-\"*40 + \"\\n\"\n",
    "        for metric in basic_metrics:\n",
    "            if f'{metric}_mean' in metrics_dict:\n",
    "                mean_val = metrics_dict[f'{metric}_mean']\n",
    "                std_val = metrics_dict[f'{metric}_std']\n",
    "                report += f\"{metric.replace('_', ' ').title():20}: {mean_val:.4f} ± {std_val:.4f}\\n\"\n",
    "        \n",
    "        # Distance metrics\n",
    "        report += \"\\n📏 DISTANCE-BASED METRICS\\n\" + \"-\"*40 + \"\\n\"\n",
    "        for metric in distance_metrics:\n",
    "            if f'{metric}_mean' in metrics_dict:\n",
    "                mean_val = metrics_dict[f'{metric}_mean']\n",
    "                std_val = metrics_dict[f'{metric}_std']\n",
    "                if not np.isinf(mean_val):\n",
    "                    report += f\"{metric.replace('_', ' ').title():20}: {mean_val:.4f} ± {std_val:.4f}\\n\"\n",
    "                else:\n",
    "                    report += f\"{metric.replace('_', ' ').title():20}: Not computed\\n\"\n",
    "        \n",
    "        report += \"\\n\" + \"=\"*60 + \"\\n\"\n",
    "        return report\n",
    "\n",
    "# Loss Function Testing and Comparison\n",
    "def test_loss_functions():\n",
    "    \"\"\"\n",
    "    Test different loss functions with synthetic data\n",
    "    \"\"\"\n",
    "    print(\"🧪 Testing Loss Functions with Synthetic Data\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create synthetic data\n",
    "    batch_size, height, width = 4, 128, 128\n",
    "    \n",
    "    # Ground truth: circular mask\n",
    "    y_true = np.zeros((batch_size, height, width, 1), dtype=np.float32)\n",
    "    center = height // 2\n",
    "    radius = 30\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                if (i - center)**2 + (j - center)**2 <= radius**2:\n",
    "                    y_true[b, i, j, 0] = 1.0\n",
    "    \n",
    "    # Prediction: slightly offset circular mask\n",
    "    y_pred = np.zeros_like(y_true)\n",
    "    offset_center = center + 5\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                if (i - offset_center)**2 + (j - offset_center)**2 <= radius**2:\n",
    "                    y_pred[b, i, j, 0] = 0.8  # Soft prediction\n",
    "    \n",
    "    # Convert to tensors\n",
    "    y_true_tf = tf.constant(y_true)\n",
    "    y_pred_tf = tf.constant(y_pred)\n",
    "    \n",
    "    # Test different loss functions\n",
    "    loss_functions = {\n",
    "        'Dice Loss': dice_loss,\n",
    "        'BCE Loss': binary_crossentropy_loss,\n",
    "        'Focal Loss': lambda yt, yp: focal_loss(yt, yp, alpha=0.25, gamma=2.0),\n",
    "        'Tversky Loss': lambda yt, yp: tversky_loss(yt, yp, alpha=0.3, beta=0.7),\n",
    "        'Dice+BCE Loss': lambda yt, yp: dice_bce_loss(yt, yp, 0.5, 0.5),\n",
    "        'Boundary Loss': boundary_loss,\n",
    "    }\n",
    "    \n",
    "    print(\"Loss Function Comparison:\")\n",
    "    for name, loss_fn in loss_functions.items():\n",
    "        try:\n",
    "            with tf.GradientTape():\n",
    "                loss_value = loss_fn(y_true_tf, y_pred_tf)\n",
    "            print(f\"{name:15}: {loss_value.numpy():.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{name:15}: Error - {str(e)[:50]}\")\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = SegmentationEvaluator()\n",
    "\n",
    "print(\"\\n✅ Comprehensive Evaluation Framework Implemented:\")\n",
    "print(\"- SegmentationEvaluator class\")\n",
    "print(\"- Single prediction evaluation\")\n",
    "print(\"- Batch evaluation with aggregation\")\n",
    "print(\"- Metrics reporting\")\n",
    "print(\"- Loss function testing utilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88071c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Visualization and Demonstration Functions:\n",
      "- visualize_loss_functions(): Compare loss function behaviors\n",
      "- demonstrate_metrics_calculation(): Show comprehensive evaluation\n",
      "- plot_metrics_comparison(): Compare metrics across scenarios\n",
      "\n",
      "Run these functions to see the visualizations!\n"
     ]
    }
   ],
   "source": [
    "# Visualization and Demonstration\n",
    "\n",
    "def visualize_loss_functions():\n",
    "    \"\"\"\n",
    "    Visualize the behavior of different loss functions\n",
    "    \"\"\"\n",
    "    print(\"📈 Visualizing Loss Function Behavior\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create test data with varying prediction quality\n",
    "    prediction_qualities = np.linspace(0.1, 0.9, 20)\n",
    "    \n",
    "    # Create synthetic ground truth (simple circle)\n",
    "    size = 64\n",
    "    y_true = np.zeros((size, size))\n",
    "    center = size // 2\n",
    "    radius = 15\n",
    "    \n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if (i - center)**2 + (j - center)**2 <= radius**2:\n",
    "                y_true[i, j] = 1.0\n",
    "    \n",
    "    # Calculate losses for different prediction qualities\n",
    "    loss_results = {\n",
    "        'Dice Loss': [],\n",
    "        'BCE Loss': [],\n",
    "        'Focal Loss': [],\n",
    "        'Tversky Loss': [],\n",
    "        'Dice+BCE': []\n",
    "    }\n",
    "    \n",
    "    for quality in prediction_qualities:\n",
    "        # Create prediction with varying quality\n",
    "        y_pred = y_true * quality + (1 - y_true) * (1 - quality) * 0.1\n",
    "        \n",
    "        # Convert to tensors\n",
    "        y_true_tf = tf.constant(y_true.reshape(1, size, size, 1), dtype=tf.float32)\n",
    "        y_pred_tf = tf.constant(y_pred.reshape(1, size, size, 1), dtype=tf.float32)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss_results['Dice Loss'].append(dice_loss(y_true_tf, y_pred_tf).numpy())\n",
    "        loss_results['BCE Loss'].append(binary_crossentropy_loss(y_true_tf, y_pred_tf).numpy())\n",
    "        loss_results['Focal Loss'].append(focal_loss(y_true_tf, y_pred_tf).numpy())\n",
    "        loss_results['Tversky Loss'].append(tversky_loss(y_true_tf, y_pred_tf).numpy())\n",
    "        loss_results['Dice+BCE'].append(dice_bce_loss(y_true_tf, y_pred_tf).numpy())\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for loss_name, loss_values in loss_results.items():\n",
    "        plt.plot(prediction_qualities, loss_values, marker='o', label=loss_name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Prediction Quality (IoU-like measure)')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('Comparison of Loss Functions vs Prediction Quality')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return loss_results\n",
    "\n",
    "def demonstrate_metrics_calculation():\n",
    "    \"\"\"\n",
    "    Demonstrate metrics calculation with real examples\n",
    "    \"\"\"\n",
    "    print(\"🔍 Demonstrating Metrics Calculation\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Generate synthetic test data\n",
    "    y_true_test, y_pred_test = test_loss_functions()\n",
    "    \n",
    "    # Evaluate with comprehensive metrics\n",
    "    print(\"\\nEvaluating synthetic predictions...\")\n",
    "    results = evaluator.evaluate_batch(y_true_test, y_pred_test, include_distance=False)\n",
    "    \n",
    "    # Generate report\n",
    "    report = evaluator.create_metrics_report(results, \"Synthetic Data Evaluation\")\n",
    "    print(report)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_metrics_comparison():\n",
    "    \"\"\"\n",
    "    Create a comprehensive metrics comparison plot\n",
    "    \"\"\"\n",
    "    # Generate different prediction scenarios\n",
    "    scenarios = {\n",
    "        'Perfect': (1.0, 0.0),      # Perfect prediction, no offset\n",
    "        'Good': (0.9, 2),           # Good prediction, small offset\n",
    "        'Moderate': (0.7, 5),       # Moderate prediction, medium offset\n",
    "        'Poor': (0.5, 10)           # Poor prediction, large offset\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    # Create test data for each scenario\n",
    "    for scenario_name, (quality, offset) in scenarios.items():\n",
    "        # Create synthetic data\n",
    "        size = 64\n",
    "        batch_size = 8\n",
    "        \n",
    "        y_true = np.zeros((batch_size, size, size))\n",
    "        y_pred = np.zeros((batch_size, size, size))\n",
    "        \n",
    "        center = size // 2\n",
    "        radius = 15\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Ground truth\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    if (i - center)**2 + (j - center)**2 <= radius**2:\n",
    "                        y_true[b, i, j] = 1.0\n",
    "            \n",
    "            # Prediction with offset and quality variation\n",
    "            pred_center = center + offset\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    if (i - pred_center)**2 + (j - pred_center)**2 <= radius**2:\n",
    "                        y_pred[b, i, j] = quality\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluator.evaluate_batch(y_true, y_pred, include_distance=False)\n",
    "        all_results[scenario_name] = results\n",
    "    \n",
    "    # Plot comparison\n",
    "    metrics_to_plot = ['dice_mean', 'iou_mean', 'sensitivity_mean', 'specificity_mean', 'f1_mean']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(20, 4))\n",
    "    fig.suptitle('Metrics Comparison Across Different Prediction Scenarios', fontsize=16)\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        values = [all_results[scenario][metric] for scenario in scenarios.keys()]\n",
    "        \n",
    "        bars = axes[idx].bar(scenarios.keys(), values, alpha=0.7, \n",
    "                           color=['green', 'blue', 'orange', 'red'])\n",
    "        axes[idx].set_title(metric.replace('_mean', '').replace('_', ' ').title())\n",
    "        axes[idx].set_ylabel('Score')\n",
    "        axes[idx].set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            axes[idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                          f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"📊 Visualization and Demonstration Functions:\")\n",
    "print(\"- visualize_loss_functions(): Compare loss function behaviors\")\n",
    "print(\"- demonstrate_metrics_calculation(): Show comprehensive evaluation\")\n",
    "print(\"- plot_metrics_comparison(): Compare metrics across scenarios\")\n",
    "print(\"\\nRun these functions to see the visualizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08350aa",
   "metadata": {},
   "source": [
    "## 📋 Summary and Next Steps\n",
    "\n",
    "### ✅ What We've Implemented\n",
    "\n",
    "This notebook provides a comprehensive suite of loss functions and metrics specifically designed for medical image segmentation:\n",
    "\n",
    "#### **Loss Functions**\n",
    "1. **Individual Loss Functions**:\n",
    "   - Dice Loss (standard and generalized)\n",
    "   - Binary Cross-Entropy Loss\n",
    "   - Focal Loss (for handling class imbalance)\n",
    "   - Tversky Loss (generalized Dice)\n",
    "\n",
    "2. **Hybrid Loss Functions**:\n",
    "   - Dice + BCE Loss (balanced approach)\n",
    "   - Focal + Dice Loss (imbalance-aware)\n",
    "   - Tversky + Focal Loss (advanced combination)\n",
    "   - Adaptive Loss (dynamic weighting)\n",
    "\n",
    "3. **Boundary-Aware Losses**:\n",
    "   - Boundary Loss (edge detection focus)\n",
    "   - Surface Loss (boundary alignment)\n",
    "   - Hausdorff Loss (distance-based)\n",
    "   - Combined Boundary-Aware Loss\n",
    "\n",
    "#### **Evaluation Metrics**\n",
    "1. **Basic Segmentation Metrics**:\n",
    "   - Dice Coefficient\n",
    "   - IoU (Intersection over Union)\n",
    "   - Sensitivity (Recall)\n",
    "   - Specificity\n",
    "   - Precision\n",
    "   - F1 Score\n",
    "   - Volume Similarity\n",
    "\n",
    "2. **Distance-Based Metrics**:\n",
    "   - Hausdorff Distance (with percentile option)\n",
    "   - Average Surface Distance\n",
    "   - Surface Dice Coefficient\n",
    "\n",
    "3. **Evaluation Framework**:\n",
    "   - Comprehensive evaluator class\n",
    "   - Batch processing capabilities\n",
    "   - Statistical aggregation\n",
    "   - Formatted reporting\n",
    "\n",
    "### 🔧 Key Features\n",
    "\n",
    "- **Medical-Specific Design**: All functions optimized for medical segmentation challenges\n",
    "- **Numerical Stability**: Proper handling of edge cases and numerical issues\n",
    "- **Modular Architecture**: Easy to integrate with training pipelines\n",
    "- **Comprehensive Evaluation**: Both basic and advanced metrics\n",
    "- **Visualization Tools**: Functions to compare and visualize performance\n",
    "- **Production Ready**: Efficient implementations suitable for training\n",
    "\n",
    "### 🎯 Usage in Training Pipeline\n",
    "\n",
    "```python\n",
    "# Example usage in model compilation\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=dice_bce_loss,  # or any other implemented loss\n",
    "    metrics=[dice_coefficient, iou_score, sensitivity_score]\n",
    ")\n",
    "\n",
    "# Example usage in evaluation\n",
    "evaluator = SegmentationEvaluator()\n",
    "results = evaluator.evaluate_batch(y_true, y_pred, include_distance=True)\n",
    "report = evaluator.create_metrics_report(results)\n",
    "print(report)\n",
    "```\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "1. **Integration with Training Pipeline** (Notebook 05):\n",
    "   - Implement training loop with these loss functions\n",
    "   - Add callbacks for early stopping based on metrics\n",
    "   - Learning rate scheduling strategies\n",
    "\n",
    "2. **Model Evaluation** (Notebook 06):\n",
    "   - Comprehensive model testing\n",
    "   - Validation set evaluation\n",
    "   - Cross-validation strategies\n",
    "\n",
    "3. **Post-processing** (Notebook 07):\n",
    "   - Morphological operations\n",
    "   - Connected component analysis\n",
    "   - Anatomical validation\n",
    "\n",
    "### 💡 Tips for Usage\n",
    "\n",
    "1. **Loss Function Selection**:\n",
    "   - Use Dice + BCE for balanced training\n",
    "   - Use Focal Loss for highly imbalanced datasets\n",
    "   - Use boundary-aware losses when edge precision is critical\n",
    "\n",
    "2. **Metric Interpretation**:\n",
    "   - Dice > 0.8 is generally considered good for medical segmentation\n",
    "   - Hausdorff distance should be interpreted in context of image resolution\n",
    "   - Always report multiple metrics for comprehensive evaluation\n",
    "\n",
    "3. **Computational Considerations**:\n",
    "   - Distance-based metrics are computationally expensive\n",
    "   - Use them for final evaluation, not during training\n",
    "   - Consider batch processing for efficiency\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to proceed to the next notebook: `05_Training_Pipeline.ipynb`** 🎓"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
