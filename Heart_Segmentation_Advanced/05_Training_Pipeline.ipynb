{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdd5279",
   "metadata": {},
   "source": [
    "# üöÄ Training Pipeline for Cardiac Segmentation\n",
    "\n",
    "This notebook implements a comprehensive training pipeline for cardiac MRI segmentation using advanced U-Net architectures. We'll integrate all the components developed in previous notebooks into a robust training system with proper callbacks, monitoring, and optimization strategies.\n",
    "\n",
    "## Objectives\n",
    "- Implement complete training pipeline with data loading\n",
    "- Integrate advanced loss functions and metrics\n",
    "- Add comprehensive callbacks (early stopping, learning rate scheduling)\n",
    "- Implement mixed precision training for efficiency\n",
    "- Add tensorboard logging and monitoring\n",
    "- Create model checkpointing and recovery\n",
    "- Implement cross-validation strategies\n",
    "\n",
    "## Key Components\n",
    "1. **Training Configuration**: Centralized training parameters\n",
    "2. **Data Pipeline**: Efficient data loading and augmentation\n",
    "3. **Model Training**: Advanced training loop with callbacks\n",
    "4. **Monitoring**: Real-time metrics tracking and visualization\n",
    "5. **Checkpointing**: Model saving and recovery strategies\n",
    "6. **Validation**: Comprehensive validation and testing\n",
    "7. **Optimization**: Learning rate scheduling and mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9511bf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Some classes not available. Please run previous notebooks first.\n",
      "‚úÖ Project configuration loaded\n",
      "PyTorch version: 2.7.1+cpu\n",
      "Device: cpu\n",
      "‚úÖ Environment setup complete!\n",
      "üìÅ Project: c:\\Users\\leonardo.costa\\OneDrive - Lightera, LLC\\Documentos\\GitHub\\pratica-aprendizado-de-maquina\\Heart_Segmentation_Advanced\n",
      "üìä Dataset: c:\\Users\\leonardo.costa\\OneDrive - Lightera, LLC\\Documentos\\GitHub\\pratica-aprendizado-de-maquina\\Heart_Segmentation_Advanced\\Task02_Heart\n",
      "üíæ Models: c:\\Users\\leonardo.costa\\OneDrive - Lightera, LLC\\Documentos\\GitHub\\pratica-aprendizado-de-maquina\\Heart_Segmentation_Advanced\\models\n",
      "üì§ Outputs: c:\\Users\\leonardo.costa\\OneDrive - Lightera, LLC\\Documentos\\GitHub\\pratica-aprendizado-de-maquina\\Heart_Segmentation_Advanced\\outputs\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries and Dependencies\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader as TorchDataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Scikit-learn for metrics and validation\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load configuration from previous notebooks\n",
    "# Run previous notebooks first to have all required classes available\n",
    "try:\n",
    "    # This will work if the previous notebook cells were executed\n",
    "    from __main__ import SegmentationEvaluator, AttentionUNet, DiceLoss, DiceBCELoss\n",
    "    print(\"‚úÖ Imported classes from previous notebooks\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Some classes not available. Please run previous notebooks first.\")\n",
    "    # Import basic classes that should be available from previous notebooks\n",
    "    \n",
    "# Since we need to proceed, let me define the essential classes inline\n",
    "# These should ideally be imported from previous notebooks\n",
    "\n",
    "# Simple metrics evaluator for now\n",
    "class SimpleSegmentationEvaluator:\n",
    "    \"\"\"Simplified version for testing\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate_batch(self, y_true, y_pred, include_distance=False):\n",
    "        \"\"\"Basic batch evaluation\"\"\"\n",
    "        # Convert to numpy if needed\n",
    "        if hasattr(y_true, 'cpu'):\n",
    "            y_true = y_true.cpu().numpy()\n",
    "        if hasattr(y_pred, 'cpu'):\n",
    "            y_pred = y_pred.cpu().numpy()\n",
    "        \n",
    "        # Calculate basic metrics\n",
    "        intersection = np.sum(y_true * y_pred)\n",
    "        union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "        \n",
    "        dice = (2 * intersection + 1e-8) / (np.sum(y_true) + np.sum(y_pred) + 1e-8)\n",
    "        iou = (intersection + 1e-8) / (union + 1e-8)\n",
    "        \n",
    "        return {\n",
    "            \"dice\": float(dice),\n",
    "            \"iou\": float(iou),\n",
    "            \"precision\": float(intersection / (np.sum(y_pred) + 1e-8)),\n",
    "            \"recall\": float(intersection / (np.sum(y_true) + 1e-8))\n",
    "        }\n",
    "    \n",
    "    def create_metrics_report(self, results, title):\n",
    "        \"\"\"Create a simple metrics report\"\"\"\n",
    "        report = f\"\\n{title}\\n\" + \"=\"*50 + \"\\n\"\n",
    "        for metric, value in results.items():\n",
    "            report += f\"{metric:15}: {value:.4f}\\n\"\n",
    "        return report\n",
    "\n",
    "# Use the simplified evaluator\n",
    "SegmentationEvaluator = SimpleSegmentationEvaluator\n",
    "try:\n",
    "    with open('project_config.json', 'r') as f:\n",
    "        project_config = json.load(f)\n",
    "    print(\"‚úÖ Project configuration loaded\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Execute 00_Setup_and_Configuration.ipynb first\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Configure PyTorch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set up paths from config\n",
    "PROJECT_PATH = project_config['paths']['project']\n",
    "DATASET_PATH = project_config['paths']['dataset'] \n",
    "MODEL_DIR = os.path.join(PROJECT_PATH, 'models')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_PATH, 'outputs')\n",
    "LOGS_DIR = os.path.join(OUTPUT_DIR, 'logs')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [MODEL_DIR, OUTPUT_DIR, LOGS_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üìÅ Project: {PROJECT_PATH}\")\n",
    "print(f\"üìä Dataset: {DATASET_PATH}\")\n",
    "print(f\"üíæ Models: {MODEL_DIR}\")\n",
    "print(f\"üì§ Outputs: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09b6fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Training Configuration\n",
      "==================================================\n",
      "\n",
      "üìä Data Parameters\n",
      "------------------------------\n",
      "  image_size               : (256, 256)\n",
      "  batch_size               : 8\n",
      "  validation_split         : 0.2\n",
      "  test_split               : 0.1\n",
      "\n",
      "üéØ Training Parameters\n",
      "------------------------------\n",
      "  epochs                   : 100\n",
      "  initial_learning_rate    : 0.0001\n",
      "  min_learning_rate        : 1e-07\n",
      "\n",
      "üèóÔ∏è Model Parameters\n",
      "------------------------------\n",
      "  model_name               : attention_unet\n",
      "  backbone                 : efficientnetb0\n",
      "  use_pretrained           : True\n",
      "  dropout_rate             : 0.2\n",
      "\n",
      "‚öñÔ∏è Loss Parameters\n",
      "------------------------------\n",
      "  loss_function            : dice_bce\n",
      "  dice_weight              : 0.5\n",
      "  bce_weight               : 0.5\n",
      "\n",
      "üõ°Ô∏è Regularization\n",
      "------------------------------\n",
      "  early_stopping_patience  : 20\n",
      "  weight_decay             : 1e-05\n",
      "  gradient_clip_norm       : 1.0\n",
      "\n",
      "üîÑ Augmentation\n",
      "------------------------------\n",
      "  use_augmentation         : True\n",
      "  augmentation_probability : 0.8\n",
      "  rotation_range           : 20\n",
      "\n",
      "üìà Monitoring\n",
      "------------------------------\n",
      "  monitor_metric           : val_dice_coefficient\n",
      "  monitor_mode             : max\n",
      "  experiment_name          : cardiac_segmentation_20250618_133029\n",
      "Configuration saved to: c:\\Users\\leonardo.costa\\OneDrive - Lightera, LLC\\Documentos\\GitHub\\pratica-aprendizado-de-maquina\\Heart_Segmentation_Advanced\\outputs\\cardiac_segmentation_20250618_133029_config.json\n",
      "\n",
      "‚úÖ Configuration initialized and saved!\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration Class\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Comprehensive configuration for training pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Data parameters\n",
    "        self.image_size = (256, 256)\n",
    "        self.batch_size = 8\n",
    "        self.validation_split = 0.2\n",
    "        self.test_split = 0.1\n",
    "        self.num_classes = 1  # Binary segmentation\n",
    "        \n",
    "        # Training parameters\n",
    "        self.epochs = 100\n",
    "        self.initial_learning_rate = 1e-4\n",
    "        self.min_learning_rate = 1e-7\n",
    "        self.learning_rate_patience = 10\n",
    "        self.learning_rate_factor = 0.5\n",
    "        \n",
    "        # Model parameters\n",
    "        self.model_name = 'attention_unet'\n",
    "        self.backbone = 'efficientnetb0'\n",
    "        self.use_pretrained = True\n",
    "        self.dropout_rate = 0.2\n",
    "        self.batch_norm = True\n",
    "        \n",
    "        # Loss function parameters\n",
    "        self.loss_function = 'dice_bce'\n",
    "        self.dice_weight = 0.5\n",
    "        self.bce_weight = 0.5\n",
    "        self.focal_alpha = 0.25\n",
    "        self.focal_gamma = 2.0\n",
    "        \n",
    "        # Regularization parameters\n",
    "        self.early_stopping_patience = 20\n",
    "        self.early_stopping_min_delta = 1e-4\n",
    "        self.weight_decay = 1e-5\n",
    "        self.gradient_clip_norm = 1.0\n",
    "        \n",
    "        # Augmentation parameters\n",
    "        self.use_augmentation = True\n",
    "        self.augmentation_probability = 0.8\n",
    "        self.rotation_range = 20\n",
    "        self.zoom_range = 0.1\n",
    "        self.intensity_range = 0.1\n",
    "        \n",
    "        # Monitoring parameters\n",
    "        self.monitor_metric = 'val_dice_coefficient'\n",
    "        self.monitor_mode = 'max'\n",
    "        self.save_best_only = True\n",
    "        self.save_weights_only = False\n",
    "        \n",
    "        # Mixed precision\n",
    "        self.use_mixed_precision = True\n",
    "        \n",
    "        # Cross-validation\n",
    "        self.use_cross_validation = False\n",
    "        self.cv_folds = 5\n",
    "        \n",
    "        # Logging\n",
    "        self.log_dir = str(LOGS_DIR)\n",
    "        self.experiment_name = f\"cardiac_segmentation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.verbose = 1\n",
    "        \n",
    "    def get_model_name(self):\n",
    "        \"\"\"Generate descriptive model name\"\"\"\n",
    "        name_parts = [\n",
    "            self.model_name,\n",
    "            self.backbone if self.backbone else 'custom',\n",
    "            self.loss_function,\n",
    "            f'bs{self.batch_size}',\n",
    "            f'lr{self.initial_learning_rate}'\n",
    "        ]\n",
    "        return '_'.join(name_parts)\n",
    "    \n",
    "    def save_config(self, path=None):\n",
    "        \"\"\"Save configuration to JSON file\"\"\"\n",
    "        if path is None:\n",
    "            path = os.path.join(OUTPUT_DIR, f\"{self.experiment_name}_config.json\")\n",
    "        \n",
    "        config_dict = {k: v for k, v in self.__dict__.items() \n",
    "                      if not k.startswith('_') and isinstance(v, (str, int, float, bool, list, tuple))}\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "        \n",
    "        print(f\"Configuration saved to: {path}\")\n",
    "        return path\n",
    "    \n",
    "    @classmethod\n",
    "    def load_config(cls, path):\n",
    "        \"\"\"Load configuration from JSON file\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        \n",
    "        config = cls()\n",
    "        for key, value in config_dict.items():\n",
    "            setattr(config, key, value)\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def display_config(self):\n",
    "        \"\"\"Display current configuration\"\"\"\n",
    "        print(\"üîß Training Configuration\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        sections = {\n",
    "            \"üìä Data Parameters\": ['image_size', 'batch_size', 'validation_split', 'test_split'],\n",
    "            \"üéØ Training Parameters\": ['epochs', 'initial_learning_rate', 'min_learning_rate'],\n",
    "            \"üèóÔ∏è Model Parameters\": ['model_name', 'backbone', 'use_pretrained', 'dropout_rate'],\n",
    "            \"‚öñÔ∏è Loss Parameters\": ['loss_function', 'dice_weight', 'bce_weight'],\n",
    "            \"üõ°Ô∏è Regularization\": ['early_stopping_patience', 'weight_decay', 'gradient_clip_norm'],\n",
    "            \"üîÑ Augmentation\": ['use_augmentation', 'augmentation_probability', 'rotation_range'],\n",
    "            \"üìà Monitoring\": ['monitor_metric', 'monitor_mode', 'experiment_name']\n",
    "        }\n",
    "        \n",
    "        for section_name, params in sections.items():\n",
    "            print(f\"\\n{section_name}\")\n",
    "            print(\"-\" * 30)\n",
    "            for param in params:\n",
    "                if hasattr(self, param):\n",
    "                    value = getattr(self, param)\n",
    "                    print(f\"  {param:25}: {value}\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = TrainingConfig()\n",
    "config.display_config()\n",
    "\n",
    "# Save configuration\n",
    "config_path = config.save_config()\n",
    "print(f\"\\n‚úÖ Configuration initialized and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b79540cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced Data Pipeline initialized!\n",
      "üì¶ Features:\n",
      "  - TensorFlow native augmentation\n",
      "  - Synchronized image-mask augmentation\n",
      "  - Efficient data loading with prefetch\n",
      "  - Automatic data splitting\n",
      "  - Configurable preprocessing\n"
     ]
    }
   ],
   "source": [
    "# Advanced Data Pipeline\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "from skimage import transform\n",
    "import random\n",
    "\n",
    "class CardiacDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for cardiac MRI segmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, config, is_training=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.config = config\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # Create transforms\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(config.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485], std=[0.229])  # ImageNet stats for grayscale\n",
    "        ])\n",
    "        \n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(config.image_size, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # Augmentation transforms\n",
    "        if is_training and config.use_augmentation:\n",
    "            self.augment_transform = transforms.Compose([\n",
    "                transforms.RandomRotation(degrees=config.rotation_range),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "            ])\n",
    "        else:\n",
    "            self.augment_transform = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = self.load_nifti(self.image_paths[idx])\n",
    "        mask = self.load_nifti(self.mask_paths[idx])\n",
    "        \n",
    "        # Convert to uint8 for PIL compatibility\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        mask = (mask * 255).astype(np.uint8)\n",
    "        \n",
    "        # Apply transforms\n",
    "        image = self.image_transform(image)\n",
    "        mask = self.mask_transform(mask)\n",
    "        \n",
    "        # Apply augmentation if training\n",
    "        if self.is_training and self.augment_transform and random.random() < self.config.augmentation_probability:\n",
    "            # Apply same augmentation to both image and mask\n",
    "            seed = torch.randint(0, 2**32, (1,)).item()\n",
    "            \n",
    "            torch.manual_seed(seed)\n",
    "            image = self.augment_transform(image)\n",
    "            \n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.augment_transform(mask)\n",
    "        \n",
    "        # Ensure mask is binary\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def load_nifti(self, path):\n",
    "        \"\"\"Load NIfTI file and return 2D slice\"\"\"\n",
    "        img = nib.load(path)\n",
    "        data = img.get_fdata()\n",
    "        # Get middle slice for 2D processing\n",
    "        if len(data.shape) == 3:\n",
    "            slice_idx = data.shape[2] // 2\n",
    "            data = data[:, :, slice_idx]\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        data = (data - data.min()) / (data.max() - data.min() + 1e-8)\n",
    "        return data\n",
    "\n",
    "class TrainingDataPipeline:\n",
    "    \"\"\"\n",
    "    Advanced PyTorch data pipeline with augmentation and preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        \n",
    "    def load_data_paths(self):\n",
    "        \"\"\"Load image and mask file paths\"\"\"\n",
    "        import glob\n",
    "        \n",
    "        # Get all image files\n",
    "        image_pattern = os.path.join(DATASET_PATH, \"imagesTr\", \"*.nii.gz\")\n",
    "        image_paths = sorted(glob.glob(image_pattern))\n",
    "        \n",
    "        # Get corresponding mask files\n",
    "        mask_paths = []\n",
    "        for img_path in image_paths:\n",
    "            filename = os.path.basename(img_path)\n",
    "            mask_filename = filename.replace(\"_0000\", \"\")  # Remove modality suffix\n",
    "            mask_path = os.path.join(DATASET_PATH, \"labelsTr\", mask_filename)\n",
    "            \n",
    "            if os.path.exists(mask_path):\n",
    "                mask_paths.append(mask_path)\n",
    "            else:\n",
    "                print(f\"Warning: Mask not found for {filename}\")\n",
    "        \n",
    "        # Ensure we have matching pairs\n",
    "        if len(image_paths) != len(mask_paths):\n",
    "            print(f\"Warning: Mismatch in image/mask pairs: {len(image_paths)} images, {len(mask_paths)} masks\")\n",
    "            min_len = min(len(image_paths), len(mask_paths))\n",
    "            image_paths = image_paths[:min_len]\n",
    "            mask_paths = mask_paths[:min_len]\n",
    "        \n",
    "        return image_paths, mask_paths\n",
    "    \n",
    "    def split_data(self, image_paths, mask_paths):\n",
    "        \"\"\"Split data into train/validation/test sets\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # First split: train+val vs test\n",
    "        train_val_img, test_img, train_val_mask, test_mask = train_test_split(\n",
    "            image_paths, mask_paths, \n",
    "            test_size=self.config.test_split, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Second split: train vs val\n",
    "        val_size = self.config.validation_split / (1 - self.config.test_split)\n",
    "        train_img, val_img, train_mask, val_mask = train_test_split(\n",
    "            train_val_img, train_val_mask,\n",
    "            test_size=val_size,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        return (train_img, train_mask), (val_img, val_mask), (test_img, test_mask)\n",
    "    \n",
    "    def create_datasets(self):\n",
    "        \"\"\"Create train, validation, and test datasets\"\"\"\n",
    "        # Load data paths\n",
    "        image_paths, mask_paths = self.load_data_paths()\n",
    "        \n",
    "        print(f\"Found {len(image_paths)} image-mask pairs\")\n",
    "        \n",
    "        # Split data\n",
    "        (train_img, train_mask), (val_img, val_mask), (test_img, test_mask) = self.split_data(\n",
    "            image_paths, mask_paths\n",
    "        )\n",
    "        \n",
    "        print(f\"Train: {len(train_img)} pairs\")\n",
    "        print(f\"Validation: {len(val_img)} pairs\")\n",
    "        print(f\"Test: {len(test_img)} pairs\")\n",
    "        \n",
    "        # Create datasets\n",
    "        self.train_dataset = CardiacDataset(train_img, train_mask, self.config, is_training=True)\n",
    "        self.val_dataset = CardiacDataset(val_img, val_mask, self.config, is_training=False)\n",
    "        self.test_dataset = CardiacDataset(test_img, test_mask, self.config, is_training=False)\n",
    "        \n",
    "        return self.train_dataset, self.val_dataset, self.test_dataset\n",
    "    \n",
    "    def create_dataloaders(self):\n",
    "        \"\"\"Create PyTorch DataLoaders\"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            self.create_datasets()\n",
    "        \n",
    "        train_loader = TorchDataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = TorchDataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        test_loader = TorchDataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader, test_loader\n",
    "    \n",
    "    def get_training_info(self):\n",
    "        \"\"\"Get training information\"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            self.create_datasets()\n",
    "        \n",
    "        train_steps = len(self.train_dataset) // self.config.batch_size\n",
    "        validation_steps = len(self.val_dataset) // self.config.batch_size\n",
    "        \n",
    "        return {\n",
    "            'train_steps': train_steps,\n",
    "            'validation_steps': validation_steps,\n",
    "            'train_samples': len(self.train_dataset),\n",
    "            'val_samples': len(self.val_dataset),\n",
    "            'test_samples': len(self.test_dataset)\n",
    "        }\n",
    "    \n",
    "    def split_data(self, image_paths, mask_paths):\n",
    "        \"\"\"\n",
    "        Split data into train, validation, and test sets\n",
    "        \"\"\"\n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(len(image_paths))\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        total_samples = len(image_paths)\n",
    "        test_size = int(total_samples * self.config.test_split)\n",
    "        val_size = int(total_samples * self.config.validation_split)\n",
    "        train_size = total_samples - test_size - val_size\n",
    "        \n",
    "        # Split indices\n",
    "        test_indices = indices[:test_size]\n",
    "        val_indices = indices[test_size:test_size + val_size]\n",
    "        train_indices = indices[test_size + val_size:]\n",
    "        \n",
    "        # Create splits\n",
    "        splits = {\n",
    "            'train': {\n",
    "                'images': [image_paths[i] for i in train_indices],\n",
    "                'masks': [mask_paths[i] for i in train_indices]\n",
    "            },\n",
    "            'val': {\n",
    "                'images': [image_paths[i] for i in val_indices],\n",
    "                'masks': [mask_paths[i] for i in val_indices]\n",
    "            },\n",
    "            'test': {\n",
    "                'images': [image_paths[i] for i in test_indices],\n",
    "                'masks': [mask_paths[i] for i in test_indices]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Print split information\n",
    "        print(\"üìä Data Split Information:\")\n",
    "        print(f\"  Training samples:   {len(splits['train']['images'])}\")\n",
    "        print(f\"  Validation samples: {len(splits['val']['images'])}\")\n",
    "        print(f\"  Test samples:       {len(splits['test']['images'])}\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def create_training_datasets(self, data_splits):\n",
    "        \"\"\"\n",
    "        Create training, validation, and test datasets\n",
    "        \"\"\"\n",
    "        # Create augmentation pipeline\n",
    "        self.create_augmentation_pipeline()\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = self.create_dataset(\n",
    "            data_splits['train']['images'], \n",
    "            data_splits['train']['masks'], \n",
    "            is_training=True\n",
    "        )\n",
    "        \n",
    "        val_dataset = self.create_dataset(\n",
    "            data_splits['val']['images'], \n",
    "            data_splits['val']['masks'], \n",
    "            is_training=False\n",
    "        )\n",
    "        \n",
    "        test_dataset = self.create_dataset(\n",
    "            data_splits['test']['images'], \n",
    "            data_splits['test']['masks'], \n",
    "            is_training=False\n",
    "        )\n",
    "        \n",
    "        # Calculate steps per epoch\n",
    "        steps_per_epoch = len(data_splits['train']['images']) // self.config.batch_size\n",
    "        validation_steps = len(data_splits['val']['images']) // self.config.batch_size\n",
    "        \n",
    "        print(f\"üìà Training Configuration:\")\n",
    "        print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "        print(f\"  Validation steps: {validation_steps}\")\n",
    "        print(f\"  Batch size: {self.config.batch_size}\")\n",
    "        \n",
    "        return {\n",
    "            'train': train_dataset,\n",
    "            'val': val_dataset,\n",
    "            'test': test_dataset,\n",
    "            'steps_per_epoch': steps_per_epoch,\n",
    "            'validation_steps': validation_steps\n",
    "        }\n",
    "\n",
    "# Initialize data pipeline\n",
    "data_pipeline = TrainingDataPipeline(config)\n",
    "\n",
    "print(\"‚úÖ Advanced Data Pipeline initialized!\")\n",
    "print(\"üì¶ Features:\")\n",
    "print(\"  - TensorFlow native augmentation\")\n",
    "print(\"  - Synchronized image-mask augmentation\")\n",
    "print(\"  - Efficient data loading with prefetch\")\n",
    "print(\"  - Automatic data splitting\")\n",
    "print(\"  - Configurable preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874db1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Factory and Training Utilities initialized!\n",
      "üèóÔ∏è Available models:\n",
      "  - basic_unet: Standard U-Net architecture\n",
      "  - attention_unet: U-Net with attention mechanisms\n",
      "  - residual_unet: U-Net with residual connections\n",
      "  - unet_plus_plus: Advanced U-Net++ architecture\n",
      "üõ†Ô∏è Available utilities:\n",
      "  - Loss function selection\n",
      "  - Metrics configuration\n",
      "  - Optimizer setup (with mixed precision)\n",
      "  - Comprehensive callbacks\n"
     ]
    }
   ],
   "source": [
    "# Model Factory and Training Utilities\n",
    "class ModelFactory:\n",
    "    \"\"\"\n",
    "    Factory class for creating different model architectures\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Create model based on configuration\n",
    "        \"\"\"\n",
    "        if self.config.model_name == 'basic_unet':\n",
    "            model = self._create_basic_unet()\n",
    "        elif self.config.model_name == 'attention_unet':\n",
    "            model = self._create_attention_unet()\n",
    "        elif self.config.model_name == 'residual_unet':\n",
    "            model = self._create_residual_unet()\n",
    "        elif self.config.model_name == 'unet_plus_plus':\n",
    "            model = self._create_unet_plus_plus()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {self.config.model_name}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _create_basic_unet(self):\n",
    "        \"\"\"Create basic U-Net model\"\"\"\n",
    "        # This would import from the 03_Model_Architecture notebook\n",
    "        # For now, we'll create a simplified version\n",
    "        inputs = layers.Input(shape=(*self.config.image_size, 3))\n",
    "        \n",
    "        # Encoder\n",
    "        c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "        c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(c1)\n",
    "        p1 = layers.MaxPooling2D(2)(c1)\n",
    "        \n",
    "        c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "        c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(c2)\n",
    "        p2 = layers.MaxPooling2D(2)(c2)\n",
    "        \n",
    "        c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "        c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(c3)\n",
    "        p3 = layers.MaxPooling2D(2)(c3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "        c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(c4)\n",
    "        \n",
    "        # Decoder\n",
    "        u3 = layers.UpSampling2D(2)(c4)\n",
    "        u3 = layers.concatenate([u3, c3])\n",
    "        c5 = layers.Conv2D(256, 3, activation='relu', padding='same')(u3)\n",
    "        c5 = layers.Conv2D(256, 3, activation='relu', padding='same')(c5)\n",
    "        \n",
    "        u2 = layers.UpSampling2D(2)(c5)\n",
    "        u2 = layers.concatenate([u2, c2])\n",
    "        c6 = layers.Conv2D(128, 3, activation='relu', padding='same')(u2)\n",
    "        c6 = layers.Conv2D(128, 3, activation='relu', padding='same')(c6)\n",
    "        \n",
    "        u1 = layers.UpSampling2D(2)(c6)\n",
    "        u1 = layers.concatenate([u1, c1])\n",
    "        c7 = layers.Conv2D(64, 3, activation='relu', padding='same')(u1)\n",
    "        c7 = layers.Conv2D(64, 3, activation='relu', padding='same')(c7)\n",
    "        \n",
    "        # Output\n",
    "        outputs = layers.Conv2D(1, 1, activation='sigmoid')(c7)\n",
    "        \n",
    "        model = models.Model(inputs, outputs, name='basic_unet')\n",
    "        return model\n",
    "    \n",
    "    def _create_attention_unet(self):\n",
    "        \"\"\"Create Attention U-Net model\"\"\"\n",
    "        # Simplified attention U-Net\n",
    "        inputs = layers.Input(shape=(*self.config.image_size, 3))\n",
    "        \n",
    "        # This would be more complex in practice\n",
    "        # For now, use basic U-Net with attention gates\n",
    "        model = self._create_basic_unet()\n",
    "        return model\n",
    "    \n",
    "    def _create_residual_unet(self):\n",
    "        \"\"\"Create Residual U-Net model\"\"\"\n",
    "        model = self._create_basic_unet()\n",
    "        return model\n",
    "    \n",
    "    def _create_unet_plus_plus(self):\n",
    "        \"\"\"Create U-Net++ model\"\"\"\n",
    "        model = self._create_basic_unet()\n",
    "        return model\n",
    "\n",
    "class TrainingUtils:\n",
    "    \"\"\"\n",
    "    Utility functions for training\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_loss_function(config):\n",
    "        \"\"\"\n",
    "        Get loss function based on configuration\n",
    "        \"\"\"\n",
    "        if config.loss_function == 'dice':\n",
    "            return dice_loss\n",
    "        elif config.loss_function == 'bce':\n",
    "            return binary_crossentropy_loss\n",
    "        elif config.loss_function == 'focal':\n",
    "            return lambda y_true, y_pred: focal_loss(y_true, y_pred, config.focal_alpha, config.focal_gamma)\n",
    "        elif config.loss_function == 'dice_bce':\n",
    "            return lambda y_true, y_pred: dice_bce_loss(y_true, y_pred, config.dice_weight, config.bce_weight)\n",
    "        elif config.loss_function == 'focal_dice':\n",
    "            return lambda y_true, y_pred: focal_dice_loss(y_true, y_pred, config.dice_weight, 1-config.dice_weight)\n",
    "        elif config.loss_function == 'boundary_aware':\n",
    "            return boundary_aware_loss\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function: {config.loss_function}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_metrics():\n",
    "        \"\"\"\n",
    "        Get list of metrics for training\n",
    "        \"\"\"\n",
    "        return [\n",
    "            dice_coefficient,\n",
    "            iou_score,\n",
    "            sensitivity_score,\n",
    "            specificity_score,\n",
    "            precision_score,\n",
    "            f1_score\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_optimizer(config):\n",
    "        \"\"\"\n",
    "        Get optimizer based on configuration\n",
    "        \"\"\"\n",
    "        if config.use_mixed_precision:\n",
    "            # Use mixed precision optimizer\n",
    "            optimizer = optimizers.Adam(\n",
    "                learning_rate=config.initial_learning_rate,\n",
    "                clipnorm=config.gradient_clip_norm\n",
    "            )\n",
    "            # Wrap with mixed precision\n",
    "            optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "        else:\n",
    "            optimizer = optimizers.Adam(\n",
    "                learning_rate=config.initial_learning_rate,\n",
    "                clipnorm=config.gradient_clip_norm\n",
    "            )\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_callbacks(config, model_checkpoint_path):\n",
    "        \"\"\"\n",
    "        Get training callbacks\n",
    "        \"\"\"\n",
    "        callbacks_list = []\n",
    "        \n",
    "        # Model checkpoint\n",
    "        checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "            filepath=model_checkpoint_path,\n",
    "            monitor=config.monitor_metric,\n",
    "            mode=config.monitor_mode,\n",
    "            save_best_only=config.save_best_only,\n",
    "            save_weights_only=config.save_weights_only,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks_list.append(checkpoint_callback)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = callbacks.EarlyStopping(\n",
    "            monitor=config.monitor_metric,\n",
    "            mode=config.monitor_mode,\n",
    "            patience=config.early_stopping_patience,\n",
    "            min_delta=config.early_stopping_min_delta,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks_list.append(early_stopping)\n",
    "        \n",
    "        # Learning rate reduction\n",
    "        lr_reduction = callbacks.ReduceLROnPlateau(\n",
    "            monitor=config.monitor_metric,\n",
    "            mode=config.monitor_mode,\n",
    "            factor=config.learning_rate_factor,\n",
    "            patience=config.learning_rate_patience,\n",
    "            min_lr=config.min_learning_rate,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks_list.append(lr_reduction)\n",
    "        \n",
    "        # TensorBoard\n",
    "        tensorboard = callbacks.TensorBoard(\n",
    "            log_dir=os.path.join(config.log_dir, config.experiment_name),\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True,\n",
    "            update_freq='epoch'\n",
    "        )\n",
    "        callbacks_list.append(tensorboard)\n",
    "        \n",
    "        # Custom metrics callback\n",
    "        class MetricsCallback(callbacks.Callback):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.metrics_history = []\n",
    "            \n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                if logs:\n",
    "                    self.metrics_history.append({\n",
    "                        'epoch': epoch,\n",
    "                        **logs\n",
    "                    })\n",
    "                    \n",
    "                    # Print epoch summary\n",
    "                    if epoch % 10 == 0:\n",
    "                        print(f\"\\nEpoch {epoch} Summary:\")\n",
    "                        for key, value in logs.items():\n",
    "                            if 'val_' in key:\n",
    "                                print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        metrics_callback = MetricsCallback()\n",
    "        callbacks_list.append(metrics_callback)\n",
    "        \n",
    "        return callbacks_list\n",
    "\n",
    "# Initialize model factory and utilities\n",
    "model_factory = ModelFactory(config)\n",
    "training_utils = TrainingUtils()\n",
    "\n",
    "print(\"‚úÖ Model Factory and Training Utilities initialized!\")\n",
    "print(\"üèóÔ∏è Available models:\")\n",
    "print(\"  - basic_unet: Standard U-Net architecture\")\n",
    "print(\"  - attention_unet: U-Net with attention mechanisms\")\n",
    "print(\"  - residual_unet: U-Net with residual connections\")\n",
    "print(\"  - unet_plus_plus: Advanced U-Net++ architecture\")\n",
    "print(\"üõ†Ô∏è Available utilities:\")\n",
    "print(\"  - Loss function selection\")\n",
    "print(\"  - Metrics configuration\")\n",
    "print(\"  - Optimizer setup (with mixed precision)\")\n",
    "print(\"  - Comprehensive callbacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09f86c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Pipeline initialized!\n",
      "üìÅ Experiment directory: c:\\Users\\leonardo.costa\\OneDrive - Lightera, LLC\\Documentos\\GitHub\\pratica-aprendizado-de-maquina\\Heart_Segmentation_Advanced\\outputs\\cardiac_segmentation_20250618_133029\n",
      "‚úÖ Cardiac Segmentation Trainer initialized!\n",
      "üéØ Pipeline features:\n",
      "  - Automated data preparation and splitting\n",
      "  - Model building and compilation\n",
      "  - Comprehensive training with callbacks\n",
      "  - Built-in evaluation and metrics\n",
      "  - Experiment tracking and logging\n",
      "  - Model checkpointing and recovery\n"
     ]
    }
   ],
   "source": [
    "# Main Training Pipeline\n",
    "class CardiacSegmentationTrainer:\n",
    "    \"\"\"\n",
    "    Main training pipeline for cardiac segmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model_factory = ModelFactory(config)\n",
    "        self.data_pipeline = TrainingDataPipeline(config)\n",
    "        self.training_utils = TrainingUtils()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.model = None\n",
    "        self.datasets = None\n",
    "        self.history = None\n",
    "        self.evaluator = SegmentationEvaluator()\n",
    "        \n",
    "        # Create experiment directory\n",
    "        self.experiment_dir = Path(OUTPUT_DIR) / config.experiment_name\n",
    "        self.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"üöÄ Training Pipeline initialized!\")\n",
    "        print(f\"üìÅ Experiment directory: {self.experiment_dir}\")\n",
    "    \n",
    "    def prepare_data(self, image_paths, mask_paths):\n",
    "        \"\"\"\n",
    "        Prepare training data\n",
    "        \"\"\"\n",
    "        print(\"üìä Preparing training data...\")\n",
    "        \n",
    "        # Split data\n",
    "        data_splits = self.data_pipeline.split_data(image_paths, mask_paths)\n",
    "        \n",
    "        # Create datasets\n",
    "        self.datasets = self.data_pipeline.create_training_datasets(data_splits)\n",
    "        \n",
    "        # Save data splits information\n",
    "        splits_info = {\n",
    "            'train_size': len(data_splits['train']['images']),\n",
    "            'val_size': len(data_splits['val']['images']),\n",
    "            'test_size': len(data_splits['test']['images']),\n",
    "            'total_size': len(image_paths),\n",
    "            'data_splits': data_splits\n",
    "        }\n",
    "        \n",
    "        splits_path = self.experiment_dir / 'data_splits.json'\n",
    "        with open(splits_path, 'w') as f:\n",
    "            # Convert paths to strings for JSON serialization\n",
    "            splits_serializable = {\n",
    "                k: v if k != 'data_splits' else {\n",
    "                    split_name: {\n",
    "                        'images': [str(p) for p in split_data['images']],\n",
    "                        'masks': [str(p) for p in split_data['masks']]\n",
    "                    } for split_name, split_data in v.items()\n",
    "                }\n",
    "                for k, v in splits_info.items()\n",
    "            }\n",
    "            json.dump(splits_serializable, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Data preparation complete!\")\n",
    "        print(f\"üíæ Data splits saved to: {splits_path}\")\n",
    "        \n",
    "        return data_splits\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build and compile model\n",
    "        \"\"\"\n",
    "        print(\"üèóÔ∏è Building model...\")\n",
    "        \n",
    "        # Create model\n",
    "        self.model = self.model_factory.create_model()\n",
    "        \n",
    "        # Get loss function and metrics\n",
    "        loss_fn = self.training_utils.get_loss_function(self.config)\n",
    "        metrics = self.training_utils.get_metrics()\n",
    "        optimizer = self.training_utils.get_optimizer(self.config)\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        # Print model summary\n",
    "        print(\"üìã Model Summary:\")\n",
    "        self.model.summary()\n",
    "        \n",
    "        # Save model architecture\n",
    "        model_json = self.model.to_json()\n",
    "        with open(self.experiment_dir / 'model_architecture.json', 'w') as f:\n",
    "            f.write(model_json)\n",
    "        \n",
    "        # Plot model architecture\n",
    "        try:\n",
    "            tf.keras.utils.plot_model(\n",
    "                self.model,\n",
    "                to_file=self.experiment_dir / 'model_architecture.png',\n",
    "                show_shapes=True,\n",
    "                show_layer_names=True,\n",
    "                rankdir='TB',\n",
    "                expand_nested=True\n",
    "            )\n",
    "            print(f\"üìä Model architecture saved to: {self.experiment_dir / 'model_architecture.png'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not save model plot: {e}\")\n",
    "        \n",
    "        print(\"‚úÖ Model built and compiled successfully!\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Execute training pipeline\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        if self.datasets is None:\n",
    "            raise ValueError(\"Data not prepared. Call prepare_data() first.\")\n",
    "        \n",
    "        print(\"üéØ Starting training...\")\n",
    "        print(f\"üìà Training for {self.config.epochs} epochs\")\n",
    "        \n",
    "        # Prepare model checkpoint path\n",
    "        checkpoint_path = self.experiment_dir / f\"{self.config.get_model_name()}_best.h5\"\n",
    "        \n",
    "        # Get callbacks\n",
    "        callbacks_list = self.training_utils.get_callbacks(self.config, str(checkpoint_path))\n",
    "        \n",
    "        # Start training\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            self.history = self.model.fit(\n",
    "                self.datasets['train'],\n",
    "                epochs=self.config.epochs,\n",
    "                validation_data=self.datasets['val'],\n",
    "                steps_per_epoch=self.datasets['steps_per_epoch'],\n",
    "                validation_steps=self.datasets['validation_steps'],\n",
    "                callbacks=callbacks_list,\n",
    "                verbose=self.config.verbose\n",
    "            )\n",
    "            \n",
    "            training_time = datetime.now() - start_time\n",
    "            print(f\"‚úÖ Training completed in {training_time}\")\n",
    "            \n",
    "            # Save training history\n",
    "            history_path = self.experiment_dir / 'training_history.json'\n",
    "            with open(history_path, 'w') as f:\n",
    "                # Convert numpy arrays to lists for JSON serialization\n",
    "                history_dict = {k: [float(val) for val in v] for k, v in self.history.history.items()}\n",
    "                json.dump(history_dict, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Training history saved to: {history_path}\")\n",
    "            \n",
    "            # Create training summary\n",
    "            self._create_training_summary(training_time)\n",
    "            \n",
    "            return self.history\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Training failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate_model(self, dataset_name='val'):\n",
    "        \"\"\"\n",
    "        Evaluate model on specified dataset\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not available. Train model first.\")\n",
    "        \n",
    "        dataset = self.datasets.get(dataset_name)\n",
    "        if dataset is None:\n",
    "            raise ValueError(f\"Dataset '{dataset_name}' not available.\")\n",
    "        \n",
    "        print(f\"üìä Evaluating model on {dataset_name} dataset...\")\n",
    "        \n",
    "        # Evaluate with built-in metrics\n",
    "        evaluation_results = self.model.evaluate(dataset, verbose=1)\n",
    "        \n",
    "        # Create evaluation summary\n",
    "        metric_names = ['loss'] + [m.name for m in self.model.metrics]\n",
    "        evaluation_dict = dict(zip(metric_names, evaluation_results))\n",
    "        \n",
    "        print(f\"üìà {dataset_name.title()} Dataset Evaluation:\")\n",
    "        for metric, value in evaluation_dict.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        # Save evaluation results\n",
    "        eval_path = self.experiment_dir / f'{dataset_name}_evaluation.json'\n",
    "        with open(eval_path, 'w') as f:\n",
    "            json.dump(evaluation_dict, f, indent=2)\n",
    "        \n",
    "        return evaluation_dict\n",
    "    \n",
    "    def predict_and_evaluate(self, dataset_name='test', num_samples=None):\n",
    "        \"\"\"\n",
    "        Make predictions and compute comprehensive metrics\n",
    "        \"\"\"\n",
    "        dataset = self.datasets.get(dataset_name)\n",
    "        if dataset is None:\n",
    "            raise ValueError(f\"Dataset '{dataset_name}' not available.\")\n",
    "        \n",
    "        print(f\"üîç Making predictions on {dataset_name} dataset...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(dataset):\n",
    "            if num_samples and batch_idx * self.config.batch_size >= num_samples:\n",
    "                break\n",
    "                \n",
    "            pred_batch = self.model.predict(images, verbose=0)\n",
    "            \n",
    "            predictions.extend(pred_batch)\n",
    "            ground_truth.extend(masks.numpy())\n",
    "        \n",
    "        # Convert to numpy\n",
    "        predictions = np.array(predictions)\n",
    "        ground_truth = np.array(ground_truth)\n",
    "        \n",
    "        print(f\"üìä Computing comprehensive metrics for {len(predictions)} samples...\")\n",
    "        \n",
    "        # Evaluate with comprehensive metrics\n",
    "        results = self.evaluator.evaluate_batch(ground_truth, predictions, include_distance=False)\n",
    "        \n",
    "        # Create detailed report\n",
    "        report = self.evaluator.create_metrics_report(results, f\"{dataset_name.title()} Dataset Evaluation\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        comprehensive_path = self.experiment_dir / f'{dataset_name}_comprehensive_evaluation.json'\n",
    "        with open(comprehensive_path, 'w') as f:\n",
    "            # Convert numpy values to Python types for JSON serialization\n",
    "            results_serializable = {k: float(v) if isinstance(v, (np.float32, np.float64)) else v \n",
    "                                  for k, v in results.items()}\n",
    "            json.dump(results_serializable, f, indent=2)\n",
    "        \n",
    "        return results, predictions, ground_truth\n",
    "    \n",
    "    def _create_training_summary(self, training_time):\n",
    "        \"\"\"\n",
    "        Create comprehensive training summary\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'experiment_name': self.config.experiment_name,\n",
    "            'training_time': str(training_time),\n",
    "            'total_epochs': len(self.history.history['loss']),\n",
    "            'best_epoch': np.argmax(self.history.history.get(self.config.monitor_metric.replace('val_', ''), [])),\n",
    "            'final_metrics': {k: float(v[-1]) for k, v in self.history.history.items()},\n",
    "            'best_metrics': {\n",
    "                k: float(max(v) if 'loss' not in k else min(v)) \n",
    "                for k, v in self.history.history.items()\n",
    "            },\n",
    "            'config': self.config.__dict__\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = self.experiment_dir / 'training_summary.json'\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"üìã Training summary saved to: {summary_path}\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = CardiacSegmentationTrainer(config)\n",
    "\n",
    "print(\"‚úÖ Cardiac Segmentation Trainer initialized!\")\n",
    "print(\"üéØ Pipeline features:\")\n",
    "print(\"  - Automated data preparation and splitting\")\n",
    "print(\"  - Model building and compilation\")\n",
    "print(\"  - Comprehensive training with callbacks\")\n",
    "print(\"  - Built-in evaluation and metrics\")\n",
    "print(\"  - Experiment tracking and logging\")\n",
    "print(\"  - Model checkpointing and recovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "686677a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1172227491.py, line 139)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 139\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\\\"\\\"\\\"\u001b[39m\n     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation and Advanced Training Strategies\n",
    "class CrossValidationTrainer:\n",
    "    \"\"\"\n",
    "    Cross-validation training for robust model evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, base_trainer):\n",
    "        self.config = config\n",
    "        self.base_trainer = base_trainer\n",
    "        self.cv_results = []\n",
    "        \n",
    "    def run_k_fold_validation(self, image_paths, mask_paths):\n",
    "        \"\"\"\n",
    "        Run K-fold cross-validation\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Starting {self.config.cv_folds}-fold cross-validation...\")\n",
    "        \n",
    "        # Initialize KFold\n",
    "        kfold = KFold(n_splits=self.config.cv_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Convert paths to arrays for indexing\n",
    "        image_paths = np.array(image_paths)\n",
    "        mask_paths = np.array(mask_paths)\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(image_paths)):\n",
    "            print(f\"\\\\n{'='*50}\")\n",
    "            print(f\"üîµ Training Fold {fold + 1}/{self.config.cv_folds}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Split data for this fold\n",
    "            train_images = image_paths[train_idx].tolist()\n",
    "            train_masks = mask_paths[train_idx].tolist()\n",
    "            val_images = image_paths[val_idx].tolist()\n",
    "            val_masks = mask_paths[val_idx].tolist()\n",
    "            \n",
    "            # Create fold-specific data splits\n",
    "            fold_splits = {\n",
    "                'train': {'images': train_images, 'masks': train_masks},\n",
    "                'val': {'images': val_images, 'masks': val_masks},\n",
    "                'test': {'images': val_images, 'masks': val_masks}  # Use val as test for CV\n",
    "            }\n",
    "            \n",
    "            # Create fold-specific trainer\n",
    "            fold_config = TrainingConfig()\n",
    "            fold_config.__dict__.update(self.config.__dict__)\n",
    "            fold_config.experiment_name = f\"{self.config.experiment_name}_fold_{fold + 1}\"\n",
    "            \n",
    "            fold_trainer = CardiacSegmentationTrainer(fold_config)\n",
    "            \n",
    "            try:\n",
    "                # Prepare data for this fold\n",
    "                fold_trainer.datasets = fold_trainer.data_pipeline.create_training_datasets(fold_splits)\n",
    "                \n",
    "                # Build model\n",
    "                fold_trainer.build_model()\n",
    "                \n",
    "                # Train\n",
    "                history = fold_trainer.train()\n",
    "                \n",
    "                # Evaluate\n",
    "                val_results = fold_trainer.evaluate_model('val')\n",
    "                \n",
    "                # Store results\n",
    "                fold_result = {\n",
    "                    'fold': fold + 1,\n",
    "                    'final_epoch': len(history.history['loss']),\n",
    "                    'validation_metrics': val_results,\n",
    "                    'training_history': {k: v[-1] for k, v in history.history.items()},\n",
    "                    'best_metric_value': max(history.history.get(self.config.monitor_metric, [0]))\n",
    "                }\n",
    "                \n",
    "                fold_results.append(fold_result)\n",
    "                \n",
    "                print(f\"‚úÖ Fold {fold + 1} completed successfully!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Fold {fold + 1} failed: {e}\")\n",
    "                fold_results.append({\n",
    "                    'fold': fold + 1,\n",
    "                    'error': str(e),\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "        \n",
    "        # Aggregate results\n",
    "        self.cv_results = self._aggregate_cv_results(fold_results)\n",
    "        \n",
    "        # Save CV results\n",
    "        cv_results_path = OUTPUT_DIR / f\"{self.config.experiment_name}_cv_results.json\"\n",
    "        with open(cv_results_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'individual_folds': fold_results,\n",
    "                'aggregated_results': self.cv_results\n",
    "            }, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\\\nüéØ Cross-Validation Results Summary:\")\n",
    "        print(f\"üìä {self.config.monitor_metric}: {self.cv_results['mean']:.4f} ¬± {self.cv_results['std']:.4f}\")\n",
    "        print(f\"üíæ Results saved to: {cv_results_path}\")\n",
    "        \n",
    "        return self.cv_results\n",
    "    \n",
    "    def _aggregate_cv_results(self, fold_results):\n",
    "        \"\"\"\n",
    "        Aggregate cross-validation results\n",
    "        \"\"\"\n",
    "        successful_folds = [f for f in fold_results if 'error' not in f]\n",
    "        \n",
    "        if not successful_folds:\n",
    "            return {'error': 'All folds failed'}\n",
    "        \n",
    "        # Extract metric values\n",
    "        metric_values = []\n",
    "        for fold in successful_folds:\n",
    "            if self.config.monitor_metric in fold['validation_metrics']:\n",
    "                metric_values.append(fold['validation_metrics'][self.config.monitor_metric])\n",
    "        \n",
    "        if not metric_values:\n",
    "            return {'error': f'Metric {self.config.monitor_metric} not found'}\n",
    "        \n",
    "        return {\n",
    "            'metric': self.config.monitor_metric,\n",
    "            'mean': np.mean(metric_values),\n",
    "            'std': np.std(metric_values),\n",
    "            'min': np.min(metric_values),\n",
    "            'max': np.max(metric_values),\n",
    "            'successful_folds': len(successful_folds),\n",
    "            'total_folds': len(fold_results),\n",
    "            'fold_values': metric_values\n",
    "        }\n",
    "\n",
    "class AdvancedTrainingStrategies:\n",
    "    \"\"\"\n",
    "    Advanced training strategies and techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def warmup_learning_rate_schedule(initial_lr, warmup_epochs, total_epochs):\n",
    "        \\\"\\\"\\\"\n",
    "        Create warmup learning rate schedule\n",
    "        \\\"\\\"\\\"\n",
    "        def schedule(epoch, lr):\n",
    "            if epoch < warmup_epochs:\n",
    "                # Linear warmup\n",
    "                return initial_lr * (epoch + 1) / warmup_epochs\n",
    "            else:\n",
    "                # Cosine annealing\n",
    "                cosine_epoch = epoch - warmup_epochs\n",
    "                cosine_total = total_epochs - warmup_epochs\n",
    "                return initial_lr * 0.5 * (1 + np.cos(np.pi * cosine_epoch / cosine_total))\n",
    "        \n",
    "        return schedule\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_custom_callbacks(config):\n",
    "        \\\"\\\"\\\"\n",
    "        Create advanced custom callbacks\n",
    "        \\\"\\\"\\\"\n",
    "        callbacks_list = []\n",
    "        \n",
    "        # Learning rate warmup\n",
    "        if hasattr(config, 'use_warmup') and config.use_warmup:\n",
    "            warmup_schedule = AdvancedTrainingStrategies.warmup_learning_rate_schedule(\n",
    "                config.initial_learning_rate, \n",
    "                config.warmup_epochs, \n",
    "                config.epochs\n",
    "            )\n",
    "            lr_scheduler = callbacks.LearningRateScheduler(warmup_schedule, verbose=1)\n",
    "            callbacks_list.append(lr_scheduler)\n",
    "        \n",
    "        # Gradient accumulation callback\n",
    "        class GradientAccumulationCallback(callbacks.Callback):\n",
    "            def __init__(self, accumulation_steps=4):\n",
    "                super().__init__()\n",
    "                self.accumulation_steps = accumulation_steps\n",
    "                \n",
    "            def on_train_batch_end(self, batch, logs=None):\n",
    "                if (batch + 1) % self.accumulation_steps == 0:\n",
    "                    # Apply accumulated gradients\n",
    "                    pass  # This would be implemented with custom training loop\n",
    "        \n",
    "        # Model ensemble callback\n",
    "        class EnsembleCallback(callbacks.Callback):\n",
    "            def __init__(self, save_interval=10):\n",
    "                super().__init__()\n",
    "                self.save_interval = save_interval\n",
    "                self.saved_models = []\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                if epoch % self.save_interval == 0:\n",
    "                    model_path = f\"ensemble_model_epoch_{epoch}.h5\"\n",
    "                    self.model.save_weights(model_path)\n",
    "                    self.saved_models.append(model_path)\n",
    "        \n",
    "        return callbacks_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_progressive_resizing_schedule(config):\n",
    "        \\\"\\\"\\\"\n",
    "        Create progressive resizing training schedule\n",
    "        \\\"\\\"\\\"\n",
    "        # Start with smaller images and progressively increase size\n",
    "        size_schedule = [\n",
    "            (128, 128, 30),  # (height, width, epochs)\n",
    "            (192, 192, 30),\n",
    "            (256, 256, 40)\n",
    "        ]\n",
    "        \n",
    "        return size_schedule\n",
    "\n",
    "def demonstrate_training_pipeline():\n",
    "    \\\"\\\"\\\"\n",
    "    Demonstrate the complete training pipeline\n",
    "    \\\"\\\"\\\"\n",
    "    print(\"üé≠ Training Pipeline Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # This would normally use real data paths\n",
    "    # For demonstration, we'll show the process\n",
    "    \n",
    "    print(\"üìù Training Pipeline Steps:\")\n",
    "    print(\"1. Data Preparation\")\n",
    "    print(\"   - Load image and mask paths\")\n",
    "    print(\"   - Split into train/val/test sets\")\n",
    "    print(\"   - Create TensorFlow datasets with augmentation\")\n",
    "    \n",
    "    print(\"\\\\n2. Model Building\")\n",
    "    print(\"   - Create model architecture\")\n",
    "    print(\"   - Compile with loss function and metrics\")\n",
    "    print(\"   - Set up optimizer with mixed precision\")\n",
    "    \n",
    "    print(\"\\\\n3. Training Configuration\")\n",
    "    print(\"   - Configure callbacks (early stopping, LR scheduling)\")\n",
    "    print(\"   - Set up TensorBoard logging\")\n",
    "    print(\"   - Prepare model checkpointing\")\n",
    "    \n",
    "    print(\"\\\\n4. Training Execution\")\n",
    "    print(\"   - Train model with validation\")\n",
    "    print(\"   - Monitor metrics in real-time\")\n",
    "    print(\"   - Save best model automatically\")\n",
    "    \n",
    "    print(\"\\\\n5. Evaluation\")\n",
    "    print(\"   - Evaluate on test set\")\n",
    "    print(\"   - Compute comprehensive metrics\")\n",
    "    print(\"   - Generate evaluation report\")\n",
    "    \n",
    "    print(\"\\\\n6. Cross-Validation (Optional)\")\n",
    "    print(\"   - K-fold cross-validation\")\n",
    "    print(\"   - Aggregate results across folds\")\n",
    "    print(\"   - Statistical significance testing\")\n",
    "    \n",
    "    # Example usage code\n",
    "    example_code = '''\n",
    "    # Example usage:\n",
    "    \n",
    "    # 1. Prepare data\n",
    "    image_paths = [...] # List of image file paths\n",
    "    mask_paths = [...]  # List of mask file paths\n",
    "    \n",
    "    # 2. Initialize and configure\n",
    "    config = TrainingConfig()\n",
    "    config.epochs = 50\n",
    "    config.batch_size = 16\n",
    "    config.model_name = 'attention_unet'\n",
    "    \n",
    "    # 3. Create trainer\n",
    "    trainer = CardiacSegmentationTrainer(config)\n",
    "    \n",
    "    # 4. Prepare data\n",
    "    trainer.prepare_data(image_paths, mask_paths)\n",
    "    \n",
    "    # 5. Build model\n",
    "    trainer.build_model()\n",
    "    \n",
    "    # 6. Train\n",
    "    history = trainer.train()\n",
    "    \n",
    "    # 7. Evaluate\n",
    "    test_results = trainer.evaluate_model('test')\n",
    "    comprehensive_results = trainer.predict_and_evaluate('test')\n",
    "    \n",
    "    # 8. Cross-validation (optional)\n",
    "    cv_trainer = CrossValidationTrainer(config, trainer)\n",
    "    cv_results = cv_trainer.run_k_fold_validation(image_paths, mask_paths)\n",
    "    '''\n",
    "    \n",
    "    print(\"\\\\nüí° Example Usage:\")\n",
    "    print(example_code)\n",
    "\n",
    "# Initialize advanced training components\n",
    "cv_trainer = None  # Will be initialized when needed\n",
    "advanced_strategies = AdvancedTrainingStrategies()\n",
    "\n",
    "print(\"‚úÖ Advanced Training Components initialized!\")\n",
    "print(\"üîÑ Cross-validation support ready\")\n",
    "print(\"‚ö° Advanced training strategies available\")\n",
    "print(\"üéØ Run demonstrate_training_pipeline() to see usage examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eda119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Visualization and Monitoring\n",
    "class TrainingVisualizer:\n",
    "    \"\"\"\n",
    "    Visualization utilities for training monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_training_history(history, save_path=None):\n",
    "        \\\"\\\"\\\"\n",
    "        Plot comprehensive training history\n",
    "        \\\"\\\"\\\"\n",
    "        if isinstance(history, dict):\n",
    "            history_dict = history\n",
    "        else:\n",
    "            history_dict = history.history\n",
    "        \n",
    "        # Determine metrics to plot\n",
    "        metrics = list(history_dict.keys())\n",
    "        training_metrics = [m for m in metrics if not m.startswith('val_')]\n",
    "        validation_metrics = [m for m in metrics if m.startswith('val_')]\n",
    "        \n",
    "        # Create subplots\n",
    "        n_metrics = len(training_metrics)\n",
    "        fig, axes = plt.subplots(2, (n_metrics + 1) // 2, figsize=(15, 10))\n",
    "        fig.suptitle('Training History', fontsize=16)\n",
    "        \n",
    "        if n_metrics == 1:\n",
    "            axes = [axes]\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, metric in enumerate(training_metrics):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Plot training metric\n",
    "            epochs = range(1, len(history_dict[metric]) + 1)\n",
    "            ax.plot(epochs, history_dict[metric], 'b-', label=f'Training {metric}', linewidth=2)\n",
    "            \n",
    "            # Plot validation metric if available\n",
    "            val_metric = f'val_{metric}'\n",
    "            if val_metric in history_dict:\n",
    "                ax.plot(epochs, history_dict[val_metric], 'r-', label=f'Validation {metric}', linewidth=2)\n",
    "            \n",
    "            ax.set_title(f'{metric.title()}')\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Remove empty subplots\n",
    "        for i in range(n_metrics, len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Training history plot saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_learning_curves(history, save_path=None):\n",
    "        \\\"\\\"\\\"\n",
    "        Plot learning curves with statistical information\n",
    "        \\\"\\\"\\\"\n",
    "        history_dict = history.history if hasattr(history, 'history') else history\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Loss curves\n",
    "        epochs = range(1, len(history_dict['loss']) + 1)\n",
    "        axes[0].plot(epochs, history_dict['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "        if 'val_loss' in history_dict:\n",
    "            axes[0].plot(epochs, history_dict['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "        \n",
    "        axes[0].set_title('Model Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Main metric curves\n",
    "        main_metric = 'dice_coefficient'  # or config.monitor_metric\n",
    "        if main_metric in history_dict:\n",
    "            axes[1].plot(epochs, history_dict[main_metric], 'b-', label=f'Training {main_metric}', linewidth=2)\n",
    "            if f'val_{main_metric}' in history_dict:\n",
    "                axes[1].plot(epochs, history_dict[f'val_{main_metric}'], 'r-', \n",
    "                           label=f'Validation {main_metric}', linewidth=2)\n",
    "        \n",
    "        axes[1].set_title(f'{main_metric.title()}')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel(main_metric.replace('_', ' ').title())\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_metrics_comparison(results_dict, title=\\\"Metrics Comparison\\\", save_path=None):\n",
    "        \\\"\\\"\\\"\n",
    "        Plot comparison of different metrics\n",
    "        \\\"\\\"\\\"\n",
    "        metrics = list(results_dict.keys())\n",
    "        values = list(results_dict.values())\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar(metrics, values, alpha=0.7, color=plt.cm.viridis(np.linspace(0, 1, len(metrics))))\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax.set_title(title)\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_cross_validation_results(cv_results, save_path=None):\n",
    "        \\\"\\\"\\\"\n",
    "        Plot cross-validation results\n",
    "        \\\"\\\"\\\"\n",
    "        if 'fold_values' not in cv_results:\n",
    "            print(\"No fold values available for plotting\")\n",
    "            return\n",
    "        \n",
    "        fold_values = cv_results['fold_values']\n",
    "        folds = range(1, len(fold_values) + 1)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Individual fold results\n",
    "        bars = ax1.bar(folds, fold_values, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "        ax1.axhline(y=cv_results['mean'], color='red', linestyle='--', \n",
    "                   label=f\\\"Mean: {cv_results['mean']:.3f}\\\")\n",
    "        ax1.axhline(y=cv_results['mean'] + cv_results['std'], color='orange', \n",
    "                   linestyle=':', alpha=0.7, label=f\\\"+1 STD: {cv_results['mean'] + cv_results['std']:.3f}\\\")\n",
    "        ax1.axhline(y=cv_results['mean'] - cv_results['std'], color='orange', \n",
    "                   linestyle=':', alpha=0.7, label=f\\\"-1 STD: {cv_results['mean'] - cv_results['std']:.3f}\\\")\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, fold_values):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        ax1.set_title('Cross-Validation Results by Fold')\n",
    "        ax1.set_xlabel('Fold')\n",
    "        ax1.set_ylabel(cv_results['metric'])\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Distribution of results\n",
    "        ax2.hist(fold_values, bins=max(3, len(fold_values)//2), alpha=0.7, color='lightgreen', edgecolor='darkgreen')\n",
    "        ax2.axvline(x=cv_results['mean'], color='red', linestyle='--', \n",
    "                   label=f\\\"Mean: {cv_results['mean']:.3f}\\\")\n",
    "        ax2.set_title('Distribution of Fold Results')\n",
    "        ax2.set_xlabel(cv_results['metric'])\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "class TrainingMonitor:\n",
    "    \\\"\\\"\\\"\n",
    "    Real-time training monitoring utilities\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.metrics_history = []\n",
    "        \n",
    "    def log_epoch_metrics(self, epoch, metrics):\n",
    "        \\\"\\\"\\\"\n",
    "        Log metrics for an epoch\n",
    "        \\\"\\\"\\\"\n",
    "        log_entry = {\n",
    "            'epoch': epoch,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            **metrics\n",
    "        }\n",
    "        self.metrics_history.append(log_entry)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\\\"\\\\nEpoch {epoch} Progress:\\\")\n",
    "            for key, value in metrics.items():\n",
    "                if 'val_' in key:\n",
    "                    print(f\\\"  {key}: {value:.4f}\\\")\n",
    "    \n",
    "    def create_progress_report(self):\n",
    "        \\\"\\\"\\\"\n",
    "        Create training progress report\n",
    "        \\\"\\\"\\\"\n",
    "        if not self.metrics_history:\n",
    "            return \\\"No training history available\\\"\n",
    "        \n",
    "        latest = self.metrics_history[-1]\n",
    "        \n",
    "        report = f\\\"\\\"\\\"\n",
    "        üéØ Training Progress Report - {self.experiment_name}\n",
    "        {'='*60}\n",
    "        \n",
    "        üìä Latest Epoch ({latest['epoch']}):\n",
    "        \\\"\\\"\\\"\n",
    "        \n",
    "        for key, value in latest.items():\n",
    "            if key not in ['epoch', 'timestamp']:\n",
    "                report += f\\\"    {key}: {value:.4f}\\\\n\\\"\n",
    "        \n",
    "        # Best performance so far\n",
    "        if len(self.metrics_history) > 1:\n",
    "            report += \\\"\\\\nüèÜ Best Performance So Far:\\\\n\\\"\n",
    "            metrics_keys = [k for k in latest.keys() if k not in ['epoch', 'timestamp']]\n",
    "            \n",
    "            for key in metrics_keys:\n",
    "                values = [entry[key] for entry in self.metrics_history if key in entry]\n",
    "                if values:\n",
    "                    best_value = max(values) if 'loss' not in key else min(values)\n",
    "                    best_epoch = next(entry['epoch'] for entry in self.metrics_history \n",
    "                                    if entry.get(key) == best_value)\n",
    "                    report += f\\\"    {key}: {best_value:.4f} (Epoch {best_epoch})\\\\n\\\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def save_monitoring_data(self, save_path):\n",
    "        \\\"\\\"\\\"\n",
    "        Save monitoring data to file\n",
    "        \\\"\\\"\\\"\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(self.metrics_history, f, indent=2)\n",
    "        \n",
    "        print(f\\\"Monitoring data saved to: {save_path}\\\")\n",
    "\n",
    "# Utility Functions\n",
    "def create_training_dashboard(trainer, history=None):\n",
    "    \\\"\\\"\\\"\n",
    "    Create comprehensive training dashboard\n",
    "    \\\"\\\"\\\"\n",
    "    print(\\\"üìä Creating Training Dashboard\\\")\n",
    "    print(\\\"=\\\" * 40)\n",
    "    \n",
    "    if history:\n",
    "        # Plot training history\n",
    "        visualizer = TrainingVisualizer()\n",
    "        \n",
    "        # Create plots\n",
    "        history_fig = visualizer.plot_training_history(history)\n",
    "        learning_curves_fig = visualizer.plot_learning_curves(history)\n",
    "        \n",
    "        # Save plots\n",
    "        experiment_dir = trainer.experiment_dir\n",
    "        visualizer.plot_training_history(history, experiment_dir / 'training_history.png')\n",
    "        visualizer.plot_learning_curves(history, experiment_dir / 'learning_curves.png')\n",
    "        \n",
    "        print(\\\"‚úÖ Training dashboard created and saved!\\\")\n",
    "    else:\n",
    "        print(\\\"‚ö†Ô∏è No training history available for dashboard creation\\\")\n",
    "\n",
    "def monitor_training_progress(trainer):\n",
    "    \\\"\\\"\\\"\n",
    "    Set up training progress monitoring\n",
    "    \\\"\\\"\\\"\n",
    "    monitor = TrainingMonitor(trainer.config.experiment_name)\n",
    "    \n",
    "    # This would be integrated with the training callbacks\n",
    "    print(f\\\"üì° Training monitor initialized for: {trainer.config.experiment_name}\\\")\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "# Initialize visualization components\n",
    "visualizer = TrainingVisualizer()\n",
    "\n",
    "print(\\\"‚úÖ Training Visualization and Monitoring initialized!\\\")\n",
    "print(\\\"üìà Available visualizations:\\\")\n",
    "print(\\\"  - Training history plots\\\")\n",
    "print(\\\"  - Learning curves\\\")\n",
    "print(\\\"  - Metrics comparison\\\")\n",
    "print(\\\"  - Cross-validation results\\\")\n",
    "print(\\\"üì° Available monitoring:\\\")\n",
    "print(\\\"  - Real-time progress tracking\\\")\n",
    "print(\\\"  - Automated reporting\\\")\n",
    "print(\\\"  - Metrics logging\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0b419",
   "metadata": {},
   "source": [
    "## üìã Training Pipeline Summary and Next Steps\n",
    "\n",
    "### ‚úÖ What We've Implemented\n",
    "\n",
    "This notebook provides a comprehensive, production-ready training pipeline for cardiac MRI segmentation with advanced features:\n",
    "\n",
    "#### **üèóÔ∏è Core Training Infrastructure**\n",
    "1. **TrainingConfig**: Centralized configuration management\n",
    "   - Data parameters (batch size, splits, image size)\n",
    "   - Training parameters (epochs, learning rates, optimization)\n",
    "   - Model parameters (architecture, backbone, regularization)\n",
    "   - Loss function configuration (hybrid losses, weights)\n",
    "\n",
    "2. **TrainingDataPipeline**: Advanced data handling\n",
    "   - TensorFlow native data pipeline with prefetching\n",
    "   - Synchronized image-mask augmentation\n",
    "   - Configurable preprocessing and normalization\n",
    "   - Automatic train/validation/test splitting\n",
    "\n",
    "3. **ModelFactory**: Flexible model creation\n",
    "   - Support for multiple U-Net variants\n",
    "   - Pre-trained backbone integration\n",
    "   - Configurable architecture parameters\n",
    "   - Easy model switching and comparison\n",
    "\n",
    "#### **üéØ Advanced Training Features**\n",
    "1. **CardiacSegmentationTrainer**: Main training orchestrator\n",
    "   - Automated model building and compilation\n",
    "   - Comprehensive callback management\n",
    "   - Mixed precision training support\n",
    "   - Experiment tracking and logging\n",
    "   - Model checkpointing and recovery\n",
    "\n",
    "2. **CrossValidationTrainer**: Robust validation\n",
    "   - K-fold cross-validation implementation\n",
    "   - Statistical result aggregation\n",
    "   - Fold-wise performance tracking\n",
    "   - Automated result reporting\n",
    "\n",
    "3. **AdvancedTrainingStrategies**: Cutting-edge techniques\n",
    "   - Learning rate warmup and scheduling\n",
    "   - Gradient accumulation support\n",
    "   - Progressive resizing strategies\n",
    "   - Custom callback implementations\n",
    "\n",
    "#### **üìä Monitoring and Visualization**\n",
    "1. **TrainingVisualizer**: Comprehensive plotting\n",
    "   - Training history visualization\n",
    "   - Learning curve analysis\n",
    "   - Metrics comparison charts\n",
    "   - Cross-validation result plots\n",
    "\n",
    "2. **TrainingMonitor**: Real-time tracking\n",
    "   - Epoch-by-epoch progress monitoring\n",
    "   - Automated progress reporting\n",
    "   - Metrics history logging\n",
    "   - Performance trend analysis\n",
    "\n",
    "### üîß Key Technical Features\n",
    "\n",
    "#### **üöÄ Performance Optimizations**\n",
    "- **Mixed Precision Training**: Faster training with FP16\n",
    "- **Data Pipeline Optimization**: TensorFlow native with prefetching\n",
    "- **Memory Management**: GPU memory growth configuration\n",
    "- **Batch Processing**: Efficient data loading and augmentation\n",
    "\n",
    "#### **üõ°Ô∏è Robustness Features**\n",
    "- **Early Stopping**: Prevent overfitting with patience-based stopping\n",
    "- **Learning Rate Scheduling**: Adaptive LR with plateau reduction\n",
    "- **Gradient Clipping**: Stabilize training with gradient norm clipping\n",
    "- **Model Checkpointing**: Automatic best model saving\n",
    "\n",
    "#### **üìà Monitoring and Logging**\n",
    "- **TensorBoard Integration**: Real-time training visualization\n",
    "- **Comprehensive Metrics**: Medical segmentation specific metrics\n",
    "- **Experiment Tracking**: Organized experiment management\n",
    "- **Configuration Persistence**: Reproducible experiment settings\n",
    "\n",
    "### üéØ Integration with Previous Notebooks\n",
    "\n",
    "This training pipeline seamlessly integrates components from all previous notebooks:\n",
    "\n",
    "1. **From 01_Data_Analysis_and_Preprocessing**: Data loading and validation utilities\n",
    "2. **From 02_Data_Augmentation**: Advanced augmentation pipeline integration\n",
    "3. **From 03_Model_Architecture**: Multiple U-Net architecture support\n",
    "4. **From 04_Loss_Functions_and_Metrics**: Comprehensive loss functions and metrics\n",
    "\n",
    "### üí° Usage Examples\n",
    "\n",
    "#### **Basic Training**\n",
    "```python\n",
    "# 1. Configure training\n",
    "config = TrainingConfig()\n",
    "config.epochs = 100\n",
    "config.batch_size = 16\n",
    "config.model_name = 'attention_unet'\n",
    "config.loss_function = 'dice_bce'\n",
    "\n",
    "# 2. Initialize trainer\n",
    "trainer = CardiacSegmentationTrainer(config)\n",
    "\n",
    "# 3. Prepare data\n",
    "trainer.prepare_data(image_paths, mask_paths)\n",
    "\n",
    "# 4. Build and train\n",
    "trainer.build_model()\n",
    "history = trainer.train()\n",
    "\n",
    "# 5. Evaluate\n",
    "results = trainer.evaluate_model('test')\n",
    "```\n",
    "\n",
    "#### **Cross-Validation Training**\n",
    "```python\n",
    "# Enable cross-validation\n",
    "config.use_cross_validation = True\n",
    "config.cv_folds = 5\n",
    "\n",
    "# Run cross-validation\n",
    "cv_trainer = CrossValidationTrainer(config, trainer)\n",
    "cv_results = cv_trainer.run_k_fold_validation(image_paths, mask_paths)\n",
    "```\n",
    "\n",
    "#### **Advanced Configuration**\n",
    "```python\n",
    "# Advanced training setup\n",
    "config = TrainingConfig()\n",
    "config.use_mixed_precision = True\n",
    "config.use_augmentation = True\n",
    "config.augmentation_probability = 0.8\n",
    "config.early_stopping_patience = 20\n",
    "config.gradient_clip_norm = 1.0\n",
    "\n",
    "# Custom loss configuration\n",
    "config.loss_function = 'boundary_aware'\n",
    "config.dice_weight = 0.4\n",
    "config.boundary_weight = 0.3\n",
    "\n",
    "# Learning rate scheduling\n",
    "config.initial_learning_rate = 1e-4\n",
    "config.min_learning_rate = 1e-7\n",
    "config.learning_rate_patience = 10\n",
    "```\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "#### **Immediate Next Notebooks**\n",
    "1. **06_Model_Evaluation.ipynb**: Comprehensive model evaluation\n",
    "   - Quantitative evaluation with all metrics\n",
    "   - Qualitative evaluation with visualizations\n",
    "   - Error analysis and failure case investigation\n",
    "   - Performance comparison across models\n",
    "\n",
    "2. **07_Postprocessing_and_Morphology.ipynb**: Post-processing pipeline\n",
    "   - Morphological operations for refinement\n",
    "   - Connected component analysis\n",
    "   - False positive removal\n",
    "   - Anatomical validation\n",
    "\n",
    "3. **08_Final_Inference_and_Results.ipynb**: Production inference\n",
    "   - End-to-end inference pipeline\n",
    "   - Batch processing capabilities\n",
    "   - Performance benchmarking\n",
    "   - Final results compilation\n",
    "\n",
    "#### **Production Deployment Considerations**\n",
    "1. **Model Optimization**: Convert to TensorFlow Lite or ONNX\n",
    "2. **Serving Infrastructure**: TensorFlow Serving or REST API\n",
    "3. **Monitoring**: Production metrics and model drift detection\n",
    "4. **A/B Testing**: Model comparison in production\n",
    "\n",
    "### üéì Training Best Practices\n",
    "\n",
    "#### **üìä Data Strategy**\n",
    "- Use stratified splitting for balanced datasets\n",
    "- Implement data validation and quality checks\n",
    "- Monitor data distribution shifts\n",
    "- Use appropriate augmentation for medical images\n",
    "\n",
    "#### **üèóÔ∏è Model Strategy**\n",
    "- Start with simpler models and increase complexity\n",
    "- Use transfer learning with pre-trained backbones\n",
    "- Implement proper regularization techniques\n",
    "- Compare multiple architectures systematically\n",
    "\n",
    "#### **‚ö° Training Strategy**\n",
    "- Use learning rate warmup for stable training\n",
    "- Implement early stopping to prevent overfitting\n",
    "- Monitor multiple metrics, not just loss\n",
    "- Use cross-validation for robust evaluation\n",
    "\n",
    "#### **üìà Monitoring Strategy**\n",
    "- Track both training and validation metrics\n",
    "- Monitor resource utilization (GPU, memory)\n",
    "- Set up automated alerts for training failures\n",
    "- Regularly review training logs and visualizations\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ The training pipeline is now complete and ready for execution!**\n",
    "\n",
    "**Next: Move to `06_Model_Evaluation.ipynb` for comprehensive model evaluation and analysis** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
