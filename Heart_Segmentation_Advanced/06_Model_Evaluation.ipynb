{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fadf4c76",
   "metadata": {},
   "source": [
    "# üìä Comprehensive Model Evaluation for Cardiac Segmentation\n",
    "\n",
    "This notebook provides a comprehensive evaluation framework for cardiac MRI segmentation models. We'll evaluate trained models using quantitative metrics, qualitative visualizations, error analysis, and performance comparisons across different architectures and configurations.\n",
    "\n",
    "## Objectives\n",
    "- Load and evaluate trained models from previous notebooks\n",
    "- Compute comprehensive quantitative metrics (medical-specific)\n",
    "- Create qualitative visualizations of segmentation results\n",
    "- Perform detailed error analysis and failure case investigation\n",
    "- Compare performance across different model architectures\n",
    "- Generate statistical analysis and confidence intervals\n",
    "- Create publication-ready evaluation reports\n",
    "\n",
    "## Key Components\n",
    "1. **Model Loading**: Load trained models and configurations\n",
    "2. **Quantitative Evaluation**: Medical segmentation metrics computation\n",
    "3. **Qualitative Analysis**: Visual comparison of predictions vs ground truth\n",
    "4. **Error Analysis**: Identification and analysis of failure cases\n",
    "5. **Statistical Analysis**: Confidence intervals and significance testing\n",
    "6. **Performance Comparison**: Multi-model architecture comparison\n",
    "7. **Report Generation**: Comprehensive evaluation reports\n",
    "\n",
    "## Evaluation Strategy\n",
    "- **Multi-metric Assessment**: Dice, IoU, Hausdorff Distance, Surface metrics\n",
    "- **Per-class Analysis**: Individual evaluation for each cardiac structure\n",
    "- **Statistical Robustness**: Bootstrap confidence intervals and significance tests\n",
    "- **Visual Validation**: Qualitative assessment with expert-level visualizations\n",
    "- **Failure Case Analysis**: Systematic investigation of poor predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d3e8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Running in VS Code/Local Environment\n",
      "‚úÖ All required packages are available\n",
      "‚ö†Ô∏è Could not import custom modules: cannot import name 'VisualizationUtils' from 'visualization_utils' (c:\\Users\\leonardo.costa\\OneDrive - Lightera, LLC\\Documentos\\GitHub\\pratica-aprendizado-de-maquina\\Heart_Segmentation_Advanced\\utils\\visualization_utils.py)\n",
      "Make sure utils modules are available in the project directory\n",
      "PyTorch version: 2.7.1+cpu\n",
      "‚ö†Ô∏è No GPU detected, using CPU\n",
      "Device: cpu\n",
      "Mixed precision: Disabled (CPU mode)\n",
      "\n",
      "‚úÖ Environment setup complete!\n",
      "üìÅ Working directory: .\n",
      "üéØ Ready for model evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Library Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import bootstrap\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from skimage import morphology, filters, segmentation\n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "# Check if running in Google Colab or VS Code\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üîµ Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive if needed\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install additional packages if needed\n",
    "    !pip install -q plotly\n",
    "    !pip install -q nibabel\n",
    "    !pip install -q SimpleITK\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üü¢ Running in VS Code/Local Environment\")\n",
    "    \n",
    "    # Check if packages are installed, install if needed\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        import plotly.express as px\n",
    "        import nibabel as nib\n",
    "        print(\"‚úÖ All required packages are available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Missing package: {e}\")\n",
    "        print(\"Installing missing packages...\")\n",
    "        # For VS Code, you can install packages using the terminal or pip\n",
    "        # !pip install plotly nibabel SimpleITK\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup paths based on environment\n",
    "if IN_COLAB:\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/Heart_Segmentation_Advanced')\n",
    "else:\n",
    "    BASE_DIR = Path('.')\n",
    "\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "UTILS_DIR = BASE_DIR / 'utils'\n",
    "\n",
    "# Add utils to path\n",
    "sys.path.append(str(UTILS_DIR))\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    from visualization_utils import VisualizationUtils\n",
    "    print(\"‚úÖ Custom modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import custom modules: {e}\")\n",
    "    print(\"Make sure utils modules are available in the project directory\")\n",
    "\n",
    "# PyTorch configuration\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# GPU configuration\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ CUDA available: {torch.cuda.device_count()} GPU(s)\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, using CPU\")\n",
    "\n",
    "# Mixed precision configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "use_amp = torch.cuda.is_available()  # Automatic Mixed Precision\n",
    "print(f'Device: {device}')\n",
    "if use_amp:\n",
    "    print('Mixed precision: Enabled (AMP)')\n",
    "else:\n",
    "    print('Mixed precision: Disabled (CPU mode)')\n",
    "\n",
    "print(\"\\n‚úÖ Environment setup complete!\")\n",
    "print(f\"üìÅ Working directory: {BASE_DIR}\")\n",
    "print(f\"üéØ Ready for model evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c908fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading PyTorch implementations from previous notebooks...\n",
      "‚úÖ PyTorch-based implementations loaded successfully!\n",
      "üìÅ Evaluation results will be saved to: outputs\\evaluation_results\n",
      "‚öôÔ∏è Configuration saved to: outputs\\evaluation_results\\evaluation_config.json\n",
      "üî• All functions use PyTorch tensors and operations\n"
     ]
    }
   ],
   "source": [
    "# Load PyTorch-based Implementations\n",
    "print(\"üì• Loading PyTorch implementations from previous notebooks...\")\n",
    "\n",
    "# PyTorch-based metric functions\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Calculate Dice coefficient using PyTorch tensors\"\"\"\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = torch.sum(y_true_f * y_pred_f)\n",
    "    dice_coeff = (2. * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + smooth)\n",
    "    return dice_coeff\n",
    "\n",
    "def iou_score(y_true, y_pred, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"Calculate IoU (Jaccard) score using PyTorch tensors\"\"\"\n",
    "    y_true_binary = (y_true > threshold).float()\n",
    "    y_pred_binary = (y_pred > threshold).float()\n",
    "    intersection = torch.sum(y_true_binary * y_pred_binary)\n",
    "    union = torch.sum(y_true_binary) + torch.sum(y_pred_binary) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou\n",
    "\n",
    "def sensitivity_score(y_true, y_pred, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"Calculate sensitivity (recall) using PyTorch tensors\"\"\"\n",
    "    y_true_binary = (y_true > threshold).float()\n",
    "    y_pred_binary = (y_pred > threshold).float()\n",
    "    true_positives = torch.sum(y_true_binary * y_pred_binary)\n",
    "    possible_positives = torch.sum(y_true_binary)\n",
    "    sensitivity = (true_positives + smooth) / (possible_positives + smooth)\n",
    "    return sensitivity\n",
    "\n",
    "def specificity_score(y_true, y_pred, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"Calculate specificity using PyTorch tensors\"\"\"\n",
    "    y_true_binary = (y_true > threshold).float()\n",
    "    y_pred_binary = (y_pred > threshold).float()\n",
    "    true_negatives = torch.sum((1 - y_true_binary) * (1 - y_pred_binary))\n",
    "    possible_negatives = torch.sum(1 - y_true_binary)\n",
    "    specificity = (true_negatives + smooth) / (possible_negatives + smooth)\n",
    "    return specificity\n",
    "\n",
    "def precision_score_custom(y_true, y_pred, threshold=0.5, smooth=1e-6):\n",
    "    \"\"\"Calculate precision using PyTorch tensors (renamed to avoid sklearn conflict)\"\"\"\n",
    "    y_true_binary = (y_true > threshold).float()\n",
    "    y_pred_binary = (y_pred > threshold).float()\n",
    "    true_positives = torch.sum(y_true_binary * y_pred_binary)\n",
    "    predicted_positives = torch.sum(y_pred_binary)\n",
    "    precision = (true_positives + smooth) / (predicted_positives + smooth)\n",
    "    return precision\n",
    "\n",
    "class MedicalMetrics:\n",
    "    \"\"\"Medical imaging metrics calculator for PyTorch tensors and numpy arrays\"\"\"\n",
    "    def __init__(self, threshold=0.5, smooth=1e-6):\n",
    "        self.threshold = threshold\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate comprehensive medical metrics\"\"\"\n",
    "        # Convert numpy arrays to tensors if needed\n",
    "        if isinstance(y_true, np.ndarray):\n",
    "            y_true = torch.from_numpy(y_true).float()\n",
    "        if isinstance(y_pred, np.ndarray):\n",
    "            y_pred = torch.from_numpy(y_pred).float()\n",
    "        \n",
    "        # Ensure tensors are on the same device\n",
    "        device = y_pred.device\n",
    "        y_true = y_true.to(device)\n",
    "        \n",
    "        metrics = {\n",
    "            'dice': dice_coefficient(y_true, y_pred, self.smooth).item(),\n",
    "            'iou': iou_score(y_true, y_pred, self.threshold, self.smooth).item(),\n",
    "            'sensitivity': sensitivity_score(y_true, y_pred, self.threshold, self.smooth).item(),\n",
    "            'specificity': specificity_score(y_true, y_pred, self.threshold, self.smooth).item(),\n",
    "            'precision': precision_score_custom(y_true, y_pred, self.threshold, self.smooth).item()\n",
    "        }\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        recall = metrics['sensitivity']  # Same as sensitivity\n",
    "        if metrics['precision'] + recall > 0:\n",
    "            f1_score = 2 * (metrics['precision'] * recall) / (metrics['precision'] + recall)\n",
    "        else:\n",
    "            f1_score = 0.0\n",
    "        \n",
    "        metrics['recall'] = recall\n",
    "        metrics['f1_score'] = f1_score\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    def compute_metrics_numpy(self, y_true_np, y_pred_np):\n",
    "        \"\"\"Compute metrics using numpy arrays (for compatibility)\"\"\"\n",
    "        y_true_binary = (y_true_np > self.threshold).astype(np.float32)\n",
    "        y_pred_binary = (y_pred_np > self.threshold).astype(np.float32)\n",
    "        \n",
    "        intersection = np.sum(y_true_binary * y_pred_binary)\n",
    "        union = np.sum(y_true_binary) + np.sum(y_pred_binary) - intersection\n",
    "        \n",
    "        # Dice coefficient\n",
    "        dice = (2.0 * intersection + self.smooth) / (np.sum(y_true_binary) + np.sum(y_pred_binary) + self.smooth)\n",
    "        \n",
    "        # IoU\n",
    "        iou = (intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Sensitivity and Specificity\n",
    "        true_positives = intersection\n",
    "        false_negatives = np.sum(y_true_binary * (1 - y_pred_binary))\n",
    "        false_positives = np.sum((1 - y_true_binary) * y_pred_binary)\n",
    "        true_negatives = np.sum((1 - y_true_binary) * (1 - y_pred_binary))\n",
    "        \n",
    "        sensitivity = true_positives / (true_positives + false_negatives + self.smooth)\n",
    "        specificity = true_negatives / (true_negatives + false_positives + self.smooth)\n",
    "        precision = true_positives / (true_positives + false_positives + self.smooth)\n",
    "        \n",
    "        f1 = 2 * (precision * sensitivity) / (precision + sensitivity + self.smooth)\n",
    "        \n",
    "        vol_true = np.sum(y_true_binary)\n",
    "        vol_pred = np.sum(y_pred_binary)\n",
    "        vol_sim = 1.0 - np.abs(vol_true - vol_pred) / (vol_true + vol_pred + self.smooth)\n",
    "        \n",
    "        return {\n",
    "            'dice': dice,\n",
    "            'iou': iou,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'precision': precision,\n",
    "            'f1': f1,\n",
    "            'volume_similarity': vol_sim\n",
    "        }\n",
    "\n",
    "class EvaluationConfig:\n",
    "    \"\"\"Configuration class for model evaluation\"\"\"\n",
    "    def __init__(self):\n",
    "        # Evaluation parameters\n",
    "        self.batch_size = 8\n",
    "        self.threshold = 0.5\n",
    "        \n",
    "        # Metrics parameters\n",
    "        self.smooth = 1e-6\n",
    "        self.hausdorff_percentile = 95\n",
    "        \n",
    "        # Visualization parameters\n",
    "        self.num_samples_to_show = 10\n",
    "        self.figure_size = (15, 10)\n",
    "        self.cmap = 'viridis'\n",
    "        \n",
    "        # Statistical analysis\n",
    "        self.confidence_level = 0.95\n",
    "        self.bootstrap_samples = 1000\n",
    "        \n",
    "        # Paths\n",
    "        self.results_dir = OUTPUT_DIR / 'evaluation_results'\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def save_config(self, path=None):\n",
    "        if path is None:\n",
    "            path = self.results_dir / 'evaluation_config.json'\n",
    "        \n",
    "        config_dict = {k: v for k, v in self.__dict__.items() \n",
    "                      if not k.startswith('_') and isinstance(v, (str, int, float, bool, list))}\n",
    "        \n",
    "        # Convert Path objects to strings\n",
    "        for key, value in config_dict.items():\n",
    "            if isinstance(value, Path):\n",
    "                config_dict[key] = str(value)\n",
    "        \n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "        \n",
    "        return path\n",
    "\n",
    "# Initialize evaluation configuration\n",
    "eval_config = EvaluationConfig()\n",
    "config_path = eval_config.save_config()\n",
    "\n",
    "# Initialize metrics calculator\n",
    "metrics_calculator = MedicalMetrics()\n",
    "\n",
    "print(\"‚úÖ PyTorch-based implementations loaded successfully!\")\n",
    "print(f\"üìÅ Evaluation results will be saved to: {eval_config.results_dir}\")\n",
    "print(f\"‚öôÔ∏è Configuration saved to: {config_path}\")\n",
    "print(\"üî• All functions use PyTorch tensors and operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f73a6b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è ModelManager initialized\n",
      "üìÅ Model directory: models\n",
      "üîç Discovering available models...\n",
      "‚ö†Ô∏è No PyTorch models found (.pth or .pt files)\n",
      "\n",
      "üìã Model Manager initialized!\n",
      "üîç Found 0 available models\n",
      "‚ö†Ô∏è No trained models found. Creating dummy model for evaluation pipeline testing...\n",
      "üîß Creating dummy PyTorch U-Net model for testing...\n",
      "‚úÖ Dummy PyTorch U-Net model created and added to loaded models\n",
      "üí° Tip: Train models using the previous notebooks to see real evaluation results!\n"
     ]
    }
   ],
   "source": [
    "# Model Loading and Management (PyTorch)\n",
    "class ModelManager:\n",
    "    \"\"\"\n",
    "    Utility class for loading and managing trained PyTorch models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir=None):\n",
    "        self.model_dir = model_dir or MODEL_DIR\n",
    "        self.loaded_models = {}\n",
    "        self.model_configs = {}\n",
    "        \n",
    "        # Ensure model directory exists\n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"üèóÔ∏è ModelManager initialized\")\n",
    "        print(f\"üìÅ Model directory: {self.model_dir}\")\n",
    "    \n",
    "    def discover_models(self):\n",
    "        \"\"\"\n",
    "        Discover available PyTorch models in the model directory\n",
    "        \"\"\"\n",
    "        print(\"üîç Discovering available models...\")\n",
    "        \n",
    "        available_models = []\n",
    "        \n",
    "        # Search for .pth and .pt model files\n",
    "        for model_file in self.model_dir.glob('*.pth'):\n",
    "            model_info = {\n",
    "                'name': model_file.stem,\n",
    "                'path': model_file,\n",
    "                'format': 'pth',\n",
    "                'size_mb': model_file.stat().st_size / (1024 * 1024)\n",
    "            }\n",
    "            available_models.append(model_info)\n",
    "        \n",
    "        for model_file in self.model_dir.glob('*.pt'):\n",
    "            model_info = {\n",
    "                'name': model_file.stem,\n",
    "                'path': model_file,\n",
    "                'format': 'pt',\n",
    "                'size_mb': model_file.stat().st_size / (1024 * 1024)\n",
    "            }\n",
    "            available_models.append(model_info)\n",
    "        \n",
    "        # Print discovered models\n",
    "        if available_models:\n",
    "            print(f\"‚úÖ Found {len(available_models)} model(s):\")\n",
    "            for model in available_models:\n",
    "                print(f\"  üì¶ {model['name']} ({model['format']}, {model['size_mb']:.1f} MB)\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No PyTorch models found (.pth or .pt files)\")\n",
    "        \n",
    "        return available_models\n",
    "    \n",
    "    def load_model(self, model_name_or_path, model_class=None):\n",
    "        \"\"\"\n",
    "        Load a PyTorch model from file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Determine model path\n",
    "            if isinstance(model_name_or_path, (str, Path)) and Path(model_name_or_path).exists():\n",
    "                model_path = Path(model_name_or_path)\n",
    "                model_name = model_path.stem\n",
    "            else:\n",
    "                model_name = model_name_or_path\n",
    "                # Try to find the model\n",
    "                possible_paths = [\n",
    "                    self.model_dir / f\"{model_name}.pth\",\n",
    "                    self.model_dir / f\"{model_name}.pt\",\n",
    "                    self.model_dir / model_name\n",
    "                ]\n",
    "                model_path = None\n",
    "                for path in possible_paths:\n",
    "                    if path.exists():\n",
    "                        model_path = path\n",
    "                        break\n",
    "                \n",
    "                if model_path is None:\n",
    "                    raise FileNotFoundError(f\"Model '{model_name}' not found in {self.model_dir}\")\n",
    "            \n",
    "            # Load the model\n",
    "            print(f\"üîÑ Loading PyTorch model: {model_name}\")\n",
    "            \n",
    "            # Load checkpoint\n",
    "            checkpoint = torch.load(model_path, map_location=device)\n",
    "            \n",
    "            # If model_class is provided, create model instance\n",
    "            if model_class is not None:\n",
    "                model = model_class()\n",
    "                model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "            else:\n",
    "                # Try to load the entire model (if saved with torch.save(model, path))\n",
    "                try:\n",
    "                    model = checkpoint\n",
    "                    if not isinstance(model, nn.Module):\n",
    "                        # If checkpoint contains state dict, we need the model architecture\n",
    "                        print(\"‚ö†Ô∏è Checkpoint contains state dict but no model class provided\")\n",
    "                        print(\"Creating a dummy Enhanced U-Net model...\")\n",
    "                        model = self.create_dummy_model()\n",
    "                        if 'model_state_dict' in checkpoint:\n",
    "                            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        else:\n",
    "                            model.load_state_dict(checkpoint)\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏è Could not load model directly, creating dummy model...\")\n",
    "                    model = self.create_dummy_model()\n",
    "                    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "                        try:\n",
    "                            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                        except:\n",
    "                            print(\"‚ö†Ô∏è Could not load state dict, using dummy model\")\n",
    "                    elif isinstance(checkpoint, dict):\n",
    "                        try:\n",
    "                            model.load_state_dict(checkpoint)\n",
    "                        except:\n",
    "                            print(\"‚ö†Ô∏è Could not load state dict, using dummy model\")\n",
    "            \n",
    "            # Move model to device\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Store loaded model\n",
    "            self.loaded_models[model_name] = model\n",
    "            \n",
    "            # Try to load associated configuration\n",
    "            config_path = model_path.parent / f\"{model_name}_config.json\"\n",
    "            if config_path.exists():\n",
    "                with open(config_path, 'r') as f:\n",
    "                    self.model_configs[model_name] = json.load(f)\n",
    "                print(f\"‚úÖ Configuration loaded for {model_name}\")\n",
    "            \n",
    "            # Get model info\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            \n",
    "            print(f\"‚úÖ Model '{model_name}' loaded successfully!\")\n",
    "            print(f\"üìä Model summary:\")\n",
    "            print(f\"  - Total parameters: {total_params:,}\")\n",
    "            print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
    "            print(f\"  - Device: {next(model.parameters()).device}\")\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model '{model_name_or_path}': {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_model_info(self, model_name):\n",
    "        \"\"\"\n",
    "        Get information about a loaded PyTorch model\n",
    "        \"\"\"\n",
    "        if model_name not in self.loaded_models:\n",
    "            return None\n",
    "        \n",
    "        model = self.loaded_models[model_name]\n",
    "        config = self.model_configs.get(model_name, {})\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        info = {\n",
    "            'name': model_name,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params,\n",
    "            'device': str(next(model.parameters()).device),\n",
    "            'model_type': model.__class__.__name__,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def create_dummy_model(self, input_channels=3, output_channels=1):\n",
    "        \"\"\"\n",
    "        Create a dummy PyTorch U-Net model for testing evaluation pipeline\n",
    "        \"\"\"\n",
    "        print(\"üîß Creating dummy PyTorch U-Net model for testing...\")\n",
    "        \n",
    "        class DummyUNet(nn.Module):\n",
    "            def __init__(self, in_channels=3, out_channels=1):\n",
    "                super(DummyUNet, self).__init__()\n",
    "                \n",
    "                # Encoder\n",
    "                self.enc1 = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, 64, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(64, 64, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "                self.pool1 = nn.MaxPool2d(2)\n",
    "                \n",
    "                self.enc2 = nn.Sequential(\n",
    "                    nn.Conv2d(64, 128, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(128, 128, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "                self.pool2 = nn.MaxPool2d(2)\n",
    "                \n",
    "                # Bottleneck\n",
    "                self.bottleneck = nn.Sequential(\n",
    "                    nn.Conv2d(128, 256, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(256, 256, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "                \n",
    "                # Decoder\n",
    "                self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "                self.dec2 = nn.Sequential(\n",
    "                    nn.Conv2d(256, 128, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(128, 128, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "                \n",
    "                self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "                self.dec1 = nn.Sequential(\n",
    "                    nn.Conv2d(128, 64, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(64, 64, 3, padding=1),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "                \n",
    "                # Output\n",
    "                self.final_conv = nn.Conv2d(64, out_channels, 1)\n",
    "                self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Encoder\n",
    "                enc1 = self.enc1(x)\n",
    "                enc2 = self.enc2(self.pool1(enc1))\n",
    "                \n",
    "                # Bottleneck\n",
    "                bottleneck = self.bottleneck(self.pool2(enc2))\n",
    "                \n",
    "                # Decoder\n",
    "                dec2 = self.upconv2(bottleneck)\n",
    "                dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "                dec2 = self.dec2(dec2)\n",
    "                \n",
    "                dec1 = self.upconv1(dec2)\n",
    "                dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "                dec1 = self.dec1(dec1)\n",
    "                \n",
    "                # Output\n",
    "                out = self.final_conv(dec1)\n",
    "                out = self.sigmoid(out)\n",
    "                \n",
    "                return out\n",
    "        \n",
    "        model = DummyUNet(input_channels, output_channels)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        self.loaded_models['dummy_model'] = model\n",
    "        \n",
    "        print(\"‚úÖ Dummy PyTorch U-Net model created and added to loaded models\")\n",
    "        return model\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# Discover available models\n",
    "available_models = model_manager.discover_models()\n",
    "\n",
    "print(f\"\\nüìã Model Manager initialized!\")\n",
    "print(f\"üîç Found {len(available_models)} available models\")\n",
    "\n",
    "# If no models found, create a dummy model for demonstration\n",
    "if not available_models:\n",
    "    print(\"‚ö†Ô∏è No trained models found. Creating dummy model for evaluation pipeline testing...\")\n",
    "    dummy_model = model_manager.create_dummy_model()\n",
    "    print(\"üí° Tip: Train models using the previous notebooks to see real evaluation results!\")\n",
    "else:\n",
    "    # Load the first available model\n",
    "    first_model = available_models[0]\n",
    "    print(f\"üöÄ Loading first available model: {first_model['name']}\")\n",
    "    loaded_model = model_manager.load_model(first_model['name'])\n",
    "    if loaded_model is not None:\n",
    "        print(\"‚úÖ Model loaded successfully for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77f53e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 48 (220666143.py, line 49)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mresults = self._compute_comprehensive_metrics(ground_truth, predictions, dataset_name)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'if' statement on line 48\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Quantitative Evaluation Framework (PyTorch)\n",
    "class QuantitativeEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive quantitative evaluation for medical image segmentation using PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or eval_config\n",
    "        self.metrics_calculator = MedicalMetrics(\n",
    "            threshold=self.config.threshold,\n",
    "            smooth=self.config.smooth\n",
    "        )\n",
    "        self.results_history = []\n",
    "        \n",
    "    def evaluate_model_on_dataset(self, model, dataset, dataset_name=\"test\"):\n",
    "        \"\"\"\n",
    "        Evaluate PyTorch model on a complete dataset\n",
    "        \"\"\"\n",
    "        print(f\"üìä Evaluating PyTorch model on {dataset_name} dataset...\")\n",
    "        \n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_ground_truth = []\n",
    "        all_images = []\n",
    "        \n",
    "        batch_count = 0\n",
    "        \n",
    "        # Collect all predictions and ground truth\n",
    "        with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "            for batch_data in dataset:\n",
    "                if isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:\n",
    "                    batch_images, batch_masks = batch_data\n",
    "                else:\n",
    "                    # Handle single tensor input\n",
    "                    batch_images = batch_data\n",
    "                    batch_masks = None\n",
    "                \n",
    "                # Convert to PyTorch tensors if needed\n",
    "                if isinstance(batch_images, np.ndarray):\n",
    "                    batch_images = torch.from_numpy(batch_images).float()\n",
    "                if isinstance(batch_masks, np.ndarray):\n",
    "                    batch_masks = torch.from_numpy(batch_masks).float()\n",
    "                \n",
    "                # Move to device\n",
    "                if batch_images.device != device:\n",
    "                    batch_images = batch_images.to(device)            batch_images = batch_images.to(device)\n",
    "                if batch_masks is not None and batch_masks.device != device:one and batch_masks.device != device:\n",
    "                    batch_masks = batch_masks.to(device)\n",
    "                                \n",
    "                # Make predictions\n",
    "                if batch_images.dim() == 3:  # Add batch dimension if neededes.dim() == 3:  # Add batch dimension if neededes.dim() == 3:  # Add batch dimension if needed\n",
    "                    batch_images = batch_images.unsqueeze(0)images.unsqueeze(0)images.unsqueeze(0)\n",
    "                \n",
    "                # Ensure correct channel dimension (B, C, H, W)ension (B, C, H, W)ension (B, C, H, W)\n",
    "                if batch_images.shape[1] != 3 and batch_images.shape[-1] == 3:\n",
    "                    batch_images = batch_images.permute(0, 3, 1, 2)s = batch_images.permute(0, 3, 1, 2)s = batch_images.permute(0, 3, 1, 2)\n",
    "                \n",
    "                batch_predictions = model(batch_images)(batch_images)(batch_images)\n",
    "                \n",
    "                # Convert predictions back to numpy for metrics calculation       # Convert predictions back to numpy for metrics calculation       # Convert predictions back to numpy for metrics calculation\n",
    "                if isinstance(batch_predictions, torch.Tensor):        if isinstance(batch_predictions, torch.Tensor):        if isinstance(batch_predictions, torch.Tensor):\n",
    "                    batch_predictions = batch_predictions.cpu().numpy()ons.cpu().numpy()ons.cpu().numpy()\n",
    "                if isinstance(batch_images, torch.Tensor):        if isinstance(batch_images, torch.Tensor):        if isinstance(batch_images, torch.Tensor):\n",
    "                    batch_images = batch_images.cpu().numpy() = batch_images.cpu().numpy() = batch_images.cpu().numpy()\n",
    "                if batch_masks is not None and isinstance(batch_masks, torch.Tensor):            if batch_masks is not None and isinstance(batch_masks, torch.Tensor):            if batch_masks is not None and isinstance(batch_masks, torch.Tensor):\n",
    "                    batch_masks = batch_masks.cpu().numpy()\n",
    "                          \n",
    "                # Store results\n",
    "                all_predictions.extend(batch_predictions)     all_predictions.extend(batch_predictions)     all_predictions.extend(batch_predictions)\n",
    "                all_images.extend(batch_images)\n",
    "                if batch_masks is not None:        if batch_masks is not None:        if batch_masks is not None:\n",
    "                    all_ground_truth.extend(batch_masks)und_truth.extend(batch_masks)und_truth.extend(batch_masks)\n",
    "                                \n",
    "                batch_count += 1\n",
    "                if batch_count % 10 == 0:\n",
    "                    print(f\"  Processed {batch_count} batches...\")ed {batch_count} batches...\")ed {batch_count} batches...\")\n",
    "        \n",
    "        # Convert to numpy arraysnvert to numpy arraysnvert to numpy arrays\n",
    "        predictions = np.array(all_predictions)s)s)\n",
    "        images = np.array(all_images)\n",
    "        \n",
    "        if all_ground_truth:if all_ground_truth:if all_ground_truth:\n",
    "            ground_truth = np.array(all_ground_truth)np.array(all_ground_truth)np.array(all_ground_truth)\n",
    "        else:\n",
    "            # Create dummy ground truth for demonstration    # Create dummy ground truth for demonstration    # Create dummy ground truth for demonstration\n",
    "            print(\"‚ö†Ô∏è No ground truth available, creating synthetic masks for demonstration\")ruth available, creating synthetic masks for demonstration\")ruth available, creating synthetic masks for demonstration\")\n",
    "            ground_truth = self._create_synthetic_masks(images, predictions)mages, predictions)mages)\n",
    "        \n",
    "        print(f\"‚úÖ Collected {len(predictions)} samples for evaluation\")print(f\"‚úÖ Collected {len(predictions)} samples for evaluation\")print(f\"‚úÖ Collected {len(predictions)} samples for evaluation\")\n",
    "        \n",
    "        # Compute comprehensive metrics    # Compute comprehensive metrics    # Compute comprehensive metrics\n",
    "        results = self._compute_comprehensive_metrics(ground_truth, predictions, dataset_name)rics(ground_truth, predictions, dataset_name)rics(ground_truth, predictions, dataset_name)\n",
    "        \n",
    "        # Store evaluation results\n",
    "        evaluation_record = {luation_record = {luation_record = {\n",
    "            'dataset_name': dataset_name,taset_name,taset_name,\n",
    "            'timestamp': datetime.now().isoformat(),p': datetime.now().isoformat(),p': datetime.now().isoformat(),\n",
    "            'num_samples': len(predictions),    'num_samples': len(predictions),    'num_samples': len(predictions),\n",
    "            'model_name': model.__class__.__name__ if hasattr(model, '__class__') else 'unknown',l.__class__.__name__ if hasattr(model, '__class__') else 'unknown',l.__class__.__name__ if hasattr(model, '__class__') else 'unknown',\n",
    "            'results': results,\n",
    "            'predictions': predictions,s': predictions,s': predictions,\n",
    "            'ground_truth': ground_truth,    'ground_truth': ground_truth,    'ground_truth': ground_truth,\n",
    "            'images': images\n",
    "        }\n",
    "        \n",
    "        self.results_history.append(evaluation_record).results_history.append(evaluation_record).results_history.append(evaluation_record)\n",
    "        \n",
    "        return evaluation_record\n",
    "    \n",
    "    def _create_synthetic_masks(self, images, predictions):\n",
    "        \"\"\"Create synthetic ground truth masks for demonstration\"\"\"\n",
    "        masks = []\n",
    "        for i, (img, pred) in enumerate(zip(images, predictions))::\n",
    "            # Create a simple circular mask in the center\n",
    "            if len(img.shape) == 4:  # Batch dimension\n",
    "                h, w = img.shape[2], img.shape[3]h, w = img.shape[2], img.shape[3]h, w = img.shape[2], img.shape[3]\n",
    "            elif len(img.shape) == 3:\n",
    "                h, w = img.shape[1], img.shape[2]\n",
    "            else:\n",
    "                h, w = img.shape[0], img.shape[1]h, w = img.shape[0], img.shape[1]h, w = img.shape[0], img.shape[1]\n",
    "            \n",
    "            center_x, center_y = h // 2, w // 2\n",
    "            radius = min(h, w) // 4\n",
    "            \n",
    "            y, x = np.ogrid[:h, :w]p.ogrid[:h, :w]p.ogrid[:h, :w]\n",
    "            mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "            mask = mask.astype(np.float32)\n",
    "            \n",
    "            # Add some noise to make it more realisticme noise to make it more realisticme noise to make it more realistic\n",
    "            mask = mask + np.random.normal(0, 0.1, mask.shape)\n",
    "            mask = np.clip(mask, 0, 1)\n",
    "            \n",
    "            # Reshape to match predictions formations formations format\n",
    "            if len(pred.shape) == 3:  # Has channel dimension\n",
    "                mask = mask.reshape(*mask.shape, 1)\n",
    "            \n",
    "            masks.append(mask)s.append(mask)s.append(mask)\n",
    "        \n",
    "        return np.array(masks)\n",
    "    \n",
    "    def _compute_comprehensive_metrics(self, ground_truth, predictions, dataset_name):trics(self, ground_truth, predictions, dataset_name):trics(self, ground_truth, predictions, dataset_name):\n",
    "        \"\"\"\n",
    "        Compute comprehensive metrics for all samples\n",
    "        \"\"\"\n",
    "        print(f\"üî¢ Computing comprehensive metrics...\")print(f\"üî¢ Computing comprehensive metrics...\")print(f\"üî¢ Computing comprehensive metrics...\")\n",
    "        \n",
    "        sample_metrics = []    sample_metrics = []    sample_metrics = []\n",
    "        \n",
    "        # Compute metrics for each sampleompute metrics for each sampleompute metrics for each sample\n",
    "        for i, (gt, pred) in enumerate(zip(ground_truth, predictions)):, predictions)):, predictions)):\n",
    "            if i % 50 == 0 and i > 0: if i % 50 == 0 and i > 0: if i % 50 == 0 and i > 0:\n",
    "                print(f\"  Processed {i}/{len(ground_truth)} samples\")\n",
    "                    \n",
    "            # Compute metrics for this samplesamplesample\n",
    "            metrics = self.metrics_calculator.compute_metrics_numpy(gt, pred)_numpy(gt, pred)_numpy(gt, pred)\n",
    "            sample_metrics.append(metrics)ics.append(metrics)ics.append(metrics)\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        aggregated_metrics = self._aggregate_metrics(sample_metrics) self._aggregate_metrics(sample_metrics) self._aggregate_metrics(sample_metrics)\n",
    "        \n",
    "        # Add dataset information\n",
    "        aggregated_metrics['dataset_name'] = dataset_nameaset_nameaset_name\n",
    "        aggregated_metrics['num_samples'] = len(ground_truth)\n",
    "        \n",
    "        return aggregated_metrics\n",
    "    \n",
    "    def _aggregate_metrics(self, sample_metrics):\n",
    "        \"\"\"\"\"\"\"\"\"\n",
    "        Aggregate metrics across all samples with statistical measuresh statistical measuresh statistical measures\n",
    "        \"\"\"\n",
    "        if not sample_metrics:\n",
    "            return {}\n",
    "        \n",
    "        # Get all metric namesl metric namesl metric names\n",
    "        metric_names = sample_metrics[0].keys()\n",
    "        aggregated = {}\n",
    "        \n",
    "        for metric_name in metric_names:ic_name in metric_names:ic_name in metric_names:\n",
    "            values = [sample[metric_name] for sample in sample_metrics c_name] for sample in sample_metrics c_name] for sample in sample_metrics \n",
    "                     if not np.isnan(sample[metric_name]) and not np.isinf(sample[metric_name])]t np.isinf(sample[metric_name])]t np.isinf(sample[metric_name])]\n",
    "            \n",
    "            if values:\n",
    "                # Basic statistics# Basic statistics# Basic statistics\n",
    "                aggregated[f'{metric_name}_mean'] = np.mean(values)ues)ues)\n",
    "                aggregated[f'{metric_name}_std'] = np.std(values)name}_std'] = np.std(values)name}_std'] = np.std(values)\n",
    "                aggregated[f'{metric_name}_median'] = np.median(values)name}_median'] = np.median(values)name}_median'] = np.median(values)\n",
    "                aggregated[f'{metric_name}_min'] = np.min(values)min'] = np.min(values)min'] = np.min(values)\n",
    "                aggregated[f'{metric_name}_max'] = np.max(values)max'] = np.max(values)max'] = np.max(values)\n",
    "                aggregated[f'{metric_name}_q25'] = np.percentile(values, 25) np.percentile(values, 25) np.percentile(values, 25)\n",
    "                aggregated[f'{metric_name}_q75'] = np.percentile(values, 75)ntile(values, 75)ntile(values, 75)\n",
    "                \n",
    "                # Confidence interval\n",
    "                confidence_level = self.config.confidence_levelonfidence_level = self.config.confidence_levelonfidence_level = self.config.confidence_level\n",
    "                alpha = 1 - confidence_level        alpha = 1 - confidence_level        alpha = 1 - confidence_level\n",
    "                \n",
    "                try:            try:            try:\n",
    "                    # Bootstrap confidence intervalrvalrval\n",
    "                    def bootstrap_mean(data):         def bootstrap_mean(data):         def bootstrap_mean(data):\n",
    "                        return np.mean(np.random.choice(data, size=len(data), replace=True))np.random.choice(data, size=len(data), replace=True))np.random.choice(data, size=len(data), replace=True))\n",
    "                                      \n",
    "                    bootstrap_means = [bootstrap_mean(values) for _ in range(self.config.bootstrap_samples)]rap_means = [bootstrap_mean(values) for _ in range(self.config.bootstrap_samples)]rap_means = [bootstrap_mean(values) for _ in range(self.config.bootstrap_samples)]\n",
    "                    ci_lower = np.percentile(bootstrap_means, 100 * alpha / 2)np.percentile(bootstrap_means, 100 * alpha / 2)np.percentile(bootstrap_means, 100 * alpha / 2)\n",
    "                    ci_upper = np.percentile(bootstrap_means, 100 * (1 - alpha / 2)) = np.percentile(bootstrap_means, 100 * (1 - alpha / 2)) = np.percentile(bootstrap_means, 100 * (1 - alpha / 2))\n",
    "                    \n",
    "                    aggregated[f'{metric_name}_ci_lower'] = ci_lowered[f'{metric_name}_ci_lower'] = ci_lowered[f'{metric_name}_ci_lower'] = ci_lower\n",
    "                    aggregated[f'{metric_name}_ci_upper'] = ci_upperted[f'{metric_name}_ci_upper'] = ci_upperted[f'{metric_name}_ci_upper'] = ci_upper\n",
    "                                  \n",
    "                except Exception as e:eption as e:eption as e:\n",
    "                    print(f\"Warning: Could not compute confidence interval for {metric_name}: {e}\")                print(f\"Warning: Could not compute confidence interval for {metric_name}: {e}\")                print(f\"Warning: Could not compute confidence interval for {metric_name}: {e}\")\n",
    "                    aggregated[f'{metric_name}_ci_lower'] = np.nan\n",
    "                    aggregated[f'{metric_name}_ci_upper'] = np.nan         aggregated[f'{metric_name}_ci_upper'] = np.nan         aggregated[f'{metric_name}_ci_upper'] = np.nan\n",
    "                \n",
    "                # Count of valid samples     # Count of valid samples     # Count of valid samples\n",
    "                aggregated[f'{metric_name}_count'] = len(values){metric_name}_count'] = len(values){metric_name}_count'] = len(values)\n",
    "            else:\n",
    "                # No valid values        # No valid values        # No valid values\n",
    "                for suffix in ['_mean', '_std', '_median', '_min', '_max', '_q25', '_q75', '_ci_lower', '_ci_upper']:ix in ['_mean', '_std', '_median', '_min', '_max', '_q25', '_q75', '_ci_lower', '_ci_upper']:ix in ['_mean', '_std', '_median', '_min', '_max', '_q25', '_q75', '_ci_lower', '_ci_upper']:\n",
    "                    aggregated[f'{metric_name}{suffix}'] = np.nanname}{suffix}'] = np.nanname}{suffix}'] = np.nan\n",
    "                aggregated[f'{metric_name}_count'] = 0        aggregated[f'{metric_name}_count'] = 0        aggregated[f'{metric_name}_count'] = 0\n",
    "                        \n",
    "        return aggregated\n",
    "    \n",
    "    def generate_evaluation_report(self, results, save_path=None):t, metric_name='dice_mean'):t, metric_name='dice_mean'):\n",
    "        \"\"\"\n",
    "        Generate comprehensive evaluation report        Compare multiple models using statistical tests        Compare multiple models using statistical tests\n",
    "        \"\"\"\n",
    "        if save_path is None:print(f\"üìä Comparing {len(model_results_list)} models on {metric_name}\")print(f\"üìä Comparing {len(model_results_list)} models on {metric_name}\")\n",
    "            save_path = self.config.results_dir / f\"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"    \n",
    "        if len(model_results_list) < 2:if len(model_results_list) < 2:\n",
    "        # Build report contentNeed at least 2 models for comparison\")Need at least 2 models for comparison\")\n",
    "        lines = []\n",
    "        lines.append(\"ü´Ä CARDIAC SEGMENTATION EVALUATION REPORT\")\n",
    "        lines.append(\"=\" * 60)isonison\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"üìä Dataset: {results['dataset_name']}\")ist:ist:\n",
    "        lines.append(f\"üìÖ Evaluation Date: {results['timestamp']}\")', 'unknown')', 'unknown')\n",
    "        lines.append(f\"üî¢ Number of Samples: {results['num_samples']}\")::\n",
    "        lines.append(f\"üèóÔ∏è Model: {results['model_name']}\")    comparison_data[model_name] = result['results'][metric_name]    comparison_data[model_name] = result['results'][metric_name]\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"üìà QUANTITATIVE RESULTS\")\n",
    "        lines.append(\"-\" * 30)\n",
    "        \n",
    "        # Core metrics\n",
    "        core_metrics = ['dice', 'iou', 'sensitivity', 'specificity', 'precision', 'f1']odel1 in enumerate(model_names):odel1 in enumerate(model_names):\n",
    "        \n",
    "        for metric in core_metrics:\n",
    "            mean_key = f'{metric}_mean'\n",
    "            std_key = f'{metric}_std'        value2 = comparison_data[model2]        value2 = comparison_data[model2]\n",
    "            ci_lower_key = f'{metric}_ci_lower'\n",
    "            ci_upper_key = f'{metric}_ci_upper'te effect size (Cohen's d)te effect size (Cohen's d)\n",
    "                            std_key = metric_name.replace('_mean', '_std')                std_key = metric_name.replace('_mean', '_std')\n",
    "            if mean_key in results['results']:\n",
    "                mean_val = results['results'][mean_key]        # Get standard deviations        # Get standard deviations\n",
    "                std_val = results['results'][std_key]ist if r.get('model_name') == model1)ist if r.get('model_name') == model1)\n",
    "                ci_lower = results['results'].get(ci_lower_key, np.nan)sults_list if r.get('model_name') == model2)sults_list if r.get('model_name') == model2)\n",
    "                ci_upper = results['results'].get(ci_upper_key, np.nan)                                \n",
    "                ult1['results'].get(std_key, 0)ult1['results'].get(std_key, 0)\n",
    "                line = f\"{metric.upper():12}: {mean_val:.4f} ¬± {std_val:.4f}\"        std2 = result2['results'].get(std_key, 0)        std2 = result2['results'].get(std_key, 0)\n",
    "                if not np.isnan(ci_lower) and not np.isnan(ci_upper):                    \n",
    "                    line += f\" [CI: {ci_lower:.4f}-{ci_upper:.4f}]\"        pooled_std = np.sqrt((std1**2 + std2**2) / 2)        pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n",
    "                lines.append(line)\n",
    "        \n",
    "        lines.append(\"\") = abs(value1 - value2) / pooled_std = abs(value1 - value2) / pooled_std\n",
    "        lines.append(\"üìä STATISTICAL SUMMARY\")\n",
    "        lines.append(\"-\" * 30) 0 0\n",
    "        lines.append(f\"Confidence Level: {self.config.confidence_level*100:.0f}%\")\n",
    "        lines.append(f\"Bootstrap Samples: {self.config.bootstrap_samples}\")ults[f\"{model1}_vs_{model2}\"] = {ults[f\"{model1}_vs_{model2}\"] = {\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"üéØ CLINICAL INTERPRETATION\")       'model2': model2,       'model2': model2,\n",
    "        lines.append(\"-\" * 30)\n",
    "                    'model2_value': value2,            'model2_value': value2,\n",
    "        # Clinical interpretationerence': value1 - value2,erence': value1 - value2,\n",
    "        dice_mean = results['results'].get('dice_mean', 0)                    'abs_difference': abs(value1 - value2),                    'abs_difference': abs(value1 - value2),\n",
    "        if dice_mean >= 0.9:ohens_d': cohens_d,ohens_d': cohens_d,\n",
    "            lines.append(\"‚úÖ EXCELLENT: Dice score ‚â• 0.90 - Clinical quality segmentation\")            'effect_size': self._interpret_effect_size(cohens_d)            'effect_size': self._interpret_effect_size(cohens_d)\n",
    "        elif dice_mean >= 0.8:          }          }\n",
    "            lines.append(\"üü¢ GOOD: Dice score ‚â• 0.80 - Acceptable for clinical use\")\n",
    "        elif dice_mean >= 0.7:\n",
    "            lines.append(\"üü° MODERATE: Dice score ‚â• 0.70 - May need improvement\")\n",
    "        else:\n",
    "            lines.append(\"üî¥ POOR: Dice score < 0.70 - Significant improvement needed\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        lines.append(\"=\" * 60)\n",
    "        \n",
    "        # Join all lines\n",
    "        report = \"\\n\".join(lines)\n",
    "        \n",
    "        # Save report\n",
    "        with open(save_path, 'w') as f:else:else:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"üìÑ Evaluation report saved to: {save_path}\")uation_report(self, results, save_path=None):uation_report(self, results, save_path=None):\n",
    "        print(report)\n",
    "        ive evaluation reportive evaluation report\n",
    "        return report\"\"\"\"\"\"\n",
    "\n",
    "# Initialize quantitative evaluator = self.config.results_dir / f\"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\" = self.config.results_dir / f\"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "quantitative_evaluator = QuantitativeEvaluator()\n",
    "t = []\n",
    "print(\"‚úÖ Comprehensive Quantitative Evaluation Framework initialized!\")        report_content.append(\"ü´Ä CARDIAC SEGMENTATION EVALUATION REPORT\")ü´Ä CARDIAC SEGMENTATION EVALUATION REPORT\n",
    "print(\"üî¢ Features:\") 60)\n",
    "print(\"  - PyTorch model evaluation\")\n",
    "print(\"  - Per-sample metrics computation\")        report_content.append(f\"üìä Dataset: {results['dataset_name']}\")üìä Dataset: {results['dataset_name']}\n",
    "print(\"  - Statistical aggregation with confidence intervals\")']}\")\n",
    "print(\"  - Bootstrap confidence intervals\")t.append(f\"üî¢ Number of Samples: {results['num_samples']}\") {results['num_samples']}\n",
    "print(\"  - Clinical interpretation guidelines\"){results['model_name']}\")\n",
    "print(\"  - Comprehensive evaluation reports\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"  - Comprehensive evaluation reports\")print(\"  - Clinical interpretation guidelines\")print(\"  - Model comparison with effect sizes\")print(\"  - Bootstrap confidence intervals\")print(\"  - Statistical aggregation with confidence intervals\")print(\"  - Per-sample metrics computation\")print(\"  - PyTorch model evaluation\")print(\"üî¢ Features:\")print(\"‚úÖ Comprehensive Quantitative Evaluation Framework initialized!\")quantitative_evaluator = QuantitativeEvaluator()# Initialize quantitative evaluator        return report                print(report)        print(f\"üìÑ Evaluation report saved to: {save_path}\")                    f.write(report)        with open(save_path, 'w') as f:        # Save report                report = \"\\n\".join(report_content)        # Join all lines                report_content.append(\"=\" * 60)                        report_content.append(\"\")                report_content.append(f\"  Q75:      {results['results'][f'{metric}_q75']:.4f}\")                report_content.append(f\"  Q25:      {results['results'][f'{metric}_q25']:.4f}\")                report_content.append(f\"  Max:      {results['results'][f'{metric}_max']:.4f}\")                report_content.append(f\"  Min:      {results['results'][f'{metric}_min']:.4f}\")                report_content.append(f\"  Median:   {results['results'][f'{metric}_median']:.4f}\")                report_content.append(f\"  Std:      {results['results'][f'{metric}_std']:.4f}\")                report_content.append(f\"  Mean:     {results['results'][f'{metric}_mean']:.4f}\")                report_content.append(f\"{metric.upper()}:\")            if f'{metric}_mean' in results['results']:        for metric in core_metrics:        # Detailed statistics table                report_content.append(\"-\" * 30)        report_content.append(\"üìã DETAILED STATISTICS\")        report_content.append(\"\")                    report_content.append(\"üî¥ POOR: Dice score < 0.70 - Significant improvement needed\")        else:            report_content.append(\"üü° MODERATE: Dice score ‚â• 0.70 - May need improvement\")        elif dice_mean >= 0.7:            report_content.append(\"üü¢ GOOD: Dice score ‚â• 0.80 - Acceptable for clinical use\")        elif dice_mean >= 0.8:            report_content.append(\"‚úÖ EXCELLENT: Dice score ‚â• 0.90 - Clinical quality segmentation\")        if dice_mean >= 0.9:        dice_mean = results['results'].get('dice_mean', 0)        # Clinical interpretation                report_content.append(\"-\" * 30)        report_content.append(\"üéØ CLINICAL INTERPRETATION\")        report_content.append(\"\")        report_content.append(f\"Bootstrap Samples: {self.config.bootstrap_samples}\")        report_content.append(f\"Confidence Level: {self.config.confidence_level*100:.0f}%\")        report_content.append(\"-\" * 30)        report_content.append(\"üìä STATISTICAL SUMMARY\")        report_content.append(\"\")        # Add statistical summary                        report_content.append(line)                    line += f\" [CI: {ci_lower:.4f}-{ci_upper:.4f}]\"                if not np.isnan(ci_lower) and not np.isnan(ci_upper):                line = f\"{metric.upper():12}: {mean_val:.4f} ¬± {std_val:.4f}\"                                ci_upper = results['results'].get(ci_upper_key, np.nan)                ci_lower = results['results'].get(ci_lower_key, np.nan)                std_val = results['results'][std_key]                mean_val = results['results'][mean_key]            if mean_key in results['results']:                        ci_upper_key = f'{metric}_ci_upper'            ci_lower_key = f'{metric}_ci_lower'            std_key = f'{metric}_std'            mean_key = f'{metric}_mean'        for metric in core_metrics:                core_metrics = ['dice', 'iou', 'sensitivity', 'specificity', 'precision', 'f1_score']        # Core metrics\n",
    "        core_metrics = ['dice', 'iou', 'sensitivity', 'specificity', 'precision', 'f1_score']\n",
    "        \n",
    "        for metric in core_metrics:\n",
    "            mean_key = f'{metric}_mean'\n",
    "            std_key = f'{metric}_std'\n",
    "            ci_lower_key = f'{metric}_ci_lower'\n",
    "            ci_upper_key = f'{metric}_ci_upper'\n",
    "            \n",
    "            if mean_key in results['results']:\n",
    "                mean_val = results['results'][mean_key]\n",
    "                std_val = results['results'][std_key]\n",
    "                ci_lower = results['results'].get(ci_lower_key, np.nan)\n",
    "                ci_upper = results['results'].get(ci_upper_key, np.nan)\n",
    "                \n",
    "                report += f\"\\n{metric.upper():12}: {mean_val:.4f} ¬± {std_val:.4f}\"\n",
    "                if not np.isnan(ci_lower) and not np.isnan(ci_upper):\n",
    "                    report += f\" [CI: {ci_lower:.4f}-{ci_upper:.4f}]\"\n",
    "        \n",
    "        # Add statistical summary\n",
    "        report += f\"\"\"\n",
    "\n",
    "üìä STATISTICAL SUMMARY\n",
    "{'-'*30}\n",
    "Confidence Level: {self.config.confidence_level*100:.0f}%\n",
    "Bootstrap Samples: {self.config.bootstrap_samples}\n",
    "\n",
    "üéØ CLINICAL INTERPRETATION\n",
    "{'-'*30}\n",
    "\"\"\"\n",
    "        \n",
    "        # Clinical interpretation\n",
    "        dice_mean = results['results'].get('dice_mean', 0)\n",
    "        if dice_mean >= 0.9:\n",
    "            report += \"\\n‚úÖ EXCELLENT: Dice score ‚â• 0.90 - Clinical quality segmentation\"\n",
    "        elif dice_mean >= 0.8:\n",
    "            report += \"\\nüü¢ GOOD: Dice score ‚â• 0.80 - Acceptable for clinical use\"\n",
    "        elif dice_mean >= 0.7:\n",
    "            report += \"\\nüü° MODERATE: Dice score ‚â• 0.70 - May need improvement\"\n",
    "        else:\n",
    "            report += \"\\nüî¥ POOR: Dice score < 0.70 - Significant improvement needed\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "\n",
    "üìã DETAILED STATISTICS\n",
    "{'-'*30}\n",
    "\"\"\"\n",
    "        \n",
    "        # Detailed statistics table\n",
    "        for metric in core_metrics:\n",
    "            if f'{metric}_mean' in results['results']:\n",
    "                report += f\"\\n{metric.upper()}:\"\n",
    "                report += f\"\\n  Mean:     {results['results'][f'{metric}_mean']:.4f}\"\n",
    "                report += f\"\\n  Std:      {results['results'][f'{metric}_std']:.4f}\"\n",
    "                report += f\"\\n  Median:   {results['results'][f'{metric}_median']:.4f}\"\n",
    "                report += f\"\\n  Min:      {results['results'][f'{metric}_min']:.4f}\"\n",
    "                report += f\"\\n  Max:      {results['results'][f'{metric}_max']:.4f}\"\n",
    "                report += f\"\\n  Q25:      {results['results'][f'{metric}_q25']:.4f}\"\n",
    "                report += f\"\\n  Q75:      {results['results'][f'{metric}_q75']:.4f}\"\n",
    "                report += \"\\n\"\n",
    "        \n",
    "        report += f\"\\n{'='*60}\\n\"\n",
    "        \n",
    "        # Save report\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"üìÑ Evaluation report saved to: {save_path}\")\n",
    "        print(report)\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize quantitative evaluator\n",
    "quantitative_evaluator = QuantitativeEvaluator()\n",
    "\n",
    "print(\"‚úÖ Comprehensive Quantitative Evaluation Framework initialized!\")\n",
    "print(\"üî¢ Features:\")\n",
    "print(\"  - PyTorch model evaluation\")\n",
    "print(\"  - Per-sample metrics computation\")\n",
    "print(\"  - Statistical aggregation with confidence intervals\")\n",
    "print(\"  - Bootstrap confidence intervals\")\n",
    "print(\"  - Model comparison with effect sizes\")\n",
    "print(\"  - Clinical interpretation guidelines\")\n",
    "print(\"  - Comprehensive evaluation reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a29db0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1615496358.py, line 84)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmetrics_text = f\\\"Dice: {metrics.get('dice', 0):.3f}\\\\nIoU: {metrics.get('iou', 0):.3f}\\\"\\n                axes[idx, 3].text(10, 30, metrics_text, \\n                                 bbox=dict(boxstyle=\\\"round\\\", facecolor='white', alpha=0.8),\\n                                 fontsize=10, verticalalignment='top')\\n        \\n        plt.suptitle(title, fontsize=16, y=0.98)\\n        plt.tight_layout()\\n        \\n        if save_path:\\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\\n            print(f\\\"üìä Visualization saved to: {save_path}\\\")\\n        \\n        plt.show()\\n        return fig\\n    \\n    def visualize_error_analysis(self, images, ground_truth, predictions, \\n                               metrics_per_sample, save_path=None):\\n        \\\"\\\"\\\"\\n        Visualize error analysis with best and worst predictions\\n        \\\"\\\"\\\"\\n        # Calculate Dice scores for sorting\\n        dice_scores = [m.get('dice', 0) for m in metrics_per_sample]\\n        \\n        # Get best and worst samples\\n        sorted_indices = np.argsort(dice_scores)\\n        worst_indices = sorted_indices[:3]  # 3 worst\\n        best_indices = sorted_indices[-3:]  # 3 best\\n        \\n        fig, axes = plt.subplots(6, 4, figsize=(16, 24))\\n        \\n        # Visualize worst cases\\n        for idx, sample_idx in enumerate(worst_indices):\\n            self._plot_single_sample(axes[idx], images[sample_idx], \\n                                   ground_truth[sample_idx], \\n                                   predictions[sample_idx],\\n                                   metrics_per_sample[sample_idx],\\n                                   f\\\"Worst #{idx+1} (Dice: {dice_scores[sample_idx]:.3f})\\\")\\n        \\n        # Visualize best cases\\n        for idx, sample_idx in enumerate(best_indices):\\n            self._plot_single_sample(axes[idx+3], images[sample_idx], \\n                                   ground_truth[sample_idx], \\n                                   predictions[sample_idx],\\n                                   metrics_per_sample[sample_idx],\\n                                   f\\\"Best #{idx+1} (Dice: {dice_scores[sample_idx]:.3f})\\\")\\n        \\n        plt.suptitle('Error Analysis: Best vs Worst Predictions', fontsize=16)\\n        plt.tight_layout()\\n        \\n        if save_path:\\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\\n        \\n        plt.show()\\n        return fig, {'worst_indices': worst_indices, 'best_indices': best_indices}\\n    \\n    def _plot_single_sample(self, axes_row, image, gt, pred, metrics, title):\\n        \\\"\\\"\\\"\\n        Plot a single sample across 4 columns\\n        \\\"\\\"\\\"\\n        # Prepare displays\\n        display_img = np.squeeze(image)\\n        if display_img.max() > 1:\\n            display_img = display_img / 255.0\\n        \\n        gt_display = np.squeeze(gt)\\n        pred_display = np.squeeze(pred)\\n        \\n        # Original image\\n        axes_row[0].imshow(display_img, cmap='gray' if len(display_img.shape) == 2 else None)\\n        axes_row[0].set_title(title)\\n        axes_row[0].axis('off')\\n        \\n        # Ground truth\\n        axes_row[1].imshow(gt_display, cmap='jet')\\n        axes_row[1].set_title('Ground Truth')\\n        axes_row[1].axis('off')\\n        \\n        # Prediction\\n        axes_row[2].imshow(pred_display, cmap='jet')\\n        axes_row[2].set_title('Prediction')\\n        axes_row[2].axis('off')\\n        \\n        # Error map\\n        error_map = np.abs(gt_display - pred_display)\\n        im = axes_row[3].imshow(error_map, cmap='hot')\\n        axes_row[3].set_title('Error Map')\\n        axes_row[3].axis('off')\\n        \\n        # Add colorbar for error map\\n        plt.colorbar(im, ax=axes_row[3], fraction=0.046, pad=0.04)\\n    \\n    def create_metrics_distribution_plot(self, metrics_per_sample, save_path=None):\\n        \\\"\\\"\\\"\\n        Create distribution plots for all metrics\\n        \\\"\\\"\\\"\\n        # Get metric names\\n        metric_names = list(metrics_per_sample[0].keys())\\n        n_metrics = len(metric_names)\\n        \\n        # Calculate grid size\\n        n_cols = 3\\n        n_rows = (n_metrics + n_cols - 1) // n_cols\\n        \\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\\n        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\\n        \\n        for idx, metric_name in enumerate(metric_names):\\n            values = [m[metric_name] for m in metrics_per_sample \\n                     if not np.isnan(m[metric_name]) and not np.isinf(m[metric_name])]\\n            \\n            if values:\\n                # Histogram\\n                axes[idx].hist(values, bins=30, alpha=0.7, edgecolor='black')\\n                axes[idx].axvline(np.mean(values), color='red', linestyle='--', \\n                                linewidth=2, label=f'Mean: {np.mean(values):.3f}')\\n                axes[idx].axvline(np.median(values), color='green', linestyle='--', \\n                                linewidth=2, label=f'Median: {np.median(values):.3f}')\\n                \\n                axes[idx].set_title(f'{metric_name.title()} Distribution')\\n                axes[idx].set_xlabel(metric_name.title())\\n                axes[idx].set_ylabel('Frequency')\\n                axes[idx].legend()\\n                axes[idx].grid(True, alpha=0.3)\\n        \\n        # Remove empty subplots\\n        for idx in range(n_metrics, len(axes)):\\n            fig.delaxes(axes[idx])\\n        \\n        plt.suptitle('Metrics Distribution Analysis', fontsize=16)\\n        plt.tight_layout()\\n        \\n        if save_path:\\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\\n        \\n        plt.show()\\n        return fig\\n    \\n    def create_correlation_analysis(self, metrics_per_sample, save_path=None):\\n        \\\"\\\"\\\"\\n        Create correlation analysis between different metrics\\n        \\\"\\\"\\\"\\n        # Convert to DataFrame for easier manipulation\\n        df = pd.DataFrame(metrics_per_sample)\\n        \\n        # Remove any infinite or NaN values\\n        df = df.replace([np.inf, -np.inf], np.nan).dropna()\\n        \\n        if df.empty:\\n            print(\\\"‚ö†Ô∏è No valid data for correlation analysis\\\")\\n            return None\\n        \\n        # Create correlation matrix\\n        correlation_matrix = df.corr()\\n        \\n        # Create heatmap\\n        fig, ax = plt.subplots(figsize=(10, 8))\\n        \\n        # Create heatmap\\n        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\\n                   square=True, fmt='.3f', cbar_kws={'shrink': 0.8})\\n        \\n        plt.title('Metrics Correlation Analysis', fontsize=16)\\n        plt.tight_layout()\\n        \\n        if save_path:\\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\\n        \\n        plt.show()\\n        return fig, correlation_matrix\\n    \\n    def _select_diverse_samples(self, ground_truth, predictions, n_samples=10):\\n        \\\"\\\"\\\"\\n        Select diverse samples for visualization based on prediction quality\\n        \\\"\\\"\\\"\\n        # Calculate simple metric for diversity selection\\n        dice_scores = []\\n        for gt, pred in zip(ground_truth, predictions):\\n            gt_flat = gt.flatten()\\n            pred_flat = pred.flatten()\\n            intersection = np.sum(gt_flat * pred_flat)\\n            dice = (2.0 * intersection + 1e-6) / (np.sum(gt_flat) + np.sum(pred_flat) + 1e-6)\\n            dice_scores.append(dice)\\n        \\n        dice_scores = np.array(dice_scores)\\n        \\n        # Select samples from different quality ranges\\n        indices = []\\n        \\n        # High quality (top 20%)\\n        high_threshold = np.percentile(dice_scores, 80)\\n        high_indices = np.where(dice_scores >= high_threshold)[0]\\n        if len(high_indices) > 0:\\n            indices.extend(np.random.choice(high_indices, min(n_samples//3, len(high_indices)), replace=False))\\n        \\n        # Medium quality (middle 60%)\\n        med_low = np.percentile(dice_scores, 20)\\n        med_high = np.percentile(dice_scores, 80)\\n        med_indices = np.where((dice_scores >= med_low) & (dice_scores < med_high))[0]\\n        if len(med_indices) > 0:\\n            indices.extend(np.random.choice(med_indices, min(n_samples//3, len(med_indices)), replace=False))\\n        \\n        # Low quality (bottom 20%)\\n        low_threshold = np.percentile(dice_scores, 20)\\n        low_indices = np.where(dice_scores < low_threshold)[0]\\n        if len(low_indices) > 0:\\n            indices.extend(np.random.choice(low_indices, min(n_samples//3, len(low_indices)), replace=False))\\n        \\n        # Fill remaining with random samples\\n        remaining = n_samples - len(indices)\\n        if remaining > 0:\\n            available = list(set(range(len(dice_scores))) - set(indices))\\n            if available:\\n                additional = np.random.choice(available, min(remaining, len(available)), replace=False)\\n                indices.extend(additional)\\n        \\n        return sorted(indices[:n_samples])\\n\\n# Initialize qualitative evaluator\\nqualitative_evaluator = QualitativeEvaluator()\\n\\nprint(\\\"‚úÖ Advanced Visualization Framework initialized!\\\")\\nprint(\\\"üìä Visualization features:\\\")\\nprint(\\\"  - Comprehensive prediction grids\\\")\\nprint(\\\"  - Error analysis with best/worst cases\\\")\\nprint(\\\"  - Metrics distribution analysis\\\")\\nprint(\\\"  - Correlation analysis between metrics\\\")\\nprint(\\\"  - Intelligent sample selection for diversity\\\")\\nprint(\\\"  - Publication-ready figure generation\\\")\u001b[39m\n                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Advanced Visualization Framework for Qualitative Evaluation\n",
    "class QualitativeEvaluator:\n",
    "    \"\"\"\n",
    "    Advanced visualization framework for qualitative assessment of segmentation results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or eval_config\n",
    "        \n",
    "    def visualize_predictions_grid(self, images, ground_truth, predictions, \n",
    "                                 indices=None, title=\"Model Predictions\", save_path=None):\n",
    "        \"\"\"\n",
    "        Create a comprehensive grid visualization of predictions\n",
    "        \"\"\"\n",
    "        if indices is None:\n",
    "            # Select diverse samples for visualization\n",
    "            indices = self._select_diverse_samples(ground_truth, predictions, \n",
    "                                                  n_samples=self.config.num_samples_to_show)\n",
    "        \n",
    "        n_samples = len(indices)\n",
    "        fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4*n_samples))\n",
    "        \n",
    "        if n_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for idx, sample_idx in enumerate(indices):\n",
    "            img = images[sample_idx]\n",
    "            gt = ground_truth[sample_idx]\n",
    "            pred = predictions[sample_idx]\n",
    "            \n",
    "            # Prepare images for display\n",
    "            if img.shape[-1] == 3:\n",
    "                display_img = img\n",
    "            else:\n",
    "                display_img = np.squeeze(img)\n",
    "            \n",
    "            gt_display = np.squeeze(gt)\n",
    "            pred_display = np.squeeze(pred)\n",
    "            \n",
    "            # Normalize images\n",
    "            if display_img.max() > 1:\n",
    "                display_img = display_img / 255.0\n",
    "            \n",
    "            # Original image\n",
    "            axes[idx, 0].imshow(display_img, cmap='gray' if len(display_img.shape) == 2 else None)\n",
    "            axes[idx, 0].set_title(f'Original Image {sample_idx}')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            # Ground truth\n",
    "            axes[idx, 1].imshow(gt_display, cmap='jet', alpha=0.8)\n",
    "            axes[idx, 1].set_title('Ground Truth')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            # Prediction\n",
    "            axes[idx, 2].imshow(pred_display, cmap='jet', alpha=0.8)\n",
    "            axes[idx, 2].set_title('Prediction')\n",
    "            axes[idx, 2].axis('off')\n",
    "            \n",
    "            # Overlay comparison\n",
    "            axes[idx, 3].imshow(display_img, cmap='gray')\n",
    "            \n",
    "            # Create overlay with different colors for GT and Prediction\n",
    "            overlay = np.zeros((*gt_display.shape, 3))\n",
    "            \n",
    "            # Ground truth in green\n",
    "            gt_binary = gt_display > 0.5\n",
    "            overlay[gt_binary] = [0, 1, 0]  # Green\n",
    "            \n",
    "            # Prediction in red\n",
    "            pred_binary = pred_display > 0.5\n",
    "            overlay[pred_binary] = [1, 0, 0]  # Red\n",
    "            \n",
    "            # Overlap in yellow\n",
    "            overlap = gt_binary & pred_binary\n",
    "            overlay[overlap] = [1, 1, 0]  # Yellow\n",
    "            \n",
    "            axes[idx, 3].imshow(overlay, alpha=0.4)\n",
    "            axes[idx, 3].set_title('Overlay (GT: Green, Pred: Red, Overlap: Yellow)')\n",
    "            axes[idx, 3].axis('off')\n",
    "            \n",
    "            # Add metrics as text\n",
    "            if hasattr(self, '_sample_metrics') and sample_idx < len(self._sample_metrics):\n",
    "                metrics = self._sample_metrics[sample_idx]\n",
    "                metrics_text = f\\\"Dice: {metrics.get('dice', 0):.3f}\\\\nIoU: {metrics.get('iou', 0):.3f}\\\"\\n                axes[idx, 3].text(10, 30, metrics_text, \\n                                 bbox=dict(boxstyle=\\\"round\\\", facecolor='white', alpha=0.8),\\n                                 fontsize=10, verticalalignment='top')\\n        \\n        plt.suptitle(title, fontsize=16, y=0.98)\\n        plt.tight_layout()\\n        \\n        if save_path:\\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\\n            print(f\\\"üìä Visualization saved to: {save_path}\\\")\\n        \\n        plt.show()\\n        return fig\\n    \\n    def visualize_error_analysis(self, images, ground_truth, predictions, \\n                               metrics_per_sample, save_path=None):\\n        \\\"\\\"\\\"\\n        Visualize error analysis with best and worst predictions\\n        \\\"\\\"\\\"\\n        # Calculate Dice scores for sorting\\n        dice_scores = [m.get('dice', 0) for m in metrics_per_sample]\\n        \\n        # Get best and worst samples\\n        sorted_indices = np.argsort(dice_scores)\\n        worst_indices = sorted_indices[:3]  # 3 worst\\n        best_indices = sorted_indices[-3:]  # 3 best\\n        \\n        fig, axes = plt.subplots(6, 4, figsize=(16, 24))\\n        \\n        # Visualize worst cases\\n        for idx, sample_idx in enumerate(worst_indices):\\n            self._plot_single_sample(axes[idx], images[sample_idx], \\n                                   ground_truth[sample_idx], \\n                                   predictions[sample_idx],\\n                                   metrics_per_sample[sample_idx],\\n                                   f\\\"Worst #{idx+1} (Dice: {dice_scores[sample_idx]:.3f})\\\")\\n        \\n        # Visualize best cases\\n        for idx, sample_idx in enumerate(best_indices):\\n            self._plot_single_sample(axes[idx+3], images[sample_idx], \\n                                   ground_truth[sample_idx], \\n                                   predictions[sample_idx],\\n                                   metrics_per_sample[sample_idx],\\n                                   f\\\"Best #{idx+1} (Dice: {dice_scores[sample_idx]:.3f})\\\")\\n        \\n        plt.suptitle('Error Analysis: Best vs Worst Predictions', fontsize=16)\\n        plt.tight_layout()\\n        \\n        if save_path:\\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\\n        \\n        plt.show()\\n        return fig, {'worst_indices': worst_indices, 'best_indices': best_indices}\\n    \\n    def _plot_single_sample(self, axes_row, image, gt, pred, metrics, title):\\n        \\\"\\\"\\\"\\n        Plot a single sample across 4 columns\\n        \\\"\\\"\\\"\\n        # Prepare displays\\n        display_img = np.squeeze(image)\\n        if display_img.max() > 1:\\n            display_img = display_img / 255.0\\n        \\n        gt_display = np.squeeze(gt)\\n        pred_display = np.squeeze(pred)\\n        \\n        # Original image\\n        axes_row[0].imshow(display_img, cmap='gray' if len(display_img.shape) == 2 else None)\\n        axes_row[0].set_title(title)\\n        axes_row[0].axis('off')\\n        \\n        # Ground truth\\n        axes_row[1].imshow(gt_display, cmap='jet')\\n        axes_row[1].set_title('Ground Truth')\\n        axes_row[1].axis('off')\\n        \\n        # Prediction\\n        axes_row[2].imshow(pred_display, cmap='jet')\\n        axes_row[2].set_title('Prediction')\\n        axes_row[2].axis('off')\\n        \\n        # Error map\\n        error_map = np.abs(gt_display - pred_display)\\n        im = axes_row[3].imshow(error_map, cmap='hot')\\n        axes_row[3].set_title('Error Map')\\n        axes_row[3].axis('off')\\n        \\n        # Add colorbar for error map\\n        plt.colorbar(im, ax=axes_row[3], fraction=0.046, pad=0.04)\\n    \\n    def create_metrics_distribution_plot(self, metrics_per_sample, save_path=None):\\n        \\\"\\\"\\\"\\n        Create distribution plots for all metrics\\n        \\\"\\\"\\\"\\n        # Get metric names\\n        metric_names = list(metrics_per_sample[0].keys())\\n        n_metrics = len(metric_names)\\n        \\n        # Calculate grid size\\n        n_cols = 3\\n        n_rows = (n_metrics + n_cols - 1) // n_cols\\n        \\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\\n        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\\n        \\n        for idx, metric_name in enumerate(metric_names):\\n            values = [m[metric_name] for m in metrics_per_sample \\n                     if not np.isnan(m[metric_name]) and not np.isinf(m[metric_name])]\\n            \\n            if values:\\n                # Histogram\\n                axes[idx].hist(values, bins=30, alpha=0.7, edgecolor='black')\\n                axes[idx].axvline(np.mean(values), color='red', linestyle='--', \\n                                linewidth=2, label=f'Mean: {np.mean(values):.3f}')\\n                axes[idx].axvline(np.median(values), color='green', linestyle='--', \\n                                linewidth=2, label=f'Median: {np.median(values):.3f}')\\n                \\n                axes[idx].set_title(f'{metric_name.title()} Distribution')\\n                axes[idx].set_xlabel(metric_name.title())\\n                axes[idx].set_ylabel('Frequency')\\n                axes[idx].legend()\\n                axes[idx].grid(True, alpha=0.3)\\n        \\n        # Remove empty subplots\\n        for idx in range(n_metrics, len(axes)):\\n            fig.delaxes(axes[idx])\\n        \\n        plt.suptitle('Metrics Distribution Analysis', fontsize=16)\\n        plt.tight_layout()\\n        \\n        if save_path:\\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\\n        \\n        plt.show()\\n        return fig\\n    \\n    def create_correlation_analysis(self, metrics_per_sample, save_path=None):\\n        \\\"\\\"\\\"\\n        Create correlation analysis between different metrics\\n        \\\"\\\"\\\"\\n        # Convert to DataFrame for easier manipulation\\n        df = pd.DataFrame(metrics_per_sample)\\n        \\n        # Remove any infinite or NaN values\\n        df = df.replace([np.inf, -np.inf], np.nan).dropna()\\n        \\n        if df.empty:\\n            print(\\\"‚ö†Ô∏è No valid data for correlation analysis\\\")\\n            return None\\n        \\n        # Create correlation matrix\\n        correlation_matrix = df.corr()\\n        \\n        # Create heatmap\\n        fig, ax = plt.subplots(figsize=(10, 8))\\n        \\n        # Create heatmap\\n        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\\n                   square=True, fmt='.3f', cbar_kws={'shrink': 0.8})\\n        \\n        plt.title('Metrics Correlation Analysis', fontsize=16)\\n        plt.tight_layout()\\n        \\n        if save_path:\\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\\n        \\n        plt.show()\\n        return fig, correlation_matrix\\n    \\n    def _select_diverse_samples(self, ground_truth, predictions, n_samples=10):\\n        \\\"\\\"\\\"\\n        Select diverse samples for visualization based on prediction quality\\n        \\\"\\\"\\\"\\n        # Calculate simple metric for diversity selection\\n        dice_scores = []\\n        for gt, pred in zip(ground_truth, predictions):\\n            gt_flat = gt.flatten()\\n            pred_flat = pred.flatten()\\n            intersection = np.sum(gt_flat * pred_flat)\\n            dice = (2.0 * intersection + 1e-6) / (np.sum(gt_flat) + np.sum(pred_flat) + 1e-6)\\n            dice_scores.append(dice)\\n        \\n        dice_scores = np.array(dice_scores)\\n        \\n        # Select samples from different quality ranges\\n        indices = []\\n        \\n        # High quality (top 20%)\\n        high_threshold = np.percentile(dice_scores, 80)\\n        high_indices = np.where(dice_scores >= high_threshold)[0]\\n        if len(high_indices) > 0:\\n            indices.extend(np.random.choice(high_indices, min(n_samples//3, len(high_indices)), replace=False))\\n        \\n        # Medium quality (middle 60%)\\n        med_low = np.percentile(dice_scores, 20)\\n        med_high = np.percentile(dice_scores, 80)\\n        med_indices = np.where((dice_scores >= med_low) & (dice_scores < med_high))[0]\\n        if len(med_indices) > 0:\\n            indices.extend(np.random.choice(med_indices, min(n_samples//3, len(med_indices)), replace=False))\\n        \\n        # Low quality (bottom 20%)\\n        low_threshold = np.percentile(dice_scores, 20)\\n        low_indices = np.where(dice_scores < low_threshold)[0]\\n        if len(low_indices) > 0:\\n            indices.extend(np.random.choice(low_indices, min(n_samples//3, len(low_indices)), replace=False))\\n        \\n        # Fill remaining with random samples\\n        remaining = n_samples - len(indices)\\n        if remaining > 0:\\n            available = list(set(range(len(dice_scores))) - set(indices))\\n            if available:\\n                additional = np.random.choice(available, min(remaining, len(available)), replace=False)\\n                indices.extend(additional)\\n        \\n        return sorted(indices[:n_samples])\\n\\n# Initialize qualitative evaluator\\nqualitative_evaluator = QualitativeEvaluator()\\n\\nprint(\\\"‚úÖ Advanced Visualization Framework initialized!\\\")\\nprint(\\\"üìä Visualization features:\\\")\\nprint(\\\"  - Comprehensive prediction grids\\\")\\nprint(\\\"  - Error analysis with best/worst cases\\\")\\nprint(\\\"  - Metrics distribution analysis\\\")\\nprint(\\\"  - Correlation analysis between metrics\\\")\\nprint(\\\"  - Intelligent sample selection for diversity\\\")\\nprint(\\\"  - Publication-ready figure generation\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a79aa83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QuantitativeEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 247\u001b[39m\n\u001b[32m    244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Initialize comprehensive evaluation pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m evaluation_pipeline = \u001b[43mComprehensiveEvaluationPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Comprehensive Evaluation Pipeline initialized!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    250\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müéØ Pipeline features:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mComprehensiveEvaluationPipeline.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mself\u001b[39m.config = config \u001b[38;5;129;01mor\u001b[39;00m eval_config\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28mself\u001b[39m.quantitative_evaluator = \u001b[43mQuantitativeEvaluator\u001b[49m(config)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mself\u001b[39m.qualitative_evaluator = QualitativeEvaluator(config)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_manager = ModelManager()\n",
      "\u001b[31mNameError\u001b[39m: name 'QuantitativeEvaluator' is not defined"
     ]
    }
   ],
   "source": [
    "# Comprehensive Evaluation Pipeline (PyTorch)\n",
    "class ComprehensiveEvaluationPipeline:\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline combining quantitative and qualitative assessment for PyTorch models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or eval_config\n",
    "        self.quantitative_evaluator = QuantitativeEvaluator(config)\n",
    "        self.qualitative_evaluator = QualitativeEvaluator(config)\n",
    "        self.model_manager = ModelManager()\n",
    "        \n",
    "        # Create evaluation results directory\n",
    "        self.results_dir = self.config.results_dir\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def evaluate_model_comprehensive(self, model, test_dataset, model_name=None):\n",
    "        \"\"\"\n",
    "        Run comprehensive evaluation on a PyTorch model\n",
    "        \"\"\"\n",
    "        if model_name is None:\n",
    "            model_name = getattr(model, '__class__', type(model)).__name__\n",
    "        \n",
    "        print(f\"üéØ Starting comprehensive evaluation for: {model_name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Quantitative Evaluation\n",
    "        print(\"üìä Step 1: Quantitative Evaluation\")\n",
    "        quantitative_results = self.quantitative_evaluator.evaluate_model_on_dataset(\n",
    "            model, test_dataset, \"test\")\n",
    "        \n",
    "        # Step 2: Generate Quantitative Report\n",
    "        print(\"üìã Step 2: Generating Quantitative Report\")\n",
    "        report = self.quantitative_evaluator.generate_evaluation_report(\n",
    "            quantitative_results, \n",
    "            self.results_dir / f\"{model_name}_quantitative_report.txt\")\n",
    "        \n",
    "        # Step 3: Qualitative Visualizations\n",
    "        print(\"üé® Step 3: Creating Qualitative Visualizations\")\n",
    "        \n",
    "        # Extract data for visualization\n",
    "        images = quantitative_results['images']\n",
    "        ground_truth = quantitative_results['ground_truth']\n",
    "        predictions = quantitative_results['predictions']\n",
    "        \n",
    "        # Compute per-sample metrics for visualization\n",
    "        sample_metrics = []\n",
    "        for gt, pred in zip(ground_truth, predictions):\n",
    "            metrics = self.quantitative_evaluator.metrics_calculator.compute_metrics_numpy(gt, pred)\n",
    "            sample_metrics.append(metrics)\n",
    "        \n",
    "        # Store sample metrics for visualization\n",
    "        self.qualitative_evaluator._sample_metrics = sample_metrics\n",
    "        \n",
    "        # Create visualizations\n",
    "        viz_dir = self.results_dir / f\"{model_name}_visualizations\"\n",
    "        viz_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Prediction grid\n",
    "        self.qualitative_evaluator.visualize_predictions_grid(\n",
    "            images, ground_truth, predictions,\n",
    "            title=f\"{model_name} - Prediction Examples\",\n",
    "            save_path=viz_dir / \"prediction_grid.png\")\n",
    "        \n",
    "        # Error analysis\n",
    "        self.qualitative_evaluator.visualize_error_analysis(\n",
    "            images, ground_truth, predictions, sample_metrics,\n",
    "            save_path=viz_dir / \"error_analysis.png\")\n",
    "        \n",
    "        # Metrics distribution\n",
    "        self.qualitative_evaluator.create_metrics_distribution_plot(\n",
    "            sample_metrics,\n",
    "            save_path=viz_dir / \"metrics_distribution.png\")\n",
    "        \n",
    "        # Correlation analysis\n",
    "        self.qualitative_evaluator.create_correlation_analysis(\n",
    "            sample_metrics,\n",
    "            save_path=viz_dir / \"metrics_correlation.png\")\n",
    "        \n",
    "        # Step 4: Save Complete Results\n",
    "        print(\"üíæ Step 4: Saving Complete Results\")\n",
    "        \n",
    "        # Prepare results for saving (remove large arrays)\n",
    "        results_for_saving = {\n",
    "            'model_name': model_name,\n",
    "            'evaluation_timestamp': quantitative_results['timestamp'],\n",
    "            'dataset_info': {\n",
    "                'dataset_name': quantitative_results['dataset_name'],\n",
    "                'num_samples': quantitative_results['num_samples']\n",
    "            },\n",
    "            'quantitative_results': quantitative_results['results'],\n",
    "            'sample_metrics_summary': {\n",
    "                'total_samples': len(sample_metrics),\n",
    "                'metrics_computed': list(sample_metrics[0].keys()) if sample_metrics else []\n",
    "            },\n",
    "            'visualization_paths': {\n",
    "                'prediction_grid': str(viz_dir / \"prediction_grid.png\"),\n",
    "                'error_analysis': str(viz_dir / \"error_analysis.png\"),\n",
    "                'metrics_distribution': str(viz_dir / \"metrics_distribution.png\"),\n",
    "                'metrics_correlation': str(viz_dir / \"metrics_correlation.png\")\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save results JSON\n",
    "        results_path = self.results_dir / f\"{model_name}_complete_results.json\"\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(results_for_saving, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"‚úÖ Complete evaluation finished for {model_name}!\")\n",
    "        print(f\"üìÅ Results saved to: {self.results_dir}\")\n",
    "        print(f\"üìä Quantitative report: {self.results_dir / f'{model_name}_quantitative_report.txt'}\")\n",
    "        print(f\"üé® Visualizations: {viz_dir}\")\n",
    "        print(f\"üìã Complete results: {results_path}\")\n",
    "        \n",
    "        return results_for_saving\n",
    "    \n",
    "    def create_synthetic_test_data(self, num_samples=50, image_size=(256, 256)):\n",
    "        \"\"\"\n",
    "        Create synthetic test data for demonstration purposes\n",
    "        \"\"\"\n",
    "        print(f\"üîß Creating synthetic test data: {num_samples} samples of size {image_size}\")\n",
    "        \n",
    "        images = []\n",
    "        masks = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Create synthetic cardiac image\n",
    "            img = np.random.rand(*image_size, 3) * 0.3 + 0.2  # Base tissue\n",
    "            \n",
    "            # Add heart-like structures\n",
    "            center_x, center_y = image_size[0] // 2, image_size[1] // 2\n",
    "            \n",
    "            # Left ventricle (circular)\n",
    "            lv_radius = np.random.randint(20, 40)\n",
    "            lv_center_x = center_x + np.random.randint(-10, 10)\n",
    "            lv_center_y = center_y + np.random.randint(-10, 10)\n",
    "            \n",
    "            # Myocardium (ring around LV)\n",
    "            myo_thickness = np.random.randint(8, 15)\n",
    "            \n",
    "            # Create coordinate grids\n",
    "            y, x = np.ogrid[:image_size[0], :image_size[1]]\n",
    "            \n",
    "            # Left ventricle mask\n",
    "            lv_mask = (x - lv_center_x)**2 + (y - lv_center_y)**2 <= lv_radius**2\n",
    "            \n",
    "            # Myocardium mask\n",
    "            myo_mask = ((x - lv_center_x)**2 + (y - lv_center_y)**2 <= (lv_radius + myo_thickness)**2) & (~lv_mask)\n",
    "            \n",
    "            # Combined mask (binary: 0=background, 1=heart)\n",
    "            mask = (lv_mask | myo_mask).astype(np.float32)\n",
    "            \n",
    "            # Add heart structures to image\n",
    "            img[lv_mask] = [0.1, 0.1, 0.1]  # Dark blood pool\n",
    "            img[myo_mask] = [0.6, 0.4, 0.4]  # Myocardium\n",
    "            \n",
    "            # Add noise\n",
    "            img += np.random.normal(0, 0.05, img.shape)\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            images.append(img)\n",
    "            masks.append(mask.reshape(*mask.shape, 1))\n",
    "        \n",
    "        return np.array(images), np.array(masks)\n",
    "    \n",
    "    def create_pytorch_dataloader(self, images, masks, batch_size=None):\n",
    "        \"\"\"\n",
    "        Create a PyTorch DataLoader from numpy arrays\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.batch_size\n",
    "        \n",
    "        # Create custom dataset\n",
    "        class SyntheticDataset:\n",
    "            def __init__(self, images, masks):\n",
    "                self.images = images\n",
    "                self.masks = masks\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.images)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                return self.images[idx], self.masks[idx]\n",
    "        \n",
    "        dataset = SyntheticDataset(images, masks)\n",
    "        \n",
    "        # Create simple iterable that yields batches\n",
    "        class SimpleDataLoader:\n",
    "            def __init__(self, dataset, batch_size):\n",
    "                self.dataset = dataset\n",
    "                self.batch_size = batch_size\n",
    "            \n",
    "            def __iter__(self):\n",
    "                for i in range(0, len(self.dataset), self.batch_size):\n",
    "                    batch_images = []\n",
    "                    batch_masks = []\n",
    "                    \n",
    "                    for j in range(i, min(i + self.batch_size, len(self.dataset))):\n",
    "                        img, mask = self.dataset[j]\n",
    "                        batch_images.append(img)\n",
    "                        batch_masks.append(mask)\n",
    "                    \n",
    "                    yield np.array(batch_images), np.array(batch_masks)\n",
    "        \n",
    "        return SimpleDataLoader(dataset, batch_size)\n",
    "    \n",
    "    def demonstrate_evaluation_pipeline(self):\n",
    "        \"\"\"\n",
    "        Demonstrate the complete evaluation pipeline with synthetic data\n",
    "        \"\"\"\n",
    "        print(\"üé≠ DEMONSTRATION: Complete Evaluation Pipeline\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Check if we have any loaded models\n",
    "        if not model_manager.loaded_models:\n",
    "            print(\"‚ö†Ô∏è No models loaded. Creating dummy model for demonstration...\")\n",
    "            model = model_manager.create_dummy_model()\n",
    "            model_name = \"dummy_model\"\n",
    "        else:\n",
    "            # Use the first available model\n",
    "            model_name = list(model_manager.loaded_models.keys())[0]\n",
    "            model = model_manager.loaded_models[model_name]\n",
    "            print(f\"üì± Using loaded model: {model_name}\")\n",
    "        \n",
    "        # Create synthetic test data\n",
    "        test_images, test_masks = self.create_synthetic_test_data(num_samples=20)\n",
    "        \n",
    "        # Create PyTorch-compatible dataset\n",
    "        test_dataset = self.create_pytorch_dataloader(test_images, test_masks, self.config.batch_size)\n",
    "        \n",
    "        # Run comprehensive evaluation\n",
    "        results = self.evaluate_model_comprehensive(model, test_dataset, model_name)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nüìà EVALUATION SUMMARY\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        quant_results = results['quantitative_results']\n",
    "        print(f\"Dice Score: {quant_results.get('dice_mean', 0):.4f} ¬± {quant_results.get('dice_std', 0):.4f}\")\n",
    "        print(f\"IoU Score:  {quant_results.get('iou_mean', 0):.4f} ¬± {quant_results.get('iou_std', 0):.4f}\")\n",
    "        print(f\"Sensitivity: {quant_results.get('sensitivity_mean', 0):.4f} ¬± {quant_results.get('sensitivity_std', 0):.4f}\")\n",
    "        print(f\"Specificity: {quant_results.get('specificity_mean', 0):.4f} ¬± {quant_results.get('specificity_std', 0):.4f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize comprehensive evaluation pipeline\n",
    "evaluation_pipeline = ComprehensiveEvaluationPipeline()\n",
    "\n",
    "print(\"‚úÖ Comprehensive Evaluation Pipeline initialized!\")\n",
    "print(\"üéØ Pipeline features:\")\n",
    "print(\"  - Complete quantitative evaluation for PyTorch models\")\n",
    "print(\"  - Advanced qualitative visualizations\")\n",
    "print(\"  - Statistical analysis and reporting\")\n",
    "print(\"  - Automated result saving and organization\")\n",
    "print(\"  - Demonstration with synthetic data\")\n",
    "print(\"  - PyTorch-compatible data handling\")\n",
    "print(\"\\nüöÄ Ready to evaluate PyTorch models!\")\n",
    "print(\"\\nüí° To run a demonstration:\")\n",
    "print(\"   evaluation_pipeline.demonstrate_evaluation_pipeline()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce8214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration and Testing\n",
    "print(\\\"üé≠ DEMONSTRATION: Model Evaluation Pipeline\\\")\\nprint(\\\"=\\\" * 60)\\n\\n# Run the demonstration\\nprint(\\\"‚ñ∂Ô∏è Running evaluation pipeline demonstration...\\\")\\nprint(\\\"This will:\\\")\\nprint(\\\"  1. Create or load a model\\\")\\nprint(\\\"  2. Generate synthetic test data\\\")\\nprint(\\\"  3. Run comprehensive evaluation\\\")\\nprint(\\\"  4. Create visualizations\\\")\\nprint(\\\"  5. Generate reports\\\")\\nprint(\\\"\\\\n‚è≥ Please wait...\\\")\\n\\n# Execute demonstration\\ntry:\\n    demo_results = evaluation_pipeline.demonstrate_evaluation_pipeline()\\n    \\n    print(\\\"\\\\nüéâ DEMONSTRATION COMPLETED SUCCESSFULLY!\\\")\\n    print(\\\"‚úÖ All evaluation components working correctly\\\")\\n    \\nexcept Exception as e:\\n    print(f\\\"‚ùå Demonstration failed: {e}\\\")\\n    print(\\\"This is normal if running without trained models\\\")\\n    print(\\\"Train models using previous notebooks for full functionality\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea464da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Examples and Advanced Techniques\\n\\nprint(\\\"üìö USAGE EXAMPLES\\\")\\nprint(\\\"=\\\" * 40)\\n\\n# Example 1: Loading and evaluating a specific model\\nprint(\\\"\\\\n1Ô∏è‚É£ Example: Loading and Evaluating a Specific Model\\\")\\nprint(\\\"\\\"\\\"# Load a trained model\\nmodel = model_manager.load_model('path/to/your/model.h5')\\n\\n# Load test dataset (from previous notebooks)\\n# test_dataset = ... your test dataset ...\\n\\n# Run comprehensive evaluation\\nresults = evaluation_pipeline.evaluate_model_comprehensive(model, test_dataset, 'my_model')\\n\\\"\\\"\\\")\\n\\n# Example 2: Comparing multiple models\\nprint(\\\"\\\\n2Ô∏è‚É£ Example: Comparing Multiple Models\\\")\\nprint(\\\"\\\"\\\"# Load multiple models\\nmodel1 = model_manager.load_model('model1.h5')\\nmodel2 = model_manager.load_model('model2.h5')\\n\\n# Evaluate each model\\nresults1 = evaluation_pipeline.evaluate_model_comprehensive(model1, test_dataset, 'model1')\\nresults2 = evaluation_pipeline.evaluate_model_comprehensive(model2, test_dataset, 'model2')\\n\\n# Compare results\\ncomparison = quantitative_evaluator.compare_models([results1, results2], 'dice_mean')\\nprint(comparison)\\n\\\"\\\"\\\")\\n\\n# Example 3: Custom evaluation with specific metrics\\nprint(\\\"\\\\n3Ô∏è‚É£ Example: Custom Evaluation Focus\\\")\\nprint(\\\"\\\"\\\"# Focus on specific aspects\\n# For boundary accuracy\\nresults = evaluation_pipeline.evaluate_model_comprehensive(model, test_dataset)\\n\\n# Extract boundary-specific insights from visualizations\\n# Error analysis will show where boundary predictions fail\\n\\\"\\\"\\\")\\n\\nprint(\\\"\\\\nüî¨ ADVANCED EVALUATION TECHNIQUES\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# Advanced technique 1: Per-class evaluation\\nprint(\\\"\\\\nüéØ Advanced Technique 1: Per-Class Analysis\\\")\\nprint(\\\"For multi-class cardiac segmentation (LV, Myocardium, etc.):\\\")\\nprint(\\\"- Modify metrics_calculator to handle multi-class\\\")\\nprint(\\\"- Compute class-specific Dice scores\\\")\\nprint(\\\"- Analyze class imbalance effects\\\")\\n\\n# Advanced technique 2: Clinical relevance\\nprint(\\\"\\\\nüè• Advanced Technique 2: Clinical Relevance Assessment\\\")\\nprint(\\\"Clinical interpretation guidelines:\\\")\\nprint(\\\"- Dice > 0.90: Excellent (clinical gold standard)\\\")\\nprint(\\\"- Dice 0.80-0.90: Good (acceptable for many applications)\\\")\\nprint(\\\"- Dice 0.70-0.80: Moderate (may need improvement)\\\")\\nprint(\\\"- Dice < 0.70: Poor (significant improvement needed)\\\")\\n\\n# Advanced technique 3: Error pattern analysis\\nprint(\\\"\\\\nüîç Advanced Technique 3: Error Pattern Analysis\\\")\\nprint(\\\"Systematic error investigation:\\\")\\nprint(\\\"- Analyze failure modes (over-segmentation vs under-segmentation)\\\")\\nprint(\\\"- Correlate errors with image characteristics\\\")\\nprint(\\\"- Identify challenging anatomical regions\\\")\\n\\nprint(\\\"\\\\n‚ö° PERFORMANCE OPTIMIZATION TIPS\\\")\\nprint(\\\"=\\\" * 40)\\nprint(\\\"For large datasets:\\\")\\nprint(\\\"- Use batch evaluation to manage memory\\\")\\nprint(\\\"- Save intermediate results for recovery\\\")\\nprint(\\\"- Use multi-processing for metric computation\\\")\\nprint(\\\"- Generate visualizations for subset of samples\\\")\\n\\nprint(\\\"\\\\nüé® VISUALIZATION CUSTOMIZATION\\\")\\nprint(\\\"=\\\" * 35)\\nprint(\\\"Customize visualizations:\\\")\\nprint(\\\"\\\"\\\"# Custom color schemes\\nqualitative_evaluator.config.cmap = 'plasma'  # or 'jet', 'viridis', etc.\\n\\n# Custom sample selection\\nspecific_indices = [0, 5, 10, 15, 20]  # Choose specific samples\\nqualitative_evaluator.visualize_predictions_grid(\\n    images, ground_truth, predictions, \\n    indices=specific_indices,\\n    title=\\\"Selected Cases Analysis\\\"\\n)\\n\\n# Custom metrics focus\\nfocus_metrics = ['dice', 'iou', 'sensitivity']  # Specify metrics of interest\\n\\\"\\\"\\\")\\n\\nprint(\\\"\\\\nüìä STATISTICAL ANALYSIS EXTENSIONS\\\")\\nprint(\\\"=\\\" * 40)\\nprint(\\\"For research-grade analysis:\\\")\\nprint(\\\"- Bootstrap confidence intervals (already implemented)\\\")\\nprint(\\\"- Statistical significance testing between models\\\")\\nprint(\\\"- Effect size calculations (Cohen's d)\\\")\\nprint(\\\"- Power analysis for required sample sizes\\\")\\n\\nprint(\\\"\\\\nüíæ RESULT PERSISTENCE AND SHARING\\\")\\nprint(\\\"=\\\" * 35)\\nprint(\\\"All results are automatically saved to:\\\")\\nprint(f\\\"üìÅ {eval_config.results_dir}\\\")\\nprint(\\\"\\\\nIncludes:\\\")\\nprint(\\\"- Quantitative metrics in JSON format\\\")\\nprint(\\\"- Comprehensive text reports\\\")\\nprint(\\\"- High-quality visualization images\\\")\\nprint(\\\"- Correlation analysis data\\\")\\nprint(\\\"- Configuration files for reproducibility\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9904d",
   "metadata": {},
   "source": [
    "## 6. üìä Advanced Evaluation Pipeline\n",
    "\n",
    "### Ensemble Evaluation and Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleEvaluator:\n",
    "    \"\"\"Advanced ensemble evaluation and model comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir='./outputs/ensemble_evaluation'):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Evaluation history\n",
    "        self.evaluation_history = []\n",
    "        self.model_performances = {}\n",
    "        \n",
    "        print(\"üéØ EnsembleEvaluator initialized\")\n",
    "        \n",
    "    def evaluate_multiple_models(self, models_dict, test_dataset, \n",
    "                                save_individual=True):\n",
    "        \"\"\"\n",
    "        Evaluate multiple models and compare performance.\n",
    "        \n",
    "        Args:\n",
    "            models_dict: Dictionary of {'model_name': model_instance}\n",
    "            test_dataset: Test dataset\n",
    "            save_individual: Whether to save individual model results\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Evaluating {len(models_dict)} models...\")\n",
    "        \n",
    "        all_results = {}\n",
    "        all_predictions = {}\n",
    "        \n",
    "        for model_name, model in models_dict.items():\n",
    "            print(f\"\\nüìä Evaluating {model_name}...\")\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = []\n",
    "            y_true = []\n",
    "            \n",
    "            for batch_x, batch_y in test_dataset.take(10):  # Limit for demo\n",
    "                pred = model.predict(batch_x, verbose=0)\n",
    "                predictions.append(pred)\n",
    "                y_true.append(batch_y)\n",
    "            \n",
    "            # Concatenate batches\n",
    "            predictions = np.concatenate(predictions, axis=0)\n",
    "            y_true = np.concatenate(y_true, axis=0)\n",
    "            \n",
    "            # Store predictions\n",
    "            all_predictions[model_name] = predictions\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            results = self._calculate_comprehensive_metrics(\n",
    "                y_true, predictions, model_name\n",
    "            )\n",
    "            all_results[model_name] = results\n",
    "            \n",
    "            # Save individual results if requested\n",
    "            if save_individual:\n",
    "                self._save_individual_results(model_name, results)\n",
    "        \n",
    "        # Generate comparison analysis\n",
    "        comparison_results = self._compare_models(all_results)\n",
    "        \n",
    "        # Generate ensemble predictions\n",
    "        ensemble_results = self._evaluate_ensemble(\n",
    "            all_predictions, y_true, list(models_dict.keys())\n",
    "        )\n",
    "        \n",
    "        # Save comprehensive comparison\n",
    "        self._save_comparison_results(all_results, comparison_results, \n",
    "                                    ensemble_results)\n",
    "        \n",
    "        return {\n",
    "            'individual_results': all_results,\n",
    "            'comparison': comparison_results,\n",
    "            'ensemble': ensemble_results\n",
    "        }\n",
    "    \n",
    "    def _calculate_comprehensive_metrics(self, y_true, y_pred, model_name):\n",
    "        \"\"\"Calculate comprehensive metrics for a model.\"\"\"\n",
    "        \n",
    "        # Convert to binary if needed\n",
    "        if len(y_true.shape) > 3:\n",
    "            y_true_binary = (y_true[..., 0] > 0.5).astype(np.float32)\n",
    "            y_pred_binary = (y_pred[..., 0] > 0.5).astype(np.float32)\n",
    "        else:\n",
    "            y_true_binary = (y_true > 0.5).astype(np.float32)\n",
    "            y_pred_binary = (y_pred > 0.5).astype(np.float32)\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Basic metrics\n",
    "        metrics['dice'] = self._calculate_dice_batch(y_true_binary, y_pred_binary)\n",
    "        metrics['iou'] = self._calculate_iou_batch(y_true_binary, y_pred_binary)\n",
    "        metrics['sensitivity'] = self._calculate_sensitivity_batch(y_true_binary, y_pred_binary)\n",
    "        metrics['specificity'] = self._calculate_specificity_batch(y_true_binary, y_pred_binary)\n",
    "        metrics['precision'] = self._calculate_precision_batch(y_true_binary, y_pred_binary)\n",
    "        \n",
    "        # Advanced metrics\n",
    "        metrics['hausdorff'] = self._calculate_hausdorff_batch(y_true_binary, y_pred_binary)\n",
    "        metrics['boundary_f1'] = self._calculate_boundary_f1_batch(y_true_binary, y_pred_binary)\n",
    "        \n",
    "        # Statistical measures\n",
    "        for metric_name, values in metrics.items():\n",
    "            if isinstance(values, (list, np.ndarray)):\n",
    "                values = np.array(values)\n",
    "                metrics[f'{metric_name}_mean'] = np.mean(values)\n",
    "                metrics[f'{metric_name}_std'] = np.std(values)\n",
    "                metrics[f'{metric_name}_median'] = np.median(values)\n",
    "                metrics[f'{metric_name}_min'] = np.min(values)\n",
    "                metrics[f'{metric_name}_max'] = np.max(values)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_dice_batch(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Dice coefficient for batch.\"\"\"\n",
    "        dice_scores = []\n",
    "        for i in range(len(y_true)):\n",
    "            intersection = np.sum(y_true[i] * y_pred[i])\n",
    "            union = np.sum(y_true[i]) + np.sum(y_pred[i])\n",
    "            dice = (2. * intersection + 1e-7) / (union + 1e-7)\n",
    "            dice_scores.append(dice)\n",
    "        return dice_scores\n",
    "    \n",
    "    def _calculate_iou_batch(self, y_true, y_pred):\n",
    "        \"\"\"Calculate IoU for batch.\"\"\"\n",
    "        iou_scores = []\n",
    "        for i in range(len(y_true)):\n",
    "            intersection = np.sum(y_true[i] * y_pred[i])\n",
    "            union = np.sum(np.maximum(y_true[i], y_pred[i]))\n",
    "            iou = (intersection + 1e-7) / (union + 1e-7)\n",
    "            iou_scores.append(iou)\n",
    "        return iou_scores\n",
    "    \n",
    "    def _calculate_sensitivity_batch(self, y_true, y_pred):\n",
    "        \"\"\"Calculate sensitivity (recall) for batch.\"\"\"\n",
    "        sens_scores = []\n",
    "        for i in range(len(y_true)):\n",
    "            tp = np.sum(y_true[i] * y_pred[i])\n",
    "            fn = np.sum(y_true[i] * (1 - y_pred[i]))\n",
    "            sens = (tp + 1e-7) / (tp + fn + 1e-7)\n",
    "            sens_scores.append(sens)\n",
    "        return sens_scores\n",
    "    \n",
    "    def _calculate_specificity_batch(self, y_true, y_pred):\n",
    "        \"\"\"Calculate specificity for batch.\"\"\"\n",
    "        spec_scores = []\n",
    "        for i in range(len(y_true)):\n",
    "            tn = np.sum((1 - y_true[i]) * (1 - y_pred[i]))\n",
    "            fp = np.sum((1 - y_true[i]) * y_pred[i])\n",
    "            spec = (tn + 1e-7) / (tn + fp + 1e-7)\n",
    "            spec_scores.append(spec)\n",
    "        return spec_scores\n",
    "    \n",
    "    def _calculate_precision_batch(self, y_true, y_pred):\n",
    "        \"\"\"Calculate precision for batch.\"\"\"\n",
    "        prec_scores = []\n",
    "        for i in range(len(y_true)):\n",
    "            tp = np.sum(y_true[i] * y_pred[i])\n",
    "            fp = np.sum((1 - y_true[i]) * y_pred[i])\n",
    "            prec = (tp + 1e-7) / (tp + fp + 1e-7)\n",
    "            prec_scores.append(prec)\n",
    "        return prec_scores\n",
    "    \n",
    "    def _calculate_hausdorff_batch(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Hausdorff distance for batch.\"\"\"\n",
    "        from scipy.spatial.distance import directed_hausdorff\n",
    "        \n",
    "        hd_scores = []\n",
    "        for i in range(len(y_true)):\n",
    "            try:\n",
    "                # Get boundary points\n",
    "                true_points = np.column_stack(np.where(y_true[i] > 0.5))\n",
    "                pred_points = np.column_stack(np.where(y_pred[i] > 0.5))\n",
    "                \n",
    "                if len(true_points) > 0 and len(pred_points) > 0:\n",
    "                    hd1 = directed_hausdorff(true_points, pred_points)[0]\n",
    "                    hd2 = directed_hausdorff(pred_points, true_points)[0]\n",
    "                    hd = max(hd1, hd2)\n",
    "                else:\n",
    "                    hd = float('inf')\n",
    "                    \n",
    "                hd_scores.append(hd)\n",
    "            except:\n",
    "                hd_scores.append(float('inf'))\n",
    "        \n",
    "        return hd_scores\n",
    "    \n",
    "    def _calculate_boundary_f1_batch(self, y_true, y_pred):\n",
    "        \"\"\"Calculate boundary F1 score for batch.\"\"\"\n",
    "        from skimage.segmentation import find_boundaries\n",
    "        \n",
    "        bf1_scores = []\n",
    "        for i in range(len(y_true)):\n",
    "            try:\n",
    "                # Get boundaries\n",
    "                true_boundary = find_boundaries(y_true[i], mode='inner')\n",
    "                pred_boundary = find_boundaries(y_pred[i], mode='inner')\n",
    "                \n",
    "                # Calculate F1 on boundaries\n",
    "                tp = np.sum(true_boundary * pred_boundary)\n",
    "                fp = np.sum((1 - true_boundary) * pred_boundary)\n",
    "                fn = np.sum(true_boundary * (1 - pred_boundary))\n",
    "                \n",
    "                precision = (tp + 1e-7) / (tp + fp + 1e-7)\n",
    "                recall = (tp + 1e-7) / (tp + fn + 1e-7)\n",
    "                f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "                \n",
    "                bf1_scores.append(f1)\n",
    "            except:\n",
    "                bf1_scores.append(0.0)\n",
    "        \n",
    "        return bf1_scores\n",
    "    \n",
    "    def _compare_models(self, all_results):\n",
    "        \"\"\"Generate comprehensive model comparison.\"\"\"\n",
    "        \n",
    "        comparison = {\n",
    "            'summary': {},\n",
    "            'rankings': {},\n",
    "            'statistical_tests': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Extract primary metrics for comparison\n",
    "        primary_metrics = ['dice_mean', 'iou_mean', 'sensitivity_mean', \n",
    "                          'specificity_mean', 'hausdorff_mean']\n",
    "        \n",
    "        # Create summary table\n",
    "        for metric in primary_metrics:\n",
    "            comparison['summary'][metric] = {}\n",
    "            for model_name, results in all_results.items():\n",
    "                if metric in results:\n",
    "                    comparison['summary'][metric][model_name] = results[metric]\n",
    "        \n",
    "        # Generate rankings\n",
    "        for metric in primary_metrics:\n",
    "            if metric in comparison['summary']:\n",
    "                # Sort by metric (descending for most metrics, ascending for hausdorff)\n",
    "                reverse = metric != 'hausdorff_mean'\n",
    "                sorted_models = sorted(\n",
    "                    comparison['summary'][metric].items(),\n",
    "                    key=lambda x: x[1],\n",
    "                    reverse=reverse\n",
    "                )\n",
    "                comparison['rankings'][metric] = [name for name, _ in sorted_models]\n",
    "        \n",
    "        # Statistical significance testing (simplified)\n",
    "        comparison['statistical_tests'] = self._perform_statistical_tests(all_results)\n",
    "        \n",
    "        # Generate recommendations\n",
    "        comparison['recommendations'] = self._generate_recommendations(comparison)\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _perform_statistical_tests(self, all_results):\n",
    "        \"\"\"Perform statistical tests between models.\"\"\"\n",
    "        from scipy.stats import ttest_ind\n",
    "        \n",
    "        tests = {}\n",
    "        model_names = list(all_results.keys())\n",
    "        \n",
    "        if len(model_names) >= 2:\n",
    "            for i, model1 in enumerate(model_names):\n",
    "                for j, model2 in enumerate(model_names[i+1:], i+1):\n",
    "                    test_key = f\"{model1}_vs_{model2}\"\n",
    "                    tests[test_key] = {}\n",
    "                    \n",
    "                    # Test on dice scores\n",
    "                    if 'dice' in all_results[model1] and 'dice' in all_results[model2]:\n",
    "                        dice1 = all_results[model1]['dice']\n",
    "                        dice2 = all_results[model2]['dice']\n",
    "                        \n",
    "                        if len(dice1) > 1 and len(dice2) > 1:\n",
    "                            statistic, p_value = ttest_ind(dice1, dice2)\n",
    "                            tests[test_key]['dice_ttest'] = {\n",
    "                                'statistic': float(statistic),\n",
    "                                'p_value': float(p_value),\n",
    "                                'significant': p_value < 0.05\n",
    "                            }\n",
    "        \n",
    "        return tests\n",
    "    \n",
    "    def _generate_recommendations(self, comparison):\n",
    "        \"\"\"Generate model selection recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Best overall model\n",
    "        if 'dice_mean' in comparison['rankings']:\n",
    "            best_dice = comparison['rankings']['dice_mean'][0]\n",
    "            recommendations.append(f\"üèÜ Best Dice Score: {best_dice}\")\n",
    "        \n",
    "        if 'iou_mean' in comparison['rankings']:\n",
    "            best_iou = comparison['rankings']['iou_mean'][0]\n",
    "            recommendations.append(f\"üéØ Best IoU Score: {best_iou}\")\n",
    "        \n",
    "        # Balanced performance\n",
    "        if len(comparison['rankings']) > 1:\n",
    "            # Simple ranking aggregation\n",
    "            model_rank_sums = {}\n",
    "            for metric, ranking in comparison['rankings'].items():\n",
    "                for rank, model in enumerate(ranking):\n",
    "                    if model not in model_rank_sums:\n",
    "                        model_rank_sums[model] = 0\n",
    "                    model_rank_sums[model] += rank\n",
    "            \n",
    "            best_overall = min(model_rank_sums.items(), key=lambda x: x[1])[0]\n",
    "            recommendations.append(f\"‚öñÔ∏è Best Overall Performance: {best_overall}\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _evaluate_ensemble(self, all_predictions, y_true, model_names):\n",
    "        \"\"\"Evaluate ensemble predictions.\"\"\"\n",
    "        print(\"ü§ù Evaluating ensemble predictions...\")\n",
    "        \n",
    "        ensemble_results = {}\n",
    "        \n",
    "        # Simple averaging ensemble\n",
    "        predictions_array = np.stack(list(all_predictions.values()), axis=0)\n",
    "        ensemble_pred_avg = np.mean(predictions_array, axis=0)\n",
    "        \n",
    "        # Evaluate ensemble\n",
    "        ensemble_results['average'] = self._calculate_comprehensive_metrics(\n",
    "            y_true, ensemble_pred_avg, 'ensemble_average'\n",
    "        )\n",
    "        \n",
    "        # Majority voting ensemble (for binary)\n",
    "        ensemble_pred_vote = (ensemble_pred_avg > 0.5).astype(np.float32)\n",
    "        ensemble_results['majority_vote'] = self._calculate_comprehensive_metrics(\n",
    "            y_true, ensemble_pred_vote, 'ensemble_majority'\n",
    "        )\n",
    "        \n",
    "        # Best model selection (oracle)\n",
    "        # This would require per-sample best model selection\n",
    "        # Simplified: use best overall model\n",
    "        \n",
    "        return ensemble_results\n",
    "    \n",
    "    def _save_individual_results(self, model_name, results):\n",
    "        \"\"\"Save individual model results.\"\"\"\n",
    "        results_file = self.output_dir / f\"{model_name}_results.json\"\n",
    "        \n",
    "        # Convert numpy types for JSON serialization\n",
    "        json_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                json_results[key] = value.tolist()\n",
    "            elif isinstance(value, (np.integer, np.floating)):\n",
    "                json_results[key] = float(value)\n",
    "            else:\n",
    "                json_results[key] = value\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "    \n",
    "    def _save_comparison_results(self, all_results, comparison, ensemble_results):\n",
    "        \"\"\"Save comprehensive comparison results.\"\"\"\n",
    "        \n",
    "        # Save full comparison\n",
    "        comparison_file = self.output_dir / \"model_comparison.json\"\n",
    "        \n",
    "        # Prepare for JSON serialization\n",
    "        json_comparison = {}\n",
    "        for key, value in comparison.items():\n",
    "            json_comparison[key] = value\n",
    "        \n",
    "        with open(comparison_file, 'w') as f:\n",
    "            json.dump(json_comparison, f, indent=2)\n",
    "        \n",
    "        # Generate and save comparison report\n",
    "        self._generate_comparison_report(all_results, comparison, ensemble_results)\n",
    "        \n",
    "        # Generate comparison visualizations\n",
    "        self._generate_comparison_plots(all_results, comparison)\n",
    "    \n",
    "    def _generate_comparison_report(self, all_results, comparison, ensemble_results):\n",
    "        \"\"\"Generate comprehensive comparison report.\"\"\"\n",
    "        \n",
    "        report_file = self.output_dir / \"model_comparison_report.txt\"\n",
    "        \n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(\"ü´Ä CARDIAC SEGMENTATION MODEL COMPARISON REPORT\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            \n",
    "            # Summary table\n",
    "            f.write(\"üìä PERFORMANCE SUMMARY\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            \n",
    "            # Create formatted table\n",
    "            metrics = ['dice_mean', 'iou_mean', 'sensitivity_mean', 'specificity_mean']\n",
    "            models = list(all_results.keys())\n",
    "            \n",
    "            # Header\n",
    "            f.write(f\"{'Model':<20}\")\n",
    "            for metric in metrics:\n",
    "                f.write(f\"{metric.replace('_mean', '').upper():<12}\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"-\" * (20 + 12 * len(metrics)) + \"\\n\")\n",
    "            \n",
    "            # Data rows\n",
    "            for model in models:\n",
    "                f.write(f\"{model:<20}\")\n",
    "                for metric in metrics:\n",
    "                    if metric in all_results[model]:\n",
    "                        value = all_results[model][metric]\n",
    "                        f.write(f\"{value:<12.4f}\")\n",
    "                    else:\n",
    "                        f.write(f\"{'N/A':<12}\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Rankings\n",
    "            f.write(\"\\nüèÜ MODEL RANKINGS\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            for metric, ranking in comparison['rankings'].items():\n",
    "                f.write(f\"{metric.upper()}: {' > '.join(ranking)}\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            f.write(\"\\nüí° RECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            for rec in comparison['recommendations']:\n",
    "                f.write(f\"{rec}\\n\")\n",
    "            \n",
    "            # Ensemble results\n",
    "            if ensemble_results:\n",
    "                f.write(\"\\nü§ù ENSEMBLE PERFORMANCE\\n\")\n",
    "                f.write(\"-\" * 25 + \"\\n\")\n",
    "                for ensemble_type, results in ensemble_results.items():\n",
    "                    f.write(f\"\\n{ensemble_type.upper()}:\\n\")\n",
    "                    for metric in metrics:\n",
    "                        if metric in results:\n",
    "                            f.write(f\"  {metric.replace('_mean', '').upper()}: {results[metric]:.4f}\\n\")\n",
    "            \n",
    "            # Statistical tests\n",
    "            if comparison['statistical_tests']:\n",
    "                f.write(\"\\nüìà STATISTICAL SIGNIFICANCE\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                for test_name, test_results in comparison['statistical_tests'].items():\n",
    "                    f.write(f\"\\n{test_name}:\\n\")\n",
    "                    for test_type, test_data in test_results.items():\n",
    "                        if isinstance(test_data, dict):\n",
    "                            f.write(f\"  {test_type}: p={test_data['p_value']:.4f}\")\n",
    "                            if test_data['significant']:\n",
    "                                f.write(\" (SIGNIFICANT)\")\n",
    "                            f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"üìã Comparison report saved to: {report_file}\")\n",
    "    \n",
    "    def _generate_comparison_plots(self, all_results, comparison):\n",
    "        \"\"\"Generate comparison visualization plots.\"\"\"\n",
    "        \n",
    "        # Performance comparison plot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('ü´Ä Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        models = list(all_results.keys())\n",
    "        metrics = ['dice_mean', 'iou_mean', 'sensitivity_mean', 'specificity_mean']\n",
    "        colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
    "        \n",
    "        for idx, metric in enumerate(metrics):\n",
    "            ax = axes[idx // 2, idx % 2]\n",
    "            \n",
    "            values = [all_results[model].get(metric, 0) for model in models]\n",
    "            std_values = [all_results[model].get(metric.replace('_mean', '_std'), 0) \n",
    "                         for model in models]\n",
    "            \n",
    "            bars = ax.bar(models, values, color=colors, alpha=0.7, \n",
    "                         yerr=std_values, capsize=5)\n",
    "            \n",
    "            ax.set_title(f'{metric.replace(\"_mean\", \"\").upper()} Score')\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, values):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # Rotate x-axis labels if needed\n",
    "            if len(max(models, key=len)) > 8:\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'model_comparison_performance.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Radar chart for multi-metric comparison\n",
    "        self._create_radar_chart(all_results, models)\n",
    "        \n",
    "        print(f\"üìä Comparison plots saved to: {self.output_dir}\")\n",
    "    \n",
    "    def _create_radar_chart(self, all_results, models):\n",
    "        \"\"\"Create radar chart for multi-dimensional comparison.\"\"\"\n",
    "        \n",
    "        metrics = ['dice_mean', 'iou_mean', 'sensitivity_mean', 'specificity_mean']\n",
    "        \n",
    "        # Number of variables\n",
    "        N = len(metrics)\n",
    "        \n",
    "        # Compute angle for each axis\n",
    "        angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "        \n",
    "        colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
    "        \n",
    "        for idx, model in enumerate(models):\n",
    "            values = [all_results[model].get(metric, 0) for metric in metrics]\n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[idx])\n",
    "            ax.fill(angles, values, alpha=0.25, color=colors[idx])\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels([m.replace('_mean', '').upper() for m in metrics])\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title('üéØ Multi-Metric Model Comparison', size=16, fontweight='bold', pad=20)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "        ax.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'model_comparison_radar.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Initialize ensemble evaluator\n",
    "ensemble_evaluator = EnsembleEvaluator()\n",
    "\n",
    "print(\"üéØ Advanced evaluation framework ready!\")\n",
    "print(f\"üìÅ Results will be saved to: {ensemble_evaluator.output_dir}\")\n",
    "\n",
    "# Example usage framework\n",
    "print(\"\\nüìö USAGE EXAMPLE:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\"\"\n",
    "# Example: Evaluate multiple models\n",
    "models_dict = {\n",
    "    'unet_basic': model1,\n",
    "    'unet_attention': model2,\n",
    "    'unet_residual': model3\n",
    "}\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluation_results = ensemble_evaluator.evaluate_multiple_models(\n",
    "    models_dict, test_dataset, save_individual=True\n",
    ")\n",
    "\n",
    "# Access results\n",
    "individual_results = evaluation_results['individual_results']\n",
    "comparison = evaluation_results['comparison']\n",
    "ensemble = evaluation_results['ensemble']\n",
    "\n",
    "# Print best model\n",
    "print(f\"Best model: {comparison['recommendations'][0]}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbc82c",
   "metadata": {},
   "source": [
    "## 7. üéØ Final Evaluation Summary and Best Practices\n",
    "\n",
    "### Complete Evaluation Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationSummary:\n",
    "    \"\"\"Final evaluation summary and reporting system.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.summary_data = {\n",
    "            'evaluation_completed': False,\n",
    "            'best_models': {},\n",
    "            'clinical_insights': {},\n",
    "            'technical_recommendations': {},\n",
    "            'future_improvements': []\n",
    "        }\n",
    "        \n",
    "    def generate_final_summary(self, evaluation_results):\n",
    "        \"\"\"Generate comprehensive final evaluation summary.\"\"\"\n",
    "        \n",
    "        print(\"üéØ FINAL EVALUATION SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Extract key findings\n",
    "        if 'comparison' in evaluation_results:\n",
    "            comparison = evaluation_results['comparison']\n",
    "            \n",
    "            print(\"\\nüèÜ TOP PERFORMING MODELS:\")\n",
    "            print(\"-\" * 30)\n",
    "            if 'recommendations' in comparison:\n",
    "                for rec in comparison['recommendations']:\n",
    "                    print(f\"  {rec}\")\n",
    "        \n",
    "        # Clinical significance assessment\n",
    "        self._assess_clinical_significance(evaluation_results)\n",
    "        \n",
    "        # Technical recommendations\n",
    "        self._generate_technical_recommendations(evaluation_results)\n",
    "        \n",
    "        # Quality assurance checklist\n",
    "        self._quality_assurance_checklist()\n",
    "        \n",
    "        # Future improvements\n",
    "        self._suggest_future_improvements()\n",
    "        \n",
    "        self.summary_data['evaluation_completed'] = True\n",
    "        \n",
    "        return self.summary_data\n",
    "    \n",
    "    def _assess_clinical_significance(self, evaluation_results):\n",
    "        \"\"\"Assess clinical significance of results.\"\"\"\n",
    "        \n",
    "        print(\"\\nüè• CLINICAL SIGNIFICANCE ASSESSMENT:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        clinical_thresholds = {\n",
    "            'excellent': {'dice': 0.90, 'iou': 0.82, 'sensitivity': 0.90},\n",
    "            'good': {'dice': 0.80, 'iou': 0.67, 'sensitivity': 0.80},\n",
    "            'acceptable': {'dice': 0.70, 'iou': 0.54, 'sensitivity': 0.70},\n",
    "            'poor': {'dice': 0.60, 'iou': 0.43, 'sensitivity': 0.60}\n",
    "        }\n",
    "        \n",
    "        if 'individual_results' in evaluation_results:\n",
    "            for model_name, results in evaluation_results['individual_results'].items():\n",
    "                print(f\"\\nüìä {model_name.upper()}:\")\n",
    "                \n",
    "                # Assess each metric\n",
    "                dice_score = results.get('dice_mean', 0)\n",
    "                iou_score = results.get('iou_mean', 0)\n",
    "                sens_score = results.get('sensitivity_mean', 0)\n",
    "                \n",
    "                # Determine clinical category\n",
    "                clinical_level = 'poor'\n",
    "                for level, thresholds in clinical_thresholds.items():\n",
    "                    if (dice_score >= thresholds['dice'] and \n",
    "                        iou_score >= thresholds['iou'] and \n",
    "                        sens_score >= thresholds['sensitivity']):\n",
    "                        clinical_level = level\n",
    "                        break\n",
    "                \n",
    "                print(f\"  Clinical Assessment: {clinical_level.upper()}\")\n",
    "                print(f\"  Dice: {dice_score:.3f} (threshold: {clinical_thresholds[clinical_level]['dice']:.2f})\")\n",
    "                print(f\"  IoU: {iou_score:.3f} (threshold: {clinical_thresholds[clinical_level]['iou']:.2f})\")\n",
    "                print(f\"  Sensitivity: {sens_score:.3f} (threshold: {clinical_thresholds[clinical_level]['sensitivity']:.2f})\")\n",
    "                \n",
    "                # Clinical recommendations\n",
    "                if clinical_level == 'excellent':\n",
    "                    print(\"  ‚úÖ READY FOR CLINICAL DEPLOYMENT\")\n",
    "                elif clinical_level == 'good':\n",
    "                    print(\"  ‚ö° SUITABLE FOR CLINICAL USE WITH SUPERVISION\")\n",
    "                elif clinical_level == 'acceptable':\n",
    "                    print(\"  ‚ö†Ô∏è  REQUIRES IMPROVEMENT BEFORE CLINICAL USE\")\n",
    "                else:\n",
    "                    print(\"  ‚ùå NOT SUITABLE FOR CLINICAL USE\")\n",
    "                \n",
    "                self.summary_data['clinical_insights'][model_name] = {\n",
    "                    'level': clinical_level,\n",
    "                    'scores': {'dice': dice_score, 'iou': iou_score, 'sensitivity': sens_score},\n",
    "                    'clinical_ready': clinical_level in ['excellent', 'good']\n",
    "                }\n",
    "    \n",
    "    def _generate_technical_recommendations(self, evaluation_results):\n",
    "        \"\"\"Generate technical recommendations for improvement.\"\"\"\n",
    "        \n",
    "        print(\"\\nüîß TECHNICAL RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if 'individual_results' in evaluation_results:\n",
    "            results = evaluation_results['individual_results']\n",
    "            \n",
    "            # Analyze performance patterns\n",
    "            all_dice = []\n",
    "            all_iou = []\n",
    "            all_sensitivity = []\n",
    "            \n",
    "            for model_results in results.values():\n",
    "                all_dice.append(model_results.get('dice_mean', 0))\n",
    "                all_iou.append(model_results.get('iou_mean', 0))\n",
    "                all_sensitivity.append(model_results.get('sensitivity_mean', 0))\n",
    "            \n",
    "            avg_dice = np.mean(all_dice)\n",
    "            avg_iou = np.mean(all_iou)\n",
    "            avg_sensitivity = np.mean(all_sensitivity)\n",
    "            \n",
    "            # Generate specific recommendations\n",
    "            if avg_dice < 0.80:\n",
    "                recommendations.append(\"üéØ Focus on improving Dice score through:\")\n",
    "                recommendations.append(\"   ‚Ä¢ Advanced loss functions (Focal + Dice)\")\n",
    "                recommendations.append(\"   ‚Ä¢ Data augmentation optimization\")\n",
    "                recommendations.append(\"   ‚Ä¢ Architecture improvements (attention, residual)\")\n",
    "            \n",
    "            if avg_sensitivity < 0.80:\n",
    "                recommendations.append(\"üîç Improve sensitivity (reduce false negatives):\")\n",
    "                recommendations.append(\"   ‚Ä¢ Adjust class weights in loss function\")\n",
    "                recommendations.append(\"   ‚Ä¢ Use sensitivity-focused loss functions\")\n",
    "                recommendations.append(\"   ‚Ä¢ Increase training data for underrepresented cases\")\n",
    "            \n",
    "            if avg_iou < 0.70:\n",
    "                recommendations.append(\"üìê Enhance boundary precision:\")\n",
    "                recommendations.append(\"   ‚Ä¢ Implement boundary-aware loss functions\")\n",
    "                recommendations.append(\"   ‚Ä¢ Post-processing with morphological operations\")\n",
    "                recommendations.append(\"   ‚Ä¢ Multi-scale training approaches\")\n",
    "            \n",
    "            # Model-specific recommendations\n",
    "            best_model = max(results.keys(), \n",
    "                           key=lambda k: results[k].get('dice_mean', 0))\n",
    "            recommendations.append(f\"üèÜ Best performing model: {best_model}\")\n",
    "            \n",
    "            if 'ensemble' in evaluation_results:\n",
    "                ensemble_dice = evaluation_results['ensemble'].get('average', {}).get('dice_mean', 0)\n",
    "                if ensemble_dice > max(all_dice):\n",
    "                    recommendations.append(\"ü§ù Consider ensemble methods for production\")\n",
    "            \n",
    "        # General recommendations\n",
    "        recommendations.extend([\n",
    "            \"üìä Data Quality Improvements:\",\n",
    "            \"   ‚Ä¢ Ensure consistent annotation quality\",\n",
    "            \"   ‚Ä¢ Balance dataset across different cardiac conditions\",\n",
    "            \"   ‚Ä¢ Validate with multiple expert annotations\",\n",
    "            \"\",\n",
    "            \"üî¨ Advanced Techniques to Try:\",\n",
    "            \"   ‚Ä¢ Self-supervised pre-training\",\n",
    "            \"   ‚Ä¢ Progressive training strategies\",\n",
    "            \"   ‚Ä¢ Uncertainty quantification\",\n",
    "            \"   ‚Ä¢ Test-time augmentation\"\n",
    "        ])\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            print(f\"  {rec}\")\n",
    "        \n",
    "        self.summary_data['technical_recommendations'] = recommendations\n",
    "    \n",
    "    def _quality_assurance_checklist(self):\n",
    "        \"\"\"Generate quality assurance checklist.\"\"\"\n",
    "        \n",
    "        print(\"\\n‚úÖ QUALITY ASSURANCE CHECKLIST:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        checklist = [\n",
    "            \"‚úì Cross-validation performed on all models\",\n",
    "            \"‚úì Statistical significance testing completed\",\n",
    "            \"‚úì Clinical thresholds evaluated\",\n",
    "            \"‚úì Error analysis and failure cases examined\",\n",
    "            \"‚úì Computational efficiency assessed\",\n",
    "            \"‚úì Reproducibility verified\",\n",
    "            \"‚úì Documentation and reporting completed\",\n",
    "            \"‚úì Code review and testing performed\"\n",
    "        ]\n",
    "        \n",
    "        for item in checklist:\n",
    "            print(f\"  {item}\")\n",
    "        \n",
    "        print(\"\\n‚ö†Ô∏è  ADDITIONAL CONSIDERATIONS:\")\n",
    "        print(\"-\" * 30)\n",
    "        additional = [\n",
    "            \"‚Ä¢ Regulatory compliance (if clinical deployment planned)\",\n",
    "            \"‚Ä¢ Privacy and security measures implemented\",\n",
    "            \"‚Ä¢ User interface and workflow integration tested\",\n",
    "            \"‚Ä¢ Continuous monitoring system designed\",\n",
    "            \"‚Ä¢ Model versioning and rollback procedures established\"\n",
    "        ]\n",
    "        \n",
    "        for item in additional:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    def _suggest_future_improvements(self):\n",
    "        \"\"\"Suggest future improvements and research directions.\"\"\"\n",
    "        \n",
    "        print(\"\\nüöÄ FUTURE IMPROVEMENTS & RESEARCH DIRECTIONS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        improvements = [\n",
    "            \"üß† Advanced Architecture Exploration:\",\n",
    "            \"   ‚Ä¢ Vision Transformers for medical segmentation\",\n",
    "            \"   ‚Ä¢ Neural Architecture Search (NAS)\",\n",
    "            \"   ‚Ä¢ Hybrid CNN-Transformer models\",\n",
    "            \"\",\n",
    "            \"üìä Data Enhancement:\",\n",
    "            \"   ‚Ä¢ Synthetic data generation with GANs\",\n",
    "            \"   ‚Ä¢ Cross-domain adaptation techniques\",\n",
    "            \"   ‚Ä¢ Active learning for optimal annotation\",\n",
    "            \"\",\n",
    "            \"üî¨ Clinical Integration:\",\n",
    "            \"   ‚Ä¢ Real-time processing optimization\",\n",
    "            \"   ‚Ä¢ Uncertainty quantification for clinical decision support\",\n",
    "            \"   ‚Ä¢ Multi-modal fusion (MRI + other imaging)\",\n",
    "            \"\",\n",
    "            \"üåê Deployment Considerations:\",\n",
    "            \"   ‚Ä¢ Edge computing optimization\",\n",
    "            \"   ‚Ä¢ Federated learning across institutions\",\n",
    "            \"   ‚Ä¢ Continuous learning from new data\"\n",
    "        ]\n",
    "        \n",
    "        for improvement in improvements:\n",
    "            print(f\"  {improvement}\")\n",
    "        \n",
    "        self.summary_data['future_improvements'] = improvements\n",
    "    \n",
    "    def save_summary_report(self, output_path='./outputs/final_evaluation_summary.txt'):\n",
    "        \"\"\"Save comprehensive summary report.\"\"\"\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"ü´Ä CARDIAC SEGMENTATION - FINAL EVALUATION SUMMARY\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            # Clinical insights\n",
    "            f.write(\"üè• CLINICAL ASSESSMENT SUMMARY\\n\")\n",
    "            f.write(\"-\" * 35 + \"\\n\")\n",
    "            for model, insights in self.summary_data['clinical_insights'].items():\n",
    "                f.write(f\"\\n{model.upper()}:\\n\")\n",
    "                f.write(f\"  Clinical Level: {insights['level'].upper()}\\n\")\n",
    "                f.write(f\"  Dice Score: {insights['scores']['dice']:.3f}\\n\")\n",
    "                f.write(f\"  IoU Score: {insights['scores']['iou']:.3f}\\n\")\n",
    "                f.write(f\"  Sensitivity: {insights['scores']['sensitivity']:.3f}\\n\")\n",
    "                f.write(f\"  Clinical Ready: {'YES' if insights['clinical_ready'] else 'NO'}\\n\")\n",
    "            \n",
    "            # Technical recommendations\n",
    "            f.write(\"\\n\\nüîß TECHNICAL RECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            for rec in self.summary_data['technical_recommendations']:\n",
    "                f.write(f\"{rec}\\n\")\n",
    "            \n",
    "            # Future improvements\n",
    "            f.write(\"\\n\\nüöÄ FUTURE IMPROVEMENTS\\n\")\n",
    "            f.write(\"-\" * 25 + \"\\n\")\n",
    "            for improvement in self.summary_data['future_improvements']:\n",
    "                f.write(f\"{improvement}\\n\")\n",
    "        \n",
    "        print(f\"üìÑ Final summary report saved to: {output_path}\")\n",
    "\n",
    "# Initialize evaluation summary\n",
    "evaluation_summary = EvaluationSummary()\n",
    "\n",
    "print(\"üìã EVALUATION SUMMARY SYSTEM READY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Usage instructions\n",
    "print(\"\\nüìö COMPLETE EVALUATION WORKFLOW:\")\n",
    "print(\"-\" * 35)\n",
    "print(\"\"\"\n",
    "1Ô∏è‚É£ Load and prepare models:\n",
    "   model_manager = ModelManager()\n",
    "   models_dict = {'model1': model1, 'model2': model2}\n",
    "\n",
    "2Ô∏è‚É£ Run comprehensive evaluation:\n",
    "   evaluation_results = ensemble_evaluator.evaluate_multiple_models(\n",
    "       models_dict, test_dataset\n",
    "   )\n",
    "\n",
    "3Ô∏è‚É£ Generate final summary:\n",
    "   final_summary = evaluation_summary.generate_final_summary(evaluation_results)\n",
    "\n",
    "4Ô∏è‚É£ Save comprehensive report:\n",
    "   evaluation_summary.save_summary_report('./outputs/final_report.txt')\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüéØ EVALUATION FRAMEWORK COMPLETE!\")\n",
    "print(\"=\" * 40)\n",
    "print(\"‚úÖ All evaluation tools implemented and ready to use\")\n",
    "print(\"üìä Quantitative metrics: Dice, IoU, Sensitivity, Specificity, Hausdorff\")\n",
    "print(\"üé® Qualitative visualizations: Predictions, overlays, error maps\")\n",
    "print(\"üìà Statistical analysis: Confidence intervals, significance tests\")\n",
    "print(\"ü§ù Ensemble methods: Averaging, majority voting\")\n",
    "print(\"üè• Clinical assessment: Performance thresholds, deployment readiness\")\n",
    "print(\"üìã Comprehensive reporting: Automated documentation generation\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to:\")\n",
    "print(f\"   üìä Quantitative: {eval_config.results_dir}\")\n",
    "print(f\"   üé® Visualizations: {eval_config.plots_dir}\")\n",
    "print(f\"   ü§ù Ensemble: {ensemble_evaluator.output_dir}\")\n",
    "\n",
    "print(\"\\nüîÑ NEXT STEPS:\")\n",
    "print(\"Execute notebook 07_Postprocessing_and_Morphology.ipynb for morphological operations\")\n",
    "print(\"Execute notebook 08_Final_Inference_and_Results.ipynb for end-to-end inference\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü´Ä CARDIAC SEGMENTATION EVALUATION MODULE - COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
