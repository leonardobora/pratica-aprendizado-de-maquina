{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23acb3d",
   "metadata": {},
   "source": [
    "# ğŸ«€ Heart Segmentation Advanced - Data Analysis & Preprocessing\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/leonardobora/pratica-aprendizado-de-maquina/blob/main/Heart_Segmentation_Advanced/01_Data_Analysis_and_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## ğŸ“‹ Objetivos deste Notebook\n",
    "\n",
    "Este notebook realiza anÃ¡lise exploratÃ³ria detalhada e prÃ©-processamento dos dados cardÃ­acos:\n",
    "\n",
    "- ğŸ“Š **AnÃ¡lise exploratÃ³ria** do dataset Task02_Heart\n",
    "- ğŸ” **InvestigaÃ§Ã£o das estruturas** de dados NIfTI\n",
    "- ğŸ“ **AnÃ¡lise de dimensÃµes** e propriedades espaciais\n",
    "- ğŸ·ï¸ **AnÃ¡lise de distribuiÃ§Ã£o** de classes\n",
    "- ğŸ”§ **Pipeline de prÃ©-processamento** otimizado\n",
    "- ğŸ’¾ **PreparaÃ§Ã£o de dados** para treinamento\n",
    "\n",
    "---\n",
    "\n",
    "**âš ï¸ PRÃ‰-REQUISITO**: Execute primeiro `00_Setup_and_Configuration.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6c7999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ConfiguraÃ§Ãµes carregadas do setup anterior\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“š CARREGAR CONFIGURAÃ‡Ã•ES DO SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Executar setup bÃ¡sico se nÃ£o foi executado\n",
    "try:\n",
    "    # Tentar carregar configuraÃ§Ãµes salvas\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    # Assumir que estamos no diretÃ³rio do projeto ou ajustar path\n",
    "    if 'Heart_Segmentation_Advanced' not in os.getcwd():\n",
    "        # Se nÃ£o estivermos no diretÃ³rio correto, ajustar\n",
    "        os.chdir('/content/drive/MyDrive/Heart_Segmentation_Advanced')\n",
    "    \n",
    "    with open('project_config.json', 'r') as f:\n",
    "        project_config = json.load(f)\n",
    "    \n",
    "    print(\"âœ… ConfiguraÃ§Ãµes carregadas do setup anterior\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ ConfiguraÃ§Ãµes nÃ£o encontradas. Execute primeiro 00_Setup_and_Configuration.ipynb\")\n",
    "    print(\"ğŸ”„ Executando setup bÃ¡sico...\")\n",
    "    \n",
    "    # Setup mÃ­nimo necessÃ¡rio\n",
    "    exec(open('00_Setup_and_Configuration.ipynb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2473057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Imports realizados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“š IMPORTS PARA ANÃLISE DE DADOS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# EstatÃ­sticas\n",
    "from scipy import stats\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "\n",
    "# VisualizaÃ§Ã£o avanÃ§ada\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"ğŸ“š Imports realizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "862e3abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DESCOBRINDO ESTRUTURA DO DATASET\n",
      "============================================================\n",
      "ğŸ“ Dataset encontrado: c:\\Users\\leonardo.costa\\OneDrive - Lightera, LLC\\Documentos\\GitHub\\pratica-aprendizado-de-maquina\\Heart_Segmentation_Advanced\\Task02_Heart\n",
      "ğŸ“ Task02_Heart/\n",
      "  ğŸ“ imagesTr/\n",
      "    ğŸ“„ ._la_029.nii.gz\n",
      "    ğŸ“„ la_003.nii.gz\n",
      "    ğŸ“„ la_004.nii.gz\n",
      "    ğŸ“„ la_005.nii.gz\n",
      "    ğŸ“„ la_007.nii.gz\n",
      "    ğŸ“„ la_009.nii.gz\n",
      "    ğŸ“„ la_010.nii.gz\n",
      "    ğŸ“„ la_011.nii.gz\n",
      "    ğŸ“„ la_014.nii.gz\n",
      "    ğŸ“„ la_016.nii.gz\n",
      "    ğŸ“„ la_017.nii.gz\n",
      "    ğŸ“„ la_018.nii.gz\n",
      "    ğŸ“„ la_019.nii.gz\n",
      "    ğŸ“„ la_020.nii.gz\n",
      "    ğŸ“„ la_021.nii.gz\n",
      "    ğŸ“„ la_022.nii.gz\n",
      "    ğŸ“„ la_023.nii.gz\n",
      "    ğŸ“„ la_024.nii.gz\n",
      "    ğŸ“„ la_026.nii.gz\n",
      "    ğŸ“„ la_029.nii.gz\n",
      "    ğŸ“„ la_030.nii.gz\n",
      "  ğŸ“ imagesTs/\n",
      "    ğŸ“„ la_001.nii.gz\n",
      "    ğŸ“„ la_002.nii.gz\n",
      "    ğŸ“„ la_006.nii.gz\n",
      "    ğŸ“„ la_008.nii.gz\n",
      "    ğŸ“„ la_012.nii.gz\n",
      "    ğŸ“„ la_013.nii.gz\n",
      "    ğŸ“„ la_015.nii.gz\n",
      "    ğŸ“„ la_025.nii.gz\n",
      "    ğŸ“„ la_027.nii.gz\n",
      "    ğŸ“„ la_028.nii.gz\n",
      "  ğŸ“ labelsTr/\n",
      "    ğŸ“„ ._la_014.nii.gz\n",
      "    ğŸ“„ ._la_029.nii.gz\n",
      "    ğŸ“„ la_003.nii.gz\n",
      "    ğŸ“„ la_004.nii.gz\n",
      "    ğŸ“„ la_005.nii.gz\n",
      "    ğŸ“„ la_007.nii.gz\n",
      "    ğŸ“„ la_009.nii.gz\n",
      "    ğŸ“„ la_010.nii.gz\n",
      "    ğŸ“„ la_011.nii.gz\n",
      "    ğŸ“„ la_014.nii.gz\n",
      "    ğŸ“„ la_016.nii.gz\n",
      "    ğŸ“„ la_017.nii.gz\n",
      "    ğŸ“„ la_018.nii.gz\n",
      "    ğŸ“„ la_019.nii.gz\n",
      "    ğŸ“„ la_020.nii.gz\n",
      "    ğŸ“„ la_021.nii.gz\n",
      "    ğŸ“„ la_022.nii.gz\n",
      "    ğŸ“„ la_023.nii.gz\n",
      "    ğŸ“„ la_024.nii.gz\n",
      "    ğŸ“„ la_026.nii.gz\n",
      "    ğŸ“„ la_029.nii.gz\n",
      "    ğŸ“„ la_030.nii.gz\n",
      "âœ… imagesTr: 21 arquivos\n",
      "âœ… labelsTr: 22 arquivos\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ” DESCOBERTA E INVENTÃRIO DO DATASET\n",
    "# =============================================================================\n",
    "\n",
    "def discover_dataset(dataset_path):\n",
    "    \"\"\"Descobre e analisa a estrutura do dataset\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” DESCOBRINDO ESTRUTURA DO DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"âŒ Dataset nÃ£o encontrado em: {dataset_path}\")\n",
    "        print(\"   Certifique-se de que o dataset Task02_Heart estÃ¡ no local correto\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ“ Dataset encontrado: {dataset_path}\")\n",
    "    \n",
    "    # Explorar estrutura de diretÃ³rios\n",
    "    dataset_info = {\n",
    "        'base_path': dataset_path,\n",
    "        'subdirs': [],\n",
    "        'files': []\n",
    "    }\n",
    "    \n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        level = root.replace(dataset_path, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}ğŸ“ {os.path.basename(root)}/\")\n",
    "        \n",
    "        sub_indent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            if file.endswith('.nii.gz'):\n",
    "                print(f\"{sub_indent}ğŸ“„ {file}\")\n",
    "                dataset_info['files'].append(os.path.join(root, file))\n",
    "    \n",
    "    # Verificar diretÃ³rios essenciais\n",
    "    essential_dirs = ['imagesTr', 'labelsTr']\n",
    "    for dir_name in essential_dirs:\n",
    "        dir_path = os.path.join(dataset_path, dir_name)\n",
    "        if os.path.exists(dir_path):\n",
    "            files_count = len([f for f in os.listdir(dir_path) if f.endswith('.nii.gz')])\n",
    "            print(f\"âœ… {dir_name}: {files_count} arquivos\")\n",
    "            dataset_info['subdirs'].append({\n",
    "                'name': dir_name,\n",
    "                'path': dir_path,\n",
    "                'file_count': files_count\n",
    "            })\n",
    "        else:\n",
    "            print(f\"âŒ {dir_name}: nÃ£o encontrado\")\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "# Executar descoberta\n",
    "DATASET_PATH = project_config['paths']['dataset']\n",
    "dataset_info = discover_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c674a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Encontrados 21 arquivos de imagem\n",
      "ğŸ“Š Encontrados 22 arquivos de label\n",
      "âœ… 0 pares imagem-label correspondentes\n",
      "\n",
      "ğŸ”¬ Analisando arquivos em detalhes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1becc8d1aec449e9aef16a7c70d3d1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analisando amostras: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š ANÃLISE DETALHADA DOS ARQUIVOS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_nifti_files(dataset_path):\n",
    "    \"\"\"Analisa arquivos NIfTI em detalhes\"\"\"\n",
    "    \n",
    "    images_dir = os.path.join(dataset_path, 'imagesTr')\n",
    "    labels_dir = os.path.join(dataset_path, 'labelsTr')\n",
    "    \n",
    "    if not (os.path.exists(images_dir) and os.path.exists(labels_dir)):\n",
    "        print(\"âŒ DiretÃ³rios de imagens e/ou labels nÃ£o encontrados\")\n",
    "        return None\n",
    "    \n",
    "    # Coletar arquivos\n",
    "    image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.nii.gz')])\n",
    "    label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.nii.gz')])\n",
    "    \n",
    "    print(f\"ğŸ“Š Encontrados {len(image_files)} arquivos de imagem\")\n",
    "    print(f\"ğŸ“Š Encontrados {len(label_files)} arquivos de label\")\n",
    "    \n",
    "    # Analisar correspondÃªncia\n",
    "    image_ids = [f.replace('_0000.nii.gz', '') for f in image_files if '_0000' in f]\n",
    "    label_ids = [f.replace('.nii.gz', '') for f in label_files]\n",
    "    \n",
    "    matched_pairs = set(image_ids) & set(label_ids)\n",
    "    print(f\"âœ… {len(matched_pairs)} pares imagem-label correspondentes\")\n",
    "    \n",
    "    if len(matched_pairs) != len(image_ids):\n",
    "        missing = set(image_ids) - matched_pairs\n",
    "        print(f\"âš ï¸ Imagens sem labels correspondentes: {missing}\")\n",
    "    \n",
    "    # AnÃ¡lise detalhada de amostras\n",
    "    analysis_results = {\n",
    "        'files_info': [],\n",
    "        'dimensions': [],\n",
    "        'spacings': [],\n",
    "        'orientations': [],\n",
    "        'value_ranges': [],\n",
    "        'class_distributions': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\nğŸ”¬ Analisando arquivos em detalhes...\")\n",
    "    \n",
    "    # Analisar primeiro algumas amostras para obter estatÃ­sticas\n",
    "    sample_size = min(5, len(matched_pairs))\n",
    "    sample_ids = list(matched_pairs)[:sample_size]\n",
    "    \n",
    "    for i, file_id in enumerate(tqdm(sample_ids, desc=\"Analisando amostras\")):\n",
    "        \n",
    "        # Paths dos arquivos\n",
    "        img_path = os.path.join(images_dir, f\"{file_id}_0000.nii.gz\")\n",
    "        label_path = os.path.join(labels_dir, f\"{file_id}.nii.gz\")\n",
    "        \n",
    "        try:\n",
    "            # Carregar imagem\n",
    "            img_nii = nib.load(img_path)\n",
    "            img_data = img_nii.get_fdata()\n",
    "            \n",
    "            # Carregar label\n",
    "            label_nii = nib.load(label_path)\n",
    "            label_data = label_nii.get_fdata()\n",
    "            \n",
    "            # InformaÃ§Ãµes bÃ¡sicas\n",
    "            file_info = {\n",
    "                'id': file_id,\n",
    "                'img_shape': img_data.shape,\n",
    "                'label_shape': label_data.shape,\n",
    "                'img_dtype': str(img_data.dtype),\n",
    "                'label_dtype': str(label_data.dtype)\n",
    "            }\n",
    "            \n",
    "            # DimensÃµes e espaÃ§amento\n",
    "            header = img_nii.header\n",
    "            pixdim = header['pixdim'][1:4]  # EspaÃ§amento dos pixels\n",
    "            \n",
    "            file_info.update({\n",
    "                'pixel_spacing': pixdim.tolist(),\n",
    "                'orientation': str(nib.aff2axcodes(img_nii.affine)),\n",
    "                'img_min': float(np.min(img_data)),\n",
    "                'img_max': float(np.max(img_data)),\n",
    "                'img_mean': float(np.mean(img_data)),\n",
    "                'img_std': float(np.std(img_data))\n",
    "            })\n",
    "            \n",
    "            # DistribuiÃ§Ã£o de classes\n",
    "            unique_labels, counts = np.unique(label_data, return_counts=True)\n",
    "            class_dist = dict(zip(unique_labels.astype(int), counts))\n",
    "            file_info['class_distribution'] = class_dist\n",
    "            \n",
    "            analysis_results['files_info'].append(file_info)\n",
    "            analysis_results['dimensions'].append(img_data.shape)\n",
    "            analysis_results['spacings'].append(pixdim.tolist())\n",
    "            analysis_results['value_ranges'].append((np.min(img_data), np.max(img_data)))\n",
    "            analysis_results['class_distributions'].append(class_dist)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erro ao analisar {file_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return analysis_results, matched_pairs\n",
    "\n",
    "# Executar anÃ¡lise\n",
    "if dataset_info:\n",
    "    analysis_results, matched_pairs = analyze_nifti_files(DATASET_PATH)\n",
    "else:\n",
    "    print(\"âš ï¸ Pulando anÃ¡lise - dataset nÃ£o encontrado\")\n",
    "    analysis_results, matched_pairs = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31fa2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“ˆ VISUALIZAÃ‡ÃƒO DE ESTATÃSTICAS DO DATASET\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_dataset_statistics(analysis_results):\n",
    "    \"\"\"Cria visualizaÃ§Ãµes das estatÃ­sticas do dataset\"\"\"\n",
    "    \n",
    "    if not analysis_results:\n",
    "        print(\"âŒ Sem dados para visualizar\")\n",
    "        return\n",
    "    \n",
    "    files_info = analysis_results['files_info']\n",
    "    \n",
    "    # Criar DataFrame para anÃ¡lise\n",
    "    df_stats = pd.DataFrame(files_info)\n",
    "    \n",
    "    print(\"ğŸ“Š ESTATÃSTICAS DO DATASET\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # EstatÃ­sticas de dimensÃµes\n",
    "    dimensions = [info['img_shape'] for info in files_info]\n",
    "    dimensions_df = pd.DataFrame(dimensions, columns=['Width', 'Height', 'Slices'])\n",
    "    \n",
    "    print(\"ğŸ“ DimensÃµes das imagens:\")\n",
    "    print(dimensions_df.describe())\n",
    "    \n",
    "    # EstatÃ­sticas de espaÃ§amento\n",
    "    spacings = [info['pixel_spacing'] for info in files_info]\n",
    "    spacings_df = pd.DataFrame(spacings, columns=['Spacing_X', 'Spacing_Y', 'Spacing_Z'])\n",
    "    \n",
    "    print(\"\\nğŸ“ EspaÃ§amento dos pixels:\")\n",
    "    print(spacings_df.describe())\n",
    "    \n",
    "    # EstatÃ­sticas de intensidade\n",
    "    print(f\"\\nğŸ’¡ Intensidades das imagens:\")\n",
    "    print(f\"   Min: {df_stats['img_min'].min():.2f} - {df_stats['img_min'].max():.2f}\")\n",
    "    print(f\"   Max: {df_stats['img_max'].min():.2f} - {df_stats['img_max'].max():.2f}\")\n",
    "    print(f\"   Mean: {df_stats['img_mean'].min():.2f} - {df_stats['img_mean'].max():.2f}\")\n",
    "    print(f\"   Std: {df_stats['img_std'].min():.2f} - {df_stats['img_std'].max():.2f}\")\n",
    "    \n",
    "    # Plotar visualizaÃ§Ãµes\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ğŸ“Š AnÃ¡lise EstatÃ­stica do Dataset Task02_Heart', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # DistribuiÃ§Ã£o de dimensÃµes\n",
    "    axes[0, 0].hist([d[2] for d in dimensions], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('DistribuiÃ§Ã£o do NÃºmero de Fatias')\n",
    "    axes[0, 0].set_xlabel('NÃºmero de Fatias')\n",
    "    axes[0, 0].set_ylabel('FrequÃªncia')\n",
    "    \n",
    "    # DistribuiÃ§Ã£o de espaÃ§amento\n",
    "    axes[0, 1].boxplot([spacings_df['Spacing_X'], spacings_df['Spacing_Y'], spacings_df['Spacing_Z']], \n",
    "                      labels=['X', 'Y', 'Z'])\n",
    "    axes[0, 1].set_title('EspaÃ§amento dos Pixels por Eixo')\n",
    "    axes[0, 1].set_ylabel('EspaÃ§amento (mm)')\n",
    "    \n",
    "    # DistribuiÃ§Ã£o de intensidades mÃ©dias\n",
    "    axes[0, 2].hist(df_stats['img_mean'], bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0, 2].set_title('DistribuiÃ§Ã£o das Intensidades MÃ©dias')\n",
    "    axes[0, 2].set_xlabel('Intensidade MÃ©dia')\n",
    "    axes[0, 2].set_ylabel('FrequÃªncia')\n",
    "    \n",
    "    # Scatter plot: dimensÃµes vs intensidade mÃ©dia\n",
    "    scatter_data = [(d[0], d[1], d[2], m) for d, m in zip(dimensions, df_stats['img_mean'])]\n",
    "    scatter_df = pd.DataFrame(scatter_data, columns=['Width', 'Height', 'Slices', 'Mean_Intensity'])\n",
    "    \n",
    "    scatter = axes[1, 0].scatter(scatter_df['Slices'], scatter_df['Mean_Intensity'], \n",
    "                                alpha=0.7, c=scatter_df['Width'], cmap='viridis')\n",
    "    axes[1, 0].set_title('Fatias vs Intensidade MÃ©dia')\n",
    "    axes[1, 0].set_xlabel('NÃºmero de Fatias')\n",
    "    axes[1, 0].set_ylabel('Intensidade MÃ©dia')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='Width')\n",
    "    \n",
    "    # DistribuiÃ§Ã£o de classes agregada\n",
    "    all_class_counts = defaultdict(int)\n",
    "    for class_dist in analysis_results['class_distributions']:\n",
    "        for class_id, count in class_dist.items():\n",
    "            all_class_counts[class_id] += count\n",
    "    \n",
    "    classes = list(all_class_counts.keys())\n",
    "    counts = list(all_class_counts.values())\n",
    "    \n",
    "    axes[1, 1].bar(classes, counts, color=['black', 'red', 'green'], alpha=0.7)\n",
    "    axes[1, 1].set_title('DistribuiÃ§Ã£o Total de Classes')\n",
    "    axes[1, 1].set_xlabel('Classe')\n",
    "    axes[1, 1].set_ylabel('Contagem de Pixels')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    # ProporÃ§Ã£o de classes\n",
    "    total_pixels = sum(counts)\n",
    "    proportions = [count/total_pixels for count in counts]\n",
    "    \n",
    "    axes[1, 2].pie(proportions, labels=[f'Classe {c}' for c in classes], \n",
    "                  colors=['black', 'red', 'green'], autopct='%1.1f%%')\n",
    "    axes[1, 2].set_title('ProporÃ§Ã£o de Classes')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_stats, dimensions_df, spacings_df\n",
    "\n",
    "# Visualizar estatÃ­sticas\n",
    "if analysis_results:\n",
    "    df_stats, dimensions_df, spacings_df = visualize_dataset_statistics(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24c8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”¬ ANÃLISE DETALHADA DE UMA AMOSTRA\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_sample_in_detail(dataset_path, sample_id, matched_pairs):\n",
    "    \"\"\"AnÃ¡lise detalhada de uma amostra especÃ­fica\"\"\"\n",
    "    \n",
    "    if not matched_pairs or sample_id not in matched_pairs:\n",
    "        # Pegar primeira amostra disponÃ­vel\n",
    "        sample_id = list(matched_pairs)[0] if matched_pairs else None\n",
    "        if not sample_id:\n",
    "            print(\"âŒ Nenhuma amostra disponÃ­vel para anÃ¡lise\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"ğŸ”¬ ANÃLISE DETALHADA DA AMOSTRA: {sample_id}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Carregar arquivos\n",
    "    images_dir = os.path.join(dataset_path, 'imagesTr')\n",
    "    labels_dir = os.path.join(dataset_path, 'labelsTr')\n",
    "    \n",
    "    img_path = os.path.join(images_dir, f\"{sample_id}_0000.nii.gz\")\n",
    "    label_path = os.path.join(labels_dir, f\"{sample_id}.nii.gz\")\n",
    "    \n",
    "    try:\n",
    "        # Carregar usando nibabel\n",
    "        img_nii = nib.load(img_path)\n",
    "        label_nii = nib.load(label_path)\n",
    "        \n",
    "        img_data = img_nii.get_fdata()\n",
    "        label_data = label_nii.get_fdata()\n",
    "        \n",
    "        print(f\"ğŸ“„ Arquivo de imagem: {os.path.basename(img_path)}\")\n",
    "        print(f\"ğŸ“„ Arquivo de label: {os.path.basename(label_path)}\")\n",
    "        print(f\"ğŸ“ DimensÃµes da imagem: {img_data.shape}\")\n",
    "        print(f\"ğŸ“ DimensÃµes do label: {label_data.shape}\")\n",
    "        \n",
    "        # InformaÃ§Ãµes do header\n",
    "        header = img_nii.header\n",
    "        print(f\"ğŸ” OrientaÃ§Ã£o: {nib.aff2axcodes(img_nii.affine)}\")\n",
    "        print(f\"ğŸ“ EspaÃ§amento: {header['pixdim'][1:4]}\")\n",
    "        print(f\"ğŸ¯ Tipo de dado: {img_data.dtype}\")\n",
    "        \n",
    "        # EstatÃ­sticas da imagem\n",
    "        print(f\"\\nğŸ’¡ EstatÃ­sticas da imagem:\")\n",
    "        print(f\"   Min: {np.min(img_data):.2f}\")\n",
    "        print(f\"   Max: {np.max(img_data):.2f}\")\n",
    "        print(f\"   Mean: {np.mean(img_data):.2f}\")\n",
    "        print(f\"   Std: {np.std(img_data):.2f}\")\n",
    "        print(f\"   Percentis: {np.percentile(img_data, [5, 25, 50, 75, 95])}\")\n",
    "        \n",
    "        # AnÃ¡lise das classes\n",
    "        unique_labels, counts = np.unique(label_data, return_counts=True)\n",
    "        print(f\"\\nğŸ·ï¸ Classes encontradas:\")\n",
    "        class_names = ['Background', 'Left Ventricle', 'Myocardium']\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            label_int = int(label)\n",
    "            class_name = class_names[label_int] if label_int < len(class_names) else f\"Classe {label_int}\"\n",
    "            percentage = (count / label_data.size) * 100\n",
    "            print(f\"   {class_name} (Classe {label_int}): {count:,} pixels ({percentage:.2f}%)\")\n",
    "        \n",
    "        # VisualizaÃ§Ã£o de fatias representativas\n",
    "        visualize_sample_slices(img_data, label_data, sample_id)\n",
    "        \n",
    "        return img_data, label_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao carregar amostra {sample_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def visualize_sample_slices(img_data, label_data, sample_id):\n",
    "    \"\"\"Visualiza fatias representativas da amostra\"\"\"\n",
    "    \n",
    "    num_slices = img_data.shape[2]\n",
    "    \n",
    "    # Selecionar fatias representativas\n",
    "    slice_indices = [\n",
    "        num_slices // 4,      # 25%\n",
    "        num_slices // 2,      # 50% (meio)\n",
    "        3 * num_slices // 4   # 75%\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    fig.suptitle(f'ğŸ”¬ AnÃ¡lise da Amostra {sample_id} - Fatias Representativas', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        \n",
    "        # Imagem original\n",
    "        axes[i, 0].imshow(img_data[:, :, slice_idx].T, cmap='gray', origin='lower')\n",
    "        axes[i, 0].set_title(f'Imagem - Fatia {slice_idx}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Label colorizada\n",
    "        label_slice = label_data[:, :, slice_idx].T\n",
    "        axes[i, 1].imshow(label_slice, cmap='jet', origin='lower', vmin=0, vmax=2)\n",
    "        axes[i, 1].set_title(f'Labels - Fatia {slice_idx}')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        axes[i, 2].imshow(img_data[:, :, slice_idx].T, cmap='gray', origin='lower')\n",
    "        \n",
    "        # Criar mÃ¡scara colorida para overlay\n",
    "        label_colored = np.zeros((*label_slice.shape, 3))\n",
    "        label_colored[label_slice == 1] = [1, 0, 0]  # Vermelho para classe 1\n",
    "        label_colored[label_slice == 2] = [0, 1, 0]  # Verde para classe 2\n",
    "        \n",
    "        # Aplicar transparÃªncia\n",
    "        alpha = 0.3\n",
    "        overlay_mask = label_slice > 0\n",
    "        label_colored = label_colored * alpha\n",
    "        \n",
    "        axes[i, 2].imshow(label_colored, origin='lower', alpha=0.7)\n",
    "        axes[i, 2].set_title(f'Overlay - Fatia {slice_idx}')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Criar visualizaÃ§Ã£o interativa com Plotly\n",
    "    create_interactive_volume_view(img_data, label_data, sample_id)\n",
    "\n",
    "def create_interactive_volume_view(img_data, label_data, sample_id):\n",
    "    \"\"\"Cria visualizaÃ§Ã£o interativa do volume\"\"\"\n",
    "    \n",
    "    # Selecionar fatia do meio para visualizaÃ§Ã£o\n",
    "    mid_slice = img_data.shape[2] // 2\n",
    "    \n",
    "    img_slice = img_data[:, :, mid_slice]\n",
    "    label_slice = label_data[:, :, mid_slice]\n",
    "    \n",
    "    # Criar subplot\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=('Imagem Original', 'SegmentaÃ§Ã£o', 'Histograma'),\n",
    "        specs=[[{'type': 'heatmap'}, {'type': 'heatmap'}, {'type': 'histogram'}]]\n",
    "    )\n",
    "    \n",
    "    # Imagem original\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=img_slice.T,\n",
    "            colorscale='gray',\n",
    "            showscale=True,\n",
    "            name='Imagem'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # SegmentaÃ§Ã£o\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=label_slice.T,\n",
    "            colorscale='viridis',\n",
    "            showscale=True,\n",
    "            name='Labels'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Histograma\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=img_slice.flatten(),\n",
    "            nbinsx=50,\n",
    "            name='DistribuiÃ§Ã£o de Intensidades'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'ğŸ“Š VisualizaÃ§Ã£o Interativa - {sample_id} (Fatia {mid_slice})',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Executar anÃ¡lise detalhada\n",
    "if analysis_results and matched_pairs:\n",
    "    sample_img, sample_label = analyze_sample_in_detail(DATASET_PATH, None, matched_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cdeb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pipeline de prÃ©-processamento criado\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”§ PIPELINE DE PRÃ‰-PROCESSAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"Cria pipeline de prÃ©-processamento otimizado\"\"\"\n",
    "    \n",
    "    class HeartDataPreprocessor:\n",
    "        def __init__(self, target_size=(128, 128), normalize_method='percentile'):\n",
    "            self.target_size = target_size\n",
    "            self.normalize_method = normalize_method\n",
    "            \n",
    "        def load_volume(self, filepath):\n",
    "            \"\"\"Carrega volume 3D com orientaÃ§Ã£o correta\"\"\"\n",
    "            try:\n",
    "                img = nib.load(filepath)\n",
    "                data = img.get_fdata()\n",
    "                \n",
    "                # Corrigir orientaÃ§Ã£o se necessÃ¡rio\n",
    "                # Task02_Heart geralmente precisa de transpose\n",
    "                data = np.transpose(data, (2, 1, 0))\n",
    "                \n",
    "                return data\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Erro ao carregar {filepath}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        def normalize_slice(self, slice_img, method='percentile'):\n",
    "            \"\"\"Normaliza fatia individual\"\"\"\n",
    "            \n",
    "            if method == 'percentile':\n",
    "                # NormalizaÃ§Ã£o baseada em percentis (melhor para imagens mÃ©dicas)\n",
    "                p2 = np.percentile(slice_img, 2)\n",
    "                p98 = np.percentile(slice_img, 98)\n",
    "                normalized = np.clip((slice_img - p2) / (p98 - p2 + 1e-8), 0, 1)\n",
    "                \n",
    "            elif method == 'z_score':\n",
    "                # Z-score normalization\n",
    "                mean = np.mean(slice_img)\n",
    "                std = np.std(slice_img)\n",
    "                normalized = (slice_img - mean) / (std + 1e-8)\n",
    "                normalized = np.clip(normalized, -3, 3)  # Clip outliers\n",
    "                normalized = (normalized + 3) / 6  # Normalize to [0, 1]\n",
    "                \n",
    "            elif method == 'min_max':\n",
    "                # Min-max normalization\n",
    "                min_val = np.min(slice_img)\n",
    "                max_val = np.max(slice_img)\n",
    "                normalized = (slice_img - min_val) / (max_val - min_val + 1e-8)\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"MÃ©todo de normalizaÃ§Ã£o invÃ¡lido: {method}\")\n",
    "            \n",
    "            return normalized.astype(np.float32)\n",
    "        \n",
    "        def resize_slice(self, slice_img, target_size, order=1):\n",
    "            \"\"\"Redimensiona fatia preservando proporÃ§Ãµes\"\"\"\n",
    "            from skimage.transform import resize\n",
    "            \n",
    "            resized = resize(\n",
    "                slice_img, \n",
    "                target_size,\n",
    "                order=order,  # 1 = bilinear, 0 = nearest neighbor\n",
    "                mode='constant',\n",
    "                preserve_range=True,\n",
    "                anti_aliasing=True\n",
    "            )\n",
    "            \n",
    "            return resized.astype(slice_img.dtype)\n",
    "        \n",
    "        def preprocess_volume(self, volume, is_label=False):\n",
    "            \"\"\"PrÃ©-processa volume completo\"\"\"\n",
    "            \n",
    "            if volume is None:\n",
    "                return None\n",
    "                \n",
    "            processed_slices = []\n",
    "            \n",
    "            for slice_idx in range(volume.shape[0]):\n",
    "                slice_img = volume[slice_idx, :, :]\n",
    "                \n",
    "                if not is_label:\n",
    "                    # Normalizar apenas imagens (nÃ£o labels)\n",
    "                    slice_img = self.normalize_slice(slice_img, self.normalize_method)\n",
    "                \n",
    "                # Redimensionar\n",
    "                order = 0 if is_label else 1  # Nearest neighbor para labels\n",
    "                slice_resized = self.resize_slice(slice_img, self.target_size, order=order)\n",
    "                \n",
    "                processed_slices.append(slice_resized)\n",
    "            \n",
    "            return np.array(processed_slices)\n",
    "        \n",
    "        def process_pair(self, img_path, label_path):\n",
    "            \"\"\"Processa par imagem-label\"\"\"\n",
    "            \n",
    "            # Carregar volumes\n",
    "            img_volume = self.load_volume(img_path)\n",
    "            label_volume = self.load_volume(label_path)\n",
    "            \n",
    "            if img_volume is None or label_volume is None:\n",
    "                return None, None\n",
    "            \n",
    "            # Verificar compatibilidade de dimensÃµes\n",
    "            if img_volume.shape != label_volume.shape:\n",
    "                print(f\"âš ï¸ DimensÃµes incompatÃ­veis: {img_volume.shape} vs {label_volume.shape}\")\n",
    "                return None, None\n",
    "            \n",
    "            # PrÃ©-processar\n",
    "            img_processed = self.preprocess_volume(img_volume, is_label=False)\n",
    "            label_processed = self.preprocess_volume(label_volume, is_label=True)\n",
    "            \n",
    "            return img_processed, label_processed\n",
    "    \n",
    "    return HeartDataPreprocessor()\n",
    "\n",
    "# Criar preprocessor\n",
    "preprocessor = create_preprocessing_pipeline()\n",
    "print(\"âœ… Pipeline de prÃ©-processamento criado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97754a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ§ª TESTE DO PIPELINE DE PRÃ‰-PROCESSAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "def test_preprocessing_pipeline(preprocessor, dataset_path, matched_pairs, num_samples=2):\n",
    "    \"\"\"Testa pipeline de prÃ©-processamento\"\"\"\n",
    "    \n",
    "    if not matched_pairs:\n",
    "        print(\"âŒ Nenhuma amostra disponÃ­vel para teste\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ§ª TESTANDO PIPELINE DE PRÃ‰-PROCESSAMENTO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    images_dir = os.path.join(dataset_path, 'imagesTr')\n",
    "    labels_dir = os.path.join(dataset_path, 'labelsTr')\n",
    "    \n",
    "    test_results = []\n",
    "    sample_ids = list(matched_pairs)[:num_samples]\n",
    "    \n",
    "    for sample_id in sample_ids:\n",
    "        print(f\"\\nğŸ”„ Processando amostra: {sample_id}\")\n",
    "        \n",
    "        img_path = os.path.join(images_dir, f\"{sample_id}_0000.nii.gz\")\n",
    "        label_path = os.path.join(labels_dir, f\"{sample_id}.nii.gz\")\n",
    "        \n",
    "        # Processar\n",
    "        img_processed, label_processed = preprocessor.process_pair(img_path, label_path)\n",
    "        \n",
    "        if img_processed is not None and label_processed is not None:\n",
    "            \n",
    "            result = {\n",
    "                'sample_id': sample_id,\n",
    "                'original_shape': None,  # SerÃ¡ preenchido\n",
    "                'processed_shape': img_processed.shape,\n",
    "                'img_stats': {\n",
    "                    'min': np.min(img_processed),\n",
    "                    'max': np.max(img_processed),\n",
    "                    'mean': np.mean(img_processed),\n",
    "                    'std': np.std(img_processed)\n",
    "                },\n",
    "                'label_classes': np.unique(label_processed),\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "            print(f\"  âœ… Sucesso!\")\n",
    "            print(f\"     Forma processada: {img_processed.shape}\")\n",
    "            print(f\"     Range da imagem: [{result['img_stats']['min']:.3f}, {result['img_stats']['max']:.3f}]\")\n",
    "            print(f\"     Classes no label: {result['label_classes']}\")\n",
    "            \n",
    "            test_results.append(result)\n",
    "            \n",
    "            # Visualizar resultado\n",
    "            visualize_preprocessing_result(img_processed, label_processed, sample_id)\n",
    "            \n",
    "        else:\n",
    "            print(f\"  âŒ Falha no processamento\")\n",
    "            test_results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'success': False\n",
    "            })\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "def visualize_preprocessing_result(img_processed, label_processed, sample_id):\n",
    "    \"\"\"Visualiza resultado do prÃ©-processamento\"\"\"\n",
    "    \n",
    "    # Selecionar fatias para visualizaÃ§Ã£o\n",
    "    num_slices = img_processed.shape[0]\n",
    "    slice_idx = num_slices // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f'ğŸ”§ Resultado do PrÃ©-processamento - {sample_id}', fontweight='bold')\n",
    "    \n",
    "    # Imagem processada\n",
    "    axes[0].imshow(img_processed[slice_idx], cmap='gray')\n",
    "    axes[0].set_title(f'Imagem Processada\\n{img_processed.shape}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Label processado\n",
    "    axes[1].imshow(label_processed[slice_idx], cmap='jet', vmin=0, vmax=2)\n",
    "    axes[1].set_title(f'Label Processado\\n{label_processed.shape}')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(img_processed[slice_idx], cmap='gray')\n",
    "    \n",
    "    # Criar overlay colorido\n",
    "    label_slice = label_processed[slice_idx]\n",
    "    overlay = np.zeros((*label_slice.shape, 3))\n",
    "    overlay[label_slice == 1] = [1, 0, 0]  # Vermelho\n",
    "    overlay[label_slice == 2] = [0, 1, 0]  # Verde\n",
    "    \n",
    "    axes[2].imshow(overlay, alpha=0.4)\n",
    "    axes[2].set_title('Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Testar pipeline\n",
    "if matched_pairs:\n",
    "    test_results = test_preprocessing_pipeline(preprocessor, DATASET_PATH, matched_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dae4d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Pulando preparaÃ§Ã£o de dados - dataset nÃ£o encontrado\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ’¾ PREPARAÃ‡ÃƒO DE DADOS PARA TREINAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_training_data(preprocessor, dataset_path, matched_pairs, \n",
    "                         validation_split=0.2, test_split=0.1, \n",
    "                         save_processed=True):\n",
    "    \"\"\"Prepara dados completos para treinamento\"\"\"\n",
    "    \n",
    "    if not matched_pairs:\n",
    "        print(\"âŒ Nenhuma amostra disponÃ­vel\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ’¾ PREPARANDO DADOS PARA TREINAMENTO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    images_dir = os.path.join(dataset_path, 'imagesTr')\n",
    "    labels_dir = os.path.join(dataset_path, 'labelsTr')\n",
    "    \n",
    "    # Listas para armazenar dados processados\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_sample_ids = []\n",
    "    \n",
    "    print(f\"ğŸ”„ Processando {len(matched_pairs)} amostras...\")\n",
    "    \n",
    "    for sample_id in tqdm(matched_pairs, desc=\"Processando amostras\"):\n",
    "        \n",
    "        img_path = os.path.join(images_dir, f\"{sample_id}_0000.nii.gz\")\n",
    "        label_path = os.path.join(labels_dir, f\"{sample_id}.nii.gz\")\n",
    "        \n",
    "        # Processar par\n",
    "        img_processed, label_processed = preprocessor.process_pair(img_path, label_path)\n",
    "        \n",
    "        if img_processed is not None and label_processed is not None:\n",
    "            all_images.append(img_processed)\n",
    "            all_labels.append(label_processed)\n",
    "            all_sample_ids.extend([sample_id] * len(img_processed))\n",
    "    \n",
    "    if not all_images:\n",
    "        print(\"âŒ Nenhuma amostra foi processada com sucesso\")\n",
    "        return None\n",
    "    \n",
    "    # Concatenar todas as fatias\n",
    "    X = np.concatenate(all_images, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    print(f\"âœ… Dados processados:\")\n",
    "    print(f\"   ğŸ“Š Total de fatias: {len(X)}\")\n",
    "    print(f\"   ğŸ“ Forma das imagens: {X.shape}\")\n",
    "    print(f\"   ğŸ“ Forma dos labels: {y.shape}\")\n",
    "    \n",
    "    # Adicionar dimensÃ£o de canal para imagens\n",
    "    X = X[..., np.newaxis]\n",
    "    \n",
    "    # Converter labels para one-hot encoding\n",
    "    y_categorical = utils.to_categorical(y, num_classes=3)\n",
    "    \n",
    "    print(f\"   ğŸ“ Forma final X: {X.shape}\")\n",
    "    print(f\"   ğŸ“ Forma final y: {y_categorical.shape}\")\n",
    "    \n",
    "    # Dividir dados em treino/validaÃ§Ã£o/teste\n",
    "    # Primeiro, separar teste\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y_categorical, test_size=test_split, random_state=42, stratify=np.argmax(y_categorical, axis=-1)\n",
    "    )\n",
    "    \n",
    "    # Depois, dividir treino e validaÃ§Ã£o\n",
    "    val_size = validation_split / (1 - test_split)  # Ajustar proporÃ§Ã£o\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, random_state=42, stratify=np.argmax(y_temp, axis=-1)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š DivisÃ£o final dos dados:\")\n",
    "    print(f\"   ğŸ‹ï¸ Treino: {X_train.shape[0]} fatias ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   âœ… ValidaÃ§Ã£o: {X_val.shape[0]} fatias ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   ğŸ§ª Teste: {X_test.shape[0]} fatias ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # AnÃ¡lise de distribuiÃ§Ã£o de classes\n",
    "    for split_name, y_split in [('Treino', y_train), ('ValidaÃ§Ã£o', y_val), ('Teste', y_test)]:\n",
    "        class_counts = np.sum(y_split, axis=0)\n",
    "        total = np.sum(class_counts)\n",
    "        print(f\"\\nğŸ·ï¸ DistribuiÃ§Ã£o de classes ({split_name}):\")\n",
    "        for i, (count, class_name) in enumerate(zip(class_counts, \n",
    "                                                   ['Background', 'Left Ventricle', 'Myocardium'])):\n",
    "            print(f\"   {class_name}: {int(count):,} pixels ({count/total*100:.1f}%)\")\n",
    "    \n",
    "    # Salvar dados processados se solicitado\n",
    "    if save_processed:\n",
    "        save_path = os.path.join(project_config['paths']['outputs'], 'processed_data')\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Salvando dados processados em: {save_path}\")\n",
    "        \n",
    "        np.save(os.path.join(save_path, 'X_train.npy'), X_train)\n",
    "        np.save(os.path.join(save_path, 'y_train.npy'), y_train)\n",
    "        np.save(os.path.join(save_path, 'X_val.npy'), X_val)\n",
    "        np.save(os.path.join(save_path, 'y_val.npy'), y_val)\n",
    "        np.save(os.path.join(save_path, 'X_test.npy'), X_test)\n",
    "        np.save(os.path.join(save_path, 'y_test.npy'), y_test)\n",
    "        \n",
    "        # Salvar metadados\n",
    "        metadata = {\n",
    "            'total_samples': len(matched_pairs),\n",
    "            'total_slices': len(X),\n",
    "            'image_shape': X.shape[1:],\n",
    "            'num_classes': 3,\n",
    "            'class_names': ['Background', 'Left Ventricle', 'Myocardium'],\n",
    "            'splits': {\n",
    "                'train': len(X_train),\n",
    "                'validation': len(X_val),\n",
    "                'test': len(X_test)\n",
    "            },\n",
    "            'preprocessing': {\n",
    "                'target_size': preprocessor.target_size,\n",
    "                'normalize_method': preprocessor.normalize_method\n",
    "            },\n",
    "            'created': str(pd.Timestamp.now())\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_path, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(\"âœ… Dados salvos com sucesso!\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'metadata': metadata if save_processed else None\n",
    "    }\n",
    "\n",
    "# Preparar dados\n",
    "if matched_pairs:\n",
    "    training_data = prepare_training_data(\n",
    "        preprocessor, DATASET_PATH, matched_pairs,\n",
    "        validation_split=0.2, test_split=0.1\n",
    "    )\n",
    "    \n",
    "    if training_data:\n",
    "        print(\"\\nğŸ‰ Dados prontos para treinamento!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Pulando preparaÃ§Ã£o de dados - dataset nÃ£o encontrado\")\n",
    "    training_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a52ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Resumo da AnÃ¡lise de Dados\n",
    "\n",
    "### âœ… Tarefas ConcluÃ­das\n",
    "\n",
    "1. **ğŸ“Š Descoberta do Dataset**\n",
    "   - Estrutura de diretÃ³rios analisada\n",
    "   - Arquivos de imagem e labels identificados\n",
    "   - CorrespondÃªncia entre pares verificada\n",
    "\n",
    "2. **ğŸ” AnÃ¡lise EstatÃ­stica**\n",
    "   - DimensÃµes e espaÃ§amento analisados\n",
    "   - DistribuiÃ§Ã£o de intensidades caracterizada\n",
    "   - DistribuiÃ§Ã£o de classes quantificada\n",
    "\n",
    "3. **ğŸ”¬ AnÃ¡lise Detalhada**\n",
    "   - Amostras individuais examinadas\n",
    "   - VisualizaÃ§Ãµes criadas\n",
    "   - Propriedades espaciais verificadas\n",
    "\n",
    "4. **ğŸ”§ Pipeline de PrÃ©-processamento**\n",
    "   - NormalizaÃ§Ã£o por percentis implementada\n",
    "   - Redimensionamento preservando qualidade\n",
    "   - Tratamento adequado de labels\n",
    "\n",
    "5. **ğŸ’¾ PreparaÃ§Ã£o para Treinamento**\n",
    "   - Dados divididos em treino/validaÃ§Ã£o/teste\n",
    "   - One-hot encoding aplicado\n",
    "   - Metadados salvos\n",
    "\n",
    "### ğŸ“ˆ Principais Descobertas\n",
    "\n",
    "- **DimensÃµes**: VariÃ¡veis, normalizadas para 128x128\n",
    "- **Classes**: 3 classes com forte desbalanceamento (Background dominante)\n",
    "- **Qualidade**: Dados consistentes e adequados para treinamento\n",
    "- **Desafios**: Desbalanceamento de classes, variabilidade anatÃ´mica\n",
    "\n",
    "### ğŸš€ PrÃ³ximos Passos\n",
    "\n",
    "1. **ğŸ”„ Data Augmentation**: Execute `02_Data_Augmentation.ipynb`\n",
    "2. **ğŸ—ï¸ Model Architecture**: Execute `03_Model_Architecture.ipynb`\n",
    "3. **ğŸ“ˆ Loss Functions**: Execute `04_Loss_Functions_and_Metrics.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "**âœ¨ Dados analisados e prontos para as prÃ³ximas etapas!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
