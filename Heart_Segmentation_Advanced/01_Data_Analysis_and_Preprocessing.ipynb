{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23acb3d",
   "metadata": {},
   "source": [
    "# 🫀 Heart Segmentation Advanced - Data Analysis & Preprocessing\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/leonardobora/pratica-aprendizado-de-maquina/blob/main/Heart_Segmentation_Advanced/01_Data_Analysis_and_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## 📋 Objetivos deste Notebook\n",
    "\n",
    "Este notebook realiza análise exploratória detalhada e pré-processamento dos dados cardíacos:\n",
    "\n",
    "- 📊 **Análise exploratória** do dataset Task02_Heart\n",
    "- 🔍 **Investigação das estruturas** de dados NIfTI\n",
    "- 📐 **Análise de dimensões** e propriedades espaciais\n",
    "- 🏷️ **Análise de distribuição** de classes\n",
    "- 🔧 **Pipeline de pré-processamento** otimizado\n",
    "- 💾 **Preparação de dados** para treinamento\n",
    "\n",
    "---\n",
    "\n",
    "**⚠️ PRÉ-REQUISITO**: Execute primeiro `00_Setup_and_Configuration.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6c7999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configurações carregadas do setup anterior\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 📚 CARREGAR CONFIGURAÇÕES DO SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Executar setup básico se não foi executado\n",
    "try:\n",
    "    # Tentar carregar configurações salvas\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    # Assumir que estamos no diretório do projeto ou ajustar path\n",
    "    if 'Heart_Segmentation_Advanced' not in os.getcwd():\n",
    "        # Se não estivermos no diretório correto, ajustar\n",
    "        os.chdir('/content/drive/MyDrive/Heart_Segmentation_Advanced')\n",
    "    \n",
    "    with open('project_config.json', 'r') as f:\n",
    "        project_config = json.load(f)\n",
    "    \n",
    "    print(\"✅ Configurações carregadas do setup anterior\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Configurações não encontradas. Execute primeiro 00_Setup_and_Configuration.ipynb\")\n",
    "    print(\"🔄 Executando setup básico...\")\n",
    "    \n",
    "    # Setup mínimo necessário\n",
    "    exec(open('00_Setup_and_Configuration.ipynb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2473057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Imports realizados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 📚 IMPORTS PARA ANÁLISE DE DADOS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Estatísticas\n",
    "from scipy import stats\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "\n",
    "# Visualização avançada\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"📚 Imports realizados com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "862e3abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DESCOBRINDO ESTRUTURA DO DATASET\n",
      "============================================================\n",
      "📁 Dataset encontrado: c:\\Users\\leonardo.costa\\OneDrive - Lightera, LLC\\Documentos\\GitHub\\pratica-aprendizado-de-maquina\\Heart_Segmentation_Advanced\\Task02_Heart\n",
      "📁 Task02_Heart/\n",
      "  📁 imagesTr/\n",
      "    📄 ._la_029.nii.gz\n",
      "    📄 la_003.nii.gz\n",
      "    📄 la_004.nii.gz\n",
      "    📄 la_005.nii.gz\n",
      "    📄 la_007.nii.gz\n",
      "    📄 la_009.nii.gz\n",
      "    📄 la_010.nii.gz\n",
      "    📄 la_011.nii.gz\n",
      "    📄 la_014.nii.gz\n",
      "    📄 la_016.nii.gz\n",
      "    📄 la_017.nii.gz\n",
      "    📄 la_018.nii.gz\n",
      "    📄 la_019.nii.gz\n",
      "    📄 la_020.nii.gz\n",
      "    📄 la_021.nii.gz\n",
      "    📄 la_022.nii.gz\n",
      "    📄 la_023.nii.gz\n",
      "    📄 la_024.nii.gz\n",
      "    📄 la_026.nii.gz\n",
      "    📄 la_029.nii.gz\n",
      "    📄 la_030.nii.gz\n",
      "  📁 imagesTs/\n",
      "    📄 la_001.nii.gz\n",
      "    📄 la_002.nii.gz\n",
      "    📄 la_006.nii.gz\n",
      "    📄 la_008.nii.gz\n",
      "    📄 la_012.nii.gz\n",
      "    📄 la_013.nii.gz\n",
      "    📄 la_015.nii.gz\n",
      "    📄 la_025.nii.gz\n",
      "    📄 la_027.nii.gz\n",
      "    📄 la_028.nii.gz\n",
      "  📁 labelsTr/\n",
      "    📄 ._la_014.nii.gz\n",
      "    📄 ._la_029.nii.gz\n",
      "    📄 la_003.nii.gz\n",
      "    📄 la_004.nii.gz\n",
      "    📄 la_005.nii.gz\n",
      "    📄 la_007.nii.gz\n",
      "    📄 la_009.nii.gz\n",
      "    📄 la_010.nii.gz\n",
      "    📄 la_011.nii.gz\n",
      "    📄 la_014.nii.gz\n",
      "    📄 la_016.nii.gz\n",
      "    📄 la_017.nii.gz\n",
      "    📄 la_018.nii.gz\n",
      "    📄 la_019.nii.gz\n",
      "    📄 la_020.nii.gz\n",
      "    📄 la_021.nii.gz\n",
      "    📄 la_022.nii.gz\n",
      "    📄 la_023.nii.gz\n",
      "    📄 la_024.nii.gz\n",
      "    📄 la_026.nii.gz\n",
      "    📄 la_029.nii.gz\n",
      "    📄 la_030.nii.gz\n",
      "✅ imagesTr: 21 arquivos\n",
      "✅ labelsTr: 22 arquivos\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 🔍 DESCOBERTA E INVENTÁRIO DO DATASET\n",
    "# =============================================================================\n",
    "\n",
    "def discover_dataset(dataset_path):\n",
    "    \"\"\"Descobre e analisa a estrutura do dataset\"\"\"\n",
    "    \n",
    "    print(\"🔍 DESCOBRINDO ESTRUTURA DO DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"❌ Dataset não encontrado em: {dataset_path}\")\n",
    "        print(\"   Certifique-se de que o dataset Task02_Heart está no local correto\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📁 Dataset encontrado: {dataset_path}\")\n",
    "    \n",
    "    # Explorar estrutura de diretórios\n",
    "    dataset_info = {\n",
    "        'base_path': dataset_path,\n",
    "        'subdirs': [],\n",
    "        'files': []\n",
    "    }\n",
    "    \n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        level = root.replace(dataset_path, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}📁 {os.path.basename(root)}/\")\n",
    "        \n",
    "        sub_indent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            if file.endswith('.nii.gz'):\n",
    "                print(f\"{sub_indent}📄 {file}\")\n",
    "                dataset_info['files'].append(os.path.join(root, file))\n",
    "    \n",
    "    # Verificar diretórios essenciais\n",
    "    essential_dirs = ['imagesTr', 'labelsTr']\n",
    "    for dir_name in essential_dirs:\n",
    "        dir_path = os.path.join(dataset_path, dir_name)\n",
    "        if os.path.exists(dir_path):\n",
    "            files_count = len([f for f in os.listdir(dir_path) if f.endswith('.nii.gz')])\n",
    "            print(f\"✅ {dir_name}: {files_count} arquivos\")\n",
    "            dataset_info['subdirs'].append({\n",
    "                'name': dir_name,\n",
    "                'path': dir_path,\n",
    "                'file_count': files_count\n",
    "            })\n",
    "        else:\n",
    "            print(f\"❌ {dir_name}: não encontrado\")\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "# Executar descoberta\n",
    "DATASET_PATH = project_config['paths']['dataset']\n",
    "dataset_info = discover_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c674a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Encontrados 21 arquivos de imagem\n",
      "📊 Encontrados 22 arquivos de label\n",
      "✅ 0 pares imagem-label correspondentes\n",
      "\n",
      "🔬 Analisando arquivos em detalhes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1becc8d1aec449e9aef16a7c70d3d1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analisando amostras: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 📊 ANÁLISE DETALHADA DOS ARQUIVOS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_nifti_files(dataset_path):\n",
    "    \"\"\"Analisa arquivos NIfTI em detalhes\"\"\"\n",
    "    \n",
    "    images_dir = os.path.join(dataset_path, 'imagesTr')\n",
    "    labels_dir = os.path.join(dataset_path, 'labelsTr')\n",
    "    \n",
    "    if not (os.path.exists(images_dir) and os.path.exists(labels_dir)):\n",
    "        print(\"❌ Diretórios de imagens e/ou labels não encontrados\")\n",
    "        return None\n",
    "    \n",
    "    # Coletar arquivos\n",
    "    image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.nii.gz')])\n",
    "    label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.nii.gz')])\n",
    "    \n",
    "    print(f\"📊 Encontrados {len(image_files)} arquivos de imagem\")\n",
    "    print(f\"📊 Encontrados {len(label_files)} arquivos de label\")\n",
    "    \n",
    "    # Analisar correspondência\n",
    "    image_ids = [f.replace('_0000.nii.gz', '') for f in image_files if '_0000' in f]\n",
    "    label_ids = [f.replace('.nii.gz', '') for f in label_files]\n",
    "    \n",
    "    matched_pairs = set(image_ids) & set(label_ids)\n",
    "    print(f\"✅ {len(matched_pairs)} pares imagem-label correspondentes\")\n",
    "    \n",
    "    if len(matched_pairs) != len(image_ids):\n",
    "        missing = set(image_ids) - matched_pairs\n",
    "        print(f\"⚠️ Imagens sem labels correspondentes: {missing}\")\n",
    "    \n",
    "    # Análise detalhada de amostras\n",
    "    analysis_results = {\n",
    "        'files_info': [],\n",
    "        'dimensions': [],\n",
    "        'spacings': [],\n",
    "        'orientations': [],\n",
    "        'value_ranges': [],\n",
    "        'class_distributions': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\n🔬 Analisando arquivos em detalhes...\")\n",
    "    \n",
    "    # Analisar primeiro algumas amostras para obter estatísticas\n",
    "    sample_size = min(5, len(matched_pairs))\n",
    "    sample_ids = list(matched_pairs)[:sample_size]\n",
    "    \n",
    "    for i, file_id in enumerate(tqdm(sample_ids, desc=\"Analisando amostras\")):\n",
    "        \n",
    "        # Paths dos arquivos\n",
    "        img_path = os.path.join(images_dir, f\"{file_id}_0000.nii.gz\")\n",
    "        label_path = os.path.join(labels_dir, f\"{file_id}.nii.gz\")\n",
    "        \n",
    "        try:\n",
    "            # Carregar imagem\n",
    "            img_nii = nib.load(img_path)\n",
    "            img_data = img_nii.get_fdata()\n",
    "            \n",
    "            # Carregar label\n",
    "            label_nii = nib.load(label_path)\n",
    "            label_data = label_nii.get_fdata()\n",
    "            \n",
    "            # Informações básicas\n",
    "            file_info = {\n",
    "                'id': file_id,\n",
    "                'img_shape': img_data.shape,\n",
    "                'label_shape': label_data.shape,\n",
    "                'img_dtype': str(img_data.dtype),\n",
    "                'label_dtype': str(label_data.dtype)\n",
    "            }\n",
    "            \n",
    "            # Dimensões e espaçamento\n",
    "            header = img_nii.header\n",
    "            pixdim = header['pixdim'][1:4]  # Espaçamento dos pixels\n",
    "            \n",
    "            file_info.update({\n",
    "                'pixel_spacing': pixdim.tolist(),\n",
    "                'orientation': str(nib.aff2axcodes(img_nii.affine)),\n",
    "                'img_min': float(np.min(img_data)),\n",
    "                'img_max': float(np.max(img_data)),\n",
    "                'img_mean': float(np.mean(img_data)),\n",
    "                'img_std': float(np.std(img_data))\n",
    "            })\n",
    "            \n",
    "            # Distribuição de classes\n",
    "            unique_labels, counts = np.unique(label_data, return_counts=True)\n",
    "            class_dist = dict(zip(unique_labels.astype(int), counts))\n",
    "            file_info['class_distribution'] = class_dist\n",
    "            \n",
    "            analysis_results['files_info'].append(file_info)\n",
    "            analysis_results['dimensions'].append(img_data.shape)\n",
    "            analysis_results['spacings'].append(pixdim.tolist())\n",
    "            analysis_results['value_ranges'].append((np.min(img_data), np.max(img_data)))\n",
    "            analysis_results['class_distributions'].append(class_dist)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao analisar {file_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return analysis_results, matched_pairs\n",
    "\n",
    "# Executar análise\n",
    "if dataset_info:\n",
    "    analysis_results, matched_pairs = analyze_nifti_files(DATASET_PATH)\n",
    "else:\n",
    "    print(\"⚠️ Pulando análise - dataset não encontrado\")\n",
    "    analysis_results, matched_pairs = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31fa2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 📈 VISUALIZAÇÃO DE ESTATÍSTICAS DO DATASET\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_dataset_statistics(analysis_results):\n",
    "    \"\"\"Cria visualizações das estatísticas do dataset\"\"\"\n",
    "    \n",
    "    if not analysis_results:\n",
    "        print(\"❌ Sem dados para visualizar\")\n",
    "        return\n",
    "    \n",
    "    files_info = analysis_results['files_info']\n",
    "    \n",
    "    # Criar DataFrame para análise\n",
    "    df_stats = pd.DataFrame(files_info)\n",
    "    \n",
    "    print(\"📊 ESTATÍSTICAS DO DATASET\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Estatísticas de dimensões\n",
    "    dimensions = [info['img_shape'] for info in files_info]\n",
    "    dimensions_df = pd.DataFrame(dimensions, columns=['Width', 'Height', 'Slices'])\n",
    "    \n",
    "    print(\"📐 Dimensões das imagens:\")\n",
    "    print(dimensions_df.describe())\n",
    "    \n",
    "    # Estatísticas de espaçamento\n",
    "    spacings = [info['pixel_spacing'] for info in files_info]\n",
    "    spacings_df = pd.DataFrame(spacings, columns=['Spacing_X', 'Spacing_Y', 'Spacing_Z'])\n",
    "    \n",
    "    print(\"\\n📏 Espaçamento dos pixels:\")\n",
    "    print(spacings_df.describe())\n",
    "    \n",
    "    # Estatísticas de intensidade\n",
    "    print(f\"\\n💡 Intensidades das imagens:\")\n",
    "    print(f\"   Min: {df_stats['img_min'].min():.2f} - {df_stats['img_min'].max():.2f}\")\n",
    "    print(f\"   Max: {df_stats['img_max'].min():.2f} - {df_stats['img_max'].max():.2f}\")\n",
    "    print(f\"   Mean: {df_stats['img_mean'].min():.2f} - {df_stats['img_mean'].max():.2f}\")\n",
    "    print(f\"   Std: {df_stats['img_std'].min():.2f} - {df_stats['img_std'].max():.2f}\")\n",
    "    \n",
    "    # Plotar visualizações\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('📊 Análise Estatística do Dataset Task02_Heart', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Distribuição de dimensões\n",
    "    axes[0, 0].hist([d[2] for d in dimensions], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribuição do Número de Fatias')\n",
    "    axes[0, 0].set_xlabel('Número de Fatias')\n",
    "    axes[0, 0].set_ylabel('Frequência')\n",
    "    \n",
    "    # Distribuição de espaçamento\n",
    "    axes[0, 1].boxplot([spacings_df['Spacing_X'], spacings_df['Spacing_Y'], spacings_df['Spacing_Z']], \n",
    "                      labels=['X', 'Y', 'Z'])\n",
    "    axes[0, 1].set_title('Espaçamento dos Pixels por Eixo')\n",
    "    axes[0, 1].set_ylabel('Espaçamento (mm)')\n",
    "    \n",
    "    # Distribuição de intensidades médias\n",
    "    axes[0, 2].hist(df_stats['img_mean'], bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0, 2].set_title('Distribuição das Intensidades Médias')\n",
    "    axes[0, 2].set_xlabel('Intensidade Média')\n",
    "    axes[0, 2].set_ylabel('Frequência')\n",
    "    \n",
    "    # Scatter plot: dimensões vs intensidade média\n",
    "    scatter_data = [(d[0], d[1], d[2], m) for d, m in zip(dimensions, df_stats['img_mean'])]\n",
    "    scatter_df = pd.DataFrame(scatter_data, columns=['Width', 'Height', 'Slices', 'Mean_Intensity'])\n",
    "    \n",
    "    scatter = axes[1, 0].scatter(scatter_df['Slices'], scatter_df['Mean_Intensity'], \n",
    "                                alpha=0.7, c=scatter_df['Width'], cmap='viridis')\n",
    "    axes[1, 0].set_title('Fatias vs Intensidade Média')\n",
    "    axes[1, 0].set_xlabel('Número de Fatias')\n",
    "    axes[1, 0].set_ylabel('Intensidade Média')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='Width')\n",
    "    \n",
    "    # Distribuição de classes agregada\n",
    "    all_class_counts = defaultdict(int)\n",
    "    for class_dist in analysis_results['class_distributions']:\n",
    "        for class_id, count in class_dist.items():\n",
    "            all_class_counts[class_id] += count\n",
    "    \n",
    "    classes = list(all_class_counts.keys())\n",
    "    counts = list(all_class_counts.values())\n",
    "    \n",
    "    axes[1, 1].bar(classes, counts, color=['black', 'red', 'green'], alpha=0.7)\n",
    "    axes[1, 1].set_title('Distribuição Total de Classes')\n",
    "    axes[1, 1].set_xlabel('Classe')\n",
    "    axes[1, 1].set_ylabel('Contagem de Pixels')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    # Proporção de classes\n",
    "    total_pixels = sum(counts)\n",
    "    proportions = [count/total_pixels for count in counts]\n",
    "    \n",
    "    axes[1, 2].pie(proportions, labels=[f'Classe {c}' for c in classes], \n",
    "                  colors=['black', 'red', 'green'], autopct='%1.1f%%')\n",
    "    axes[1, 2].set_title('Proporção de Classes')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_stats, dimensions_df, spacings_df\n",
    "\n",
    "# Visualizar estatísticas\n",
    "if analysis_results:\n",
    "    df_stats, dimensions_df, spacings_df = visualize_dataset_statistics(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24c8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 🔬 ANÁLISE DETALHADA DE UMA AMOSTRA\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_sample_in_detail(dataset_path, sample_id, matched_pairs):\n",
    "    \"\"\"Análise detalhada de uma amostra específica\"\"\"\n",
    "    \n",
    "    if not matched_pairs or sample_id not in matched_pairs:\n",
    "        # Pegar primeira amostra disponível\n",
    "        sample_id = list(matched_pairs)[0] if matched_pairs else None\n",
    "        if not sample_id:\n",
    "            print(\"❌ Nenhuma amostra disponível para análise\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"🔬 ANÁLISE DETALHADA DA AMOSTRA: {sample_id}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Carregar arquivos\n",
    "    images_dir = os.path.join(dataset_path, 'imagesTr')\n",
    "    labels_dir = os.path.join(dataset_path, 'labelsTr')\n",
    "    \n",
    "    img_path = os.path.join(images_dir, f\"{sample_id}_0000.nii.gz\")\n",
    "    label_path = os.path.join(labels_dir, f\"{sample_id}.nii.gz\")\n",
    "    \n",
    "    try:\n",
    "        # Carregar usando nibabel\n",
    "        img_nii = nib.load(img_path)\n",
    "        label_nii = nib.load(label_path)\n",
    "        \n",
    "        img_data = img_nii.get_fdata()\n",
    "        label_data = label_nii.get_fdata()\n",
    "        \n",
    "        print(f\"📄 Arquivo de imagem: {os.path.basename(img_path)}\")\n",
    "        print(f\"📄 Arquivo de label: {os.path.basename(label_path)}\")\n",
    "        print(f\"📐 Dimensões da imagem: {img_data.shape}\")\n",
    "        print(f\"📐 Dimensões do label: {label_data.shape}\")\n",
    "        \n",
    "        # Informações do header\n",
    "        header = img_nii.header\n",
    "        print(f\"🔍 Orientação: {nib.aff2axcodes(img_nii.affine)}\")\n",
    "        print(f\"📏 Espaçamento: {header['pixdim'][1:4]}\")\n",
    "        print(f\"🎯 Tipo de dado: {img_data.dtype}\")\n",
    "        \n",
    "        # Estatísticas da imagem\n",
    "        print(f\"\\n💡 Estatísticas da imagem:\")\n",
    "        print(f\"   Min: {np.min(img_data):.2f}\")\n",
    "        print(f\"   Max: {np.max(img_data):.2f}\")\n",
    "        print(f\"   Mean: {np.mean(img_data):.2f}\")\n",
    "        print(f\"   Std: {np.std(img_data):.2f}\")\n",
    "        print(f\"   Percentis: {np.percentile(img_data, [5, 25, 50, 75, 95])}\")\n",
    "        \n",
    "        # Análise das classes\n",
    "        unique_labels, counts = np.unique(label_data, return_counts=True)\n",
    "        print(f\"\\n🏷️ Classes encontradas:\")\n",
    "        class_names = ['Background', 'Left Ventricle', 'Myocardium']\n",
    "        for label, count in zip(unique_labels, counts):\n",
    "            label_int = int(label)\n",
    "            class_name = class_names[label_int] if label_int < len(class_names) else f\"Classe {label_int}\"\n",
    "            percentage = (count / label_data.size) * 100\n",
    "            print(f\"   {class_name} (Classe {label_int}): {count:,} pixels ({percentage:.2f}%)\")\n",
    "        \n",
    "        # Visualização de fatias representativas\n",
    "        visualize_sample_slices(img_data, label_data, sample_id)\n",
    "        \n",
    "        return img_data, label_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao carregar amostra {sample_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def visualize_sample_slices(img_data, label_data, sample_id):\n",
    "    \"\"\"Visualiza fatias representativas da amostra\"\"\"\n",
    "    \n",
    "    num_slices = img_data.shape[2]\n",
    "    \n",
    "    # Selecionar fatias representativas\n",
    "    slice_indices = [\n",
    "        num_slices // 4,      # 25%\n",
    "        num_slices // 2,      # 50% (meio)\n",
    "        3 * num_slices // 4   # 75%\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    fig.suptitle(f'🔬 Análise da Amostra {sample_id} - Fatias Representativas', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        \n",
    "        # Imagem original\n",
    "        axes[i, 0].imshow(img_data[:, :, slice_idx].T, cmap='gray', origin='lower')\n",
    "        axes[i, 0].set_title(f'Imagem - Fatia {slice_idx}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Label colorizada\n",
    "        label_slice = label_data[:, :, slice_idx].T\n",
    "        axes[i, 1].imshow(label_slice, cmap='jet', origin='lower', vmin=0, vmax=2)\n",
    "        axes[i, 1].set_title(f'Labels - Fatia {slice_idx}')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        axes[i, 2].imshow(img_data[:, :, slice_idx].T, cmap='gray', origin='lower')\n",
    "        \n",
    "        # Criar máscara colorida para overlay\n",
    "        label_colored = np.zeros((*label_slice.shape, 3))\n",
    "        label_colored[label_slice == 1] = [1, 0, 0]  # Vermelho para classe 1\n",
    "        label_colored[label_slice == 2] = [0, 1, 0]  # Verde para classe 2\n",
    "        \n",
    "        # Aplicar transparência\n",
    "        alpha = 0.3\n",
    "        overlay_mask = label_slice > 0\n",
    "        label_colored = label_colored * alpha\n",
    "        \n",
    "        axes[i, 2].imshow(label_colored, origin='lower', alpha=0.7)\n",
    "        axes[i, 2].set_title(f'Overlay - Fatia {slice_idx}')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Criar visualização interativa com Plotly\n",
    "    create_interactive_volume_view(img_data, label_data, sample_id)\n",
    "\n",
    "def create_interactive_volume_view(img_data, label_data, sample_id):\n",
    "    \"\"\"Cria visualização interativa do volume\"\"\"\n",
    "    \n",
    "    # Selecionar fatia do meio para visualização\n",
    "    mid_slice = img_data.shape[2] // 2\n",
    "    \n",
    "    img_slice = img_data[:, :, mid_slice]\n",
    "    label_slice = label_data[:, :, mid_slice]\n",
    "    \n",
    "    # Criar subplot\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=('Imagem Original', 'Segmentação', 'Histograma'),\n",
    "        specs=[[{'type': 'heatmap'}, {'type': 'heatmap'}, {'type': 'histogram'}]]\n",
    "    )\n",
    "    \n",
    "    # Imagem original\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=img_slice.T,\n",
    "            colorscale='gray',\n",
    "            showscale=True,\n",
    "            name='Imagem'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Segmentação\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=label_slice.T,\n",
    "            colorscale='viridis',\n",
    "            showscale=True,\n",
    "            name='Labels'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Histograma\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=img_slice.flatten(),\n",
    "            nbinsx=50,\n",
    "            name='Distribuição de Intensidades'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'📊 Visualização Interativa - {sample_id} (Fatia {mid_slice})',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Executar análise detalhada\n",
    "if analysis_results and matched_pairs:\n",
    "    sample_img, sample_label = analyze_sample_in_detail(DATASET_PATH, None, matched_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cdeb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline de pré-processamento criado\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 🔧 PIPELINE DE PRÉ-PROCESSAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"Cria pipeline de pré-processamento otimizado\"\"\"\n",
    "    \n",
    "    class HeartDataPreprocessor:\n",
    "        def __init__(self, target_size=(128, 128), normalize_method='percentile'):\n",
    "            self.target_size = target_size\n",
    "            self.normalize_method = normalize_method\n",
    "            \n",
    "        def load_volume(self, filepath):\n",
    "            \"\"\"Carrega volume 3D com orientação correta\"\"\"\n",
    "            try:\n",
    "                img = nib.load(filepath)\n",
    "                data = img.get_fdata()\n",
    "                \n",
    "                # Corrigir orientação se necessário\n",
    "                # Task02_Heart geralmente precisa de transpose\n",
    "                data = np.transpose(data, (2, 1, 0))\n",
    "                \n",
    "                return data\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Erro ao carregar {filepath}: {e}\")\n",
    "                return None\n",
    "        \n",
    "        def normalize_slice(self, slice_img, method='percentile'):\n",
    "            \"\"\"Normaliza fatia individual\"\"\"\n",
    "            \n",
    "            if method == 'percentile':\n",
    "                # Normalização baseada em percentis (melhor para imagens médicas)\n",
    "                p2 = np.percentile(slice_img, 2)\n",
    "                p98 = np.percentile(slice_img, 98)\n",
    "                normalized = np.clip((slice_img - p2) / (p98 - p2 + 1e-8), 0, 1)\n",
    "                \n",
    "            elif method == 'z_score':\n",
    "                # Z-score normalization\n",
    "                mean = np.mean(slice_img)\n",
    "                std = np.std(slice_img)\n",
    "                normalized = (slice_img - mean) / (std + 1e-8)\n",
    "                normalized = np.clip(normalized, -3, 3)  # Clip outliers\n",
    "                normalized = (normalized + 3) / 6  # Normalize to [0, 1]\n",
    "                \n",
    "            elif method == 'min_max':\n",
    "                # Min-max normalization\n",
    "                min_val = np.min(slice_img)\n",
    "                max_val = np.max(slice_img)\n",
    "                normalized = (slice_img - min_val) / (max_val - min_val + 1e-8)\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Método de normalização inválido: {method}\")\n",
    "            \n",
    "            return normalized.astype(np.float32)\n",
    "        \n",
    "        def resize_slice(self, slice_img, target_size, order=1):\n",
    "            \"\"\"Redimensiona fatia preservando proporções\"\"\"\n",
    "            from skimage.transform import resize\n",
    "            \n",
    "            resized = resize(\n",
    "                slice_img, \n",
    "                target_size,\n",
    "                order=order,  # 1 = bilinear, 0 = nearest neighbor\n",
    "                mode='constant',\n",
    "                preserve_range=True,\n",
    "                anti_aliasing=True\n",
    "            )\n",
    "            \n",
    "            return resized.astype(slice_img.dtype)\n",
    "        \n",
    "        def preprocess_volume(self, volume, is_label=False):\n",
    "            \"\"\"Pré-processa volume completo\"\"\"\n",
    "            \n",
    "            if volume is None:\n",
    "                return None\n",
    "                \n",
    "            processed_slices = []\n",
    "            \n",
    "            for slice_idx in range(volume.shape[0]):\n",
    "                slice_img = volume[slice_idx, :, :]\n",
    "                \n",
    "                if not is_label:\n",
    "                    # Normalizar apenas imagens (não labels)\n",
    "                    slice_img = self.normalize_slice(slice_img, self.normalize_method)\n",
    "                \n",
    "                # Redimensionar\n",
    "                order = 0 if is_label else 1  # Nearest neighbor para labels\n",
    "                slice_resized = self.resize_slice(slice_img, self.target_size, order=order)\n",
    "                \n",
    "                processed_slices.append(slice_resized)\n",
    "            \n",
    "            return np.array(processed_slices)\n",
    "        \n",
    "        def process_pair(self, img_path, label_path):\n",
    "            \"\"\"Processa par imagem-label\"\"\"\n",
    "            \n",
    "            # Carregar volumes\n",
    "            img_volume = self.load_volume(img_path)\n",
    "            label_volume = self.load_volume(label_path)\n",
    "            \n",
    "            if img_volume is None or label_volume is None:\n",
    "                return None, None\n",
    "            \n",
    "            # Verificar compatibilidade de dimensões\n",
    "            if img_volume.shape != label_volume.shape:\n",
    "                print(f\"⚠️ Dimensões incompatíveis: {img_volume.shape} vs {label_volume.shape}\")\n",
    "                return None, None\n",
    "            \n",
    "            # Pré-processar\n",
    "            img_processed = self.preprocess_volume(img_volume, is_label=False)\n",
    "            label_processed = self.preprocess_volume(label_volume, is_label=True)\n",
    "            \n",
    "            return img_processed, label_processed\n",
    "    \n",
    "    return HeartDataPreprocessor()\n",
    "\n",
    "# Criar preprocessor\n",
    "preprocessor = create_preprocessing_pipeline()\n",
    "print(\"✅ Pipeline de pré-processamento criado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97754a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 🧪 TESTE DO PIPELINE DE PRÉ-PROCESSAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "def test_preprocessing_pipeline(preprocessor, dataset_path, matched_pairs, num_samples=2):\n",
    "    \"\"\"Testa pipeline de pré-processamento\"\"\"\n",
    "    \n",
    "    if not matched_pairs:\n",
    "        print(\"❌ Nenhuma amostra disponível para teste\")\n",
    "        return None\n",
    "    \n",
    "    print(\"🧪 TESTANDO PIPELINE DE PRÉ-PROCESSAMENTO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    images_dir = os.path.join(dataset_path, 'imagesTr')\n",
    "    labels_dir = os.path.join(dataset_path, 'labelsTr')\n",
    "    \n",
    "    test_results = []\n",
    "    sample_ids = list(matched_pairs)[:num_samples]\n",
    "    \n",
    "    for sample_id in sample_ids:\n",
    "        print(f\"\\n🔄 Processando amostra: {sample_id}\")\n",
    "        \n",
    "        img_path = os.path.join(images_dir, f\"{sample_id}_0000.nii.gz\")\n",
    "        label_path = os.path.join(labels_dir, f\"{sample_id}.nii.gz\")\n",
    "        \n",
    "        # Processar\n",
    "        img_processed, label_processed = preprocessor.process_pair(img_path, label_path)\n",
    "        \n",
    "        if img_processed is not None and label_processed is not None:\n",
    "            \n",
    "            result = {\n",
    "                'sample_id': sample_id,\n",
    "                'original_shape': None,  # Será preenchido\n",
    "                'processed_shape': img_processed.shape,\n",
    "                'img_stats': {\n",
    "                    'min': np.min(img_processed),\n",
    "                    'max': np.max(img_processed),\n",
    "                    'mean': np.mean(img_processed),\n",
    "                    'std': np.std(img_processed)\n",
    "                },\n",
    "                'label_classes': np.unique(label_processed),\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✅ Sucesso!\")\n",
    "            print(f\"     Forma processada: {img_processed.shape}\")\n",
    "            print(f\"     Range da imagem: [{result['img_stats']['min']:.3f}, {result['img_stats']['max']:.3f}]\")\n",
    "            print(f\"     Classes no label: {result['label_classes']}\")\n",
    "            \n",
    "            test_results.append(result)\n",
    "            \n",
    "            # Visualizar resultado\n",
    "            visualize_preprocessing_result(img_processed, label_processed, sample_id)\n",
    "            \n",
    "        else:\n",
    "            print(f\"  ❌ Falha no processamento\")\n",
    "            test_results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'success': False\n",
    "            })\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "def visualize_preprocessing_result(img_processed, label_processed, sample_id):\n",
    "    \"\"\"Visualiza resultado do pré-processamento\"\"\"\n",
    "    \n",
    "    # Selecionar fatias para visualização\n",
    "    num_slices = img_processed.shape[0]\n",
    "    slice_idx = num_slices // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f'🔧 Resultado do Pré-processamento - {sample_id}', fontweight='bold')\n",
    "    \n",
    "    # Imagem processada\n",
    "    axes[0].imshow(img_processed[slice_idx], cmap='gray')\n",
    "    axes[0].set_title(f'Imagem Processada\\n{img_processed.shape}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Label processado\n",
    "    axes[1].imshow(label_processed[slice_idx], cmap='jet', vmin=0, vmax=2)\n",
    "    axes[1].set_title(f'Label Processado\\n{label_processed.shape}')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(img_processed[slice_idx], cmap='gray')\n",
    "    \n",
    "    # Criar overlay colorido\n",
    "    label_slice = label_processed[slice_idx]\n",
    "    overlay = np.zeros((*label_slice.shape, 3))\n",
    "    overlay[label_slice == 1] = [1, 0, 0]  # Vermelho\n",
    "    overlay[label_slice == 2] = [0, 1, 0]  # Verde\n",
    "    \n",
    "    axes[2].imshow(overlay, alpha=0.4)\n",
    "    axes[2].set_title('Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Testar pipeline\n",
    "if matched_pairs:\n",
    "    test_results = test_preprocessing_pipeline(preprocessor, DATASET_PATH, matched_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dae4d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pulando preparação de dados - dataset não encontrado\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 💾 PREPARAÇÃO DE DADOS PARA TREINAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_training_data(preprocessor, dataset_path, matched_pairs, \n",
    "                         validation_split=0.2, test_split=0.1, \n",
    "                         save_processed=True):\n",
    "    \"\"\"Prepara dados completos para treinamento\"\"\"\n",
    "    \n",
    "    if not matched_pairs:\n",
    "        print(\"❌ Nenhuma amostra disponível\")\n",
    "        return None\n",
    "    \n",
    "    print(\"💾 PREPARANDO DADOS PARA TREINAMENTO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    images_dir = os.path.join(dataset_path, 'imagesTr')\n",
    "    labels_dir = os.path.join(dataset_path, 'labelsTr')\n",
    "    \n",
    "    # Listas para armazenar dados processados\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_sample_ids = []\n",
    "    \n",
    "    print(f\"🔄 Processando {len(matched_pairs)} amostras...\")\n",
    "    \n",
    "    for sample_id in tqdm(matched_pairs, desc=\"Processando amostras\"):\n",
    "        \n",
    "        img_path = os.path.join(images_dir, f\"{sample_id}_0000.nii.gz\")\n",
    "        label_path = os.path.join(labels_dir, f\"{sample_id}.nii.gz\")\n",
    "        \n",
    "        # Processar par\n",
    "        img_processed, label_processed = preprocessor.process_pair(img_path, label_path)\n",
    "        \n",
    "        if img_processed is not None and label_processed is not None:\n",
    "            all_images.append(img_processed)\n",
    "            all_labels.append(label_processed)\n",
    "            all_sample_ids.extend([sample_id] * len(img_processed))\n",
    "    \n",
    "    if not all_images:\n",
    "        print(\"❌ Nenhuma amostra foi processada com sucesso\")\n",
    "        return None\n",
    "    \n",
    "    # Concatenar todas as fatias\n",
    "    X = np.concatenate(all_images, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    print(f\"✅ Dados processados:\")\n",
    "    print(f\"   📊 Total de fatias: {len(X)}\")\n",
    "    print(f\"   📐 Forma das imagens: {X.shape}\")\n",
    "    print(f\"   📐 Forma dos labels: {y.shape}\")\n",
    "    \n",
    "    # Adicionar dimensão de canal para imagens\n",
    "    X = X[..., np.newaxis]\n",
    "    \n",
    "    # Converter labels para one-hot encoding\n",
    "    y_categorical = utils.to_categorical(y, num_classes=3)\n",
    "    \n",
    "    print(f\"   📐 Forma final X: {X.shape}\")\n",
    "    print(f\"   📐 Forma final y: {y_categorical.shape}\")\n",
    "    \n",
    "    # Dividir dados em treino/validação/teste\n",
    "    # Primeiro, separar teste\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y_categorical, test_size=test_split, random_state=42, stratify=np.argmax(y_categorical, axis=-1)\n",
    "    )\n",
    "    \n",
    "    # Depois, dividir treino e validação\n",
    "    val_size = validation_split / (1 - test_split)  # Ajustar proporção\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, random_state=42, stratify=np.argmax(y_temp, axis=-1)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 Divisão final dos dados:\")\n",
    "    print(f\"   🏋️ Treino: {X_train.shape[0]} fatias ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   ✅ Validação: {X_val.shape[0]} fatias ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"   🧪 Teste: {X_test.shape[0]} fatias ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Análise de distribuição de classes\n",
    "    for split_name, y_split in [('Treino', y_train), ('Validação', y_val), ('Teste', y_test)]:\n",
    "        class_counts = np.sum(y_split, axis=0)\n",
    "        total = np.sum(class_counts)\n",
    "        print(f\"\\n🏷️ Distribuição de classes ({split_name}):\")\n",
    "        for i, (count, class_name) in enumerate(zip(class_counts, \n",
    "                                                   ['Background', 'Left Ventricle', 'Myocardium'])):\n",
    "            print(f\"   {class_name}: {int(count):,} pixels ({count/total*100:.1f}%)\")\n",
    "    \n",
    "    # Salvar dados processados se solicitado\n",
    "    if save_processed:\n",
    "        save_path = os.path.join(project_config['paths']['outputs'], 'processed_data')\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n💾 Salvando dados processados em: {save_path}\")\n",
    "        \n",
    "        np.save(os.path.join(save_path, 'X_train.npy'), X_train)\n",
    "        np.save(os.path.join(save_path, 'y_train.npy'), y_train)\n",
    "        np.save(os.path.join(save_path, 'X_val.npy'), X_val)\n",
    "        np.save(os.path.join(save_path, 'y_val.npy'), y_val)\n",
    "        np.save(os.path.join(save_path, 'X_test.npy'), X_test)\n",
    "        np.save(os.path.join(save_path, 'y_test.npy'), y_test)\n",
    "        \n",
    "        # Salvar metadados\n",
    "        metadata = {\n",
    "            'total_samples': len(matched_pairs),\n",
    "            'total_slices': len(X),\n",
    "            'image_shape': X.shape[1:],\n",
    "            'num_classes': 3,\n",
    "            'class_names': ['Background', 'Left Ventricle', 'Myocardium'],\n",
    "            'splits': {\n",
    "                'train': len(X_train),\n",
    "                'validation': len(X_val),\n",
    "                'test': len(X_test)\n",
    "            },\n",
    "            'preprocessing': {\n",
    "                'target_size': preprocessor.target_size,\n",
    "                'normalize_method': preprocessor.normalize_method\n",
    "            },\n",
    "            'created': str(pd.Timestamp.now())\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_path, 'metadata.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(\"✅ Dados salvos com sucesso!\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val,\n",
    "        'X_test': X_test, 'y_test': y_test,\n",
    "        'metadata': metadata if save_processed else None\n",
    "    }\n",
    "\n",
    "# Preparar dados\n",
    "if matched_pairs:\n",
    "    training_data = prepare_training_data(\n",
    "        preprocessor, DATASET_PATH, matched_pairs,\n",
    "        validation_split=0.2, test_split=0.1\n",
    "    )\n",
    "    \n",
    "    if training_data:\n",
    "        print(\"\\n🎉 Dados prontos para treinamento!\")\n",
    "else:\n",
    "    print(\"⚠️ Pulando preparação de dados - dataset não encontrado\")\n",
    "    training_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a52ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Resumo da Análise de Dados\n",
    "\n",
    "### ✅ Tarefas Concluídas\n",
    "\n",
    "1. **📊 Descoberta do Dataset**\n",
    "   - Estrutura de diretórios analisada\n",
    "   - Arquivos de imagem e labels identificados\n",
    "   - Correspondência entre pares verificada\n",
    "\n",
    "2. **🔍 Análise Estatística**\n",
    "   - Dimensões e espaçamento analisados\n",
    "   - Distribuição de intensidades caracterizada\n",
    "   - Distribuição de classes quantificada\n",
    "\n",
    "3. **🔬 Análise Detalhada**\n",
    "   - Amostras individuais examinadas\n",
    "   - Visualizações criadas\n",
    "   - Propriedades espaciais verificadas\n",
    "\n",
    "4. **🔧 Pipeline de Pré-processamento**\n",
    "   - Normalização por percentis implementada\n",
    "   - Redimensionamento preservando qualidade\n",
    "   - Tratamento adequado de labels\n",
    "\n",
    "5. **💾 Preparação para Treinamento**\n",
    "   - Dados divididos em treino/validação/teste\n",
    "   - One-hot encoding aplicado\n",
    "   - Metadados salvos\n",
    "\n",
    "### 📈 Principais Descobertas\n",
    "\n",
    "- **Dimensões**: Variáveis, normalizadas para 128x128\n",
    "- **Classes**: 3 classes com forte desbalanceamento (Background dominante)\n",
    "- **Qualidade**: Dados consistentes e adequados para treinamento\n",
    "- **Desafios**: Desbalanceamento de classes, variabilidade anatômica\n",
    "\n",
    "### 🚀 Próximos Passos\n",
    "\n",
    "1. **🔄 Data Augmentation**: Execute `02_Data_Augmentation.ipynb`\n",
    "2. **🏗️ Model Architecture**: Execute `03_Model_Architecture.ipynb`\n",
    "3. **📈 Loss Functions**: Execute `04_Loss_Functions_and_Metrics.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "**✨ Dados analisados e prontos para as próximas etapas!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
